{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bryan Flores <br>\n",
    "ADS-509 <br>\n",
    "Assignment 2.1 <br>\n",
    "January 22, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f79baf9",
   "metadata": {},
   "source": [
    "# ADS 509 Assignment 2.1: Tokenization, Normalization, Descriptive Statistics \n",
    "\n",
    "This notebook holds Assignment 2.1 for Module 2 in ADS 509, Applied Text Mining. Work through this notebook, writing code and answering questions where required. \n",
    "\n",
    "In the previous assignment you pulled lyrics data on two artists. In this assignment we explore this data set and a pull from the now-defunct Twitter API for the artists Cher and Robyn.  If, for some reason, you did not complete that previous assignment, data to use for this assignment can be found in the assignment materials section of Canvas. \n",
    "\n",
    "This assignment asks you to write a short function to calculate some descriptive statistics on a piece of text. Then you are asked to find some interesting and unique statistics on your corpora. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae8e2e1",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it. \n",
    "\n",
    "One sign of mature code is conforming to a style guide. We recommend the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html). If you use a different style guide, please include a cell with a link. \n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential `import` statements and make sure that all such statements are moved into the designated cell. \n",
    "\n",
    "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. *Make sure to answer every question marked with a `Q:` for full credit.* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe4191c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rudy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')  # Uncomment if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2d096b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "sw = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b555ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add any additional import statements you need here\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pathlib import Path\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "923b5a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change `data_location` to the location of the folder on your machine.\n",
    "# data_location = \"/users/chandler/dropbox/teaching/repos/ads-tm-api-scrape/\"\n",
    "data_location = \"data/\"\n",
    "\n",
    "# These subfolders should still work if you correctly stored the \n",
    "# data from the Module 1 assignment\n",
    "twitter_folder = \"twitter/\"\n",
    "lyrics_folder = \"lyrics/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06522af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_stats(tokens, num_tokens = 5, verbose=True) :\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "        number of characters, lexical diversity (https://en.wikipedia.org/wiki/Lexical_diversity), \n",
    "        and num_tokens most common tokens. Return a list with the number of tokens, number\n",
    "        of unique tokens, lexical diversity, and number of characters. \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Fill in the correct values here. \n",
    "    num_tokens = len(tokens)\n",
    "    num_unique_tokens = len(Counter(tokens).keys())\n",
    "    lexical_diversity =  num_unique_tokens / num_tokens\n",
    "    num_characters =  sum([len(token) for token in tokens])\n",
    "\n",
    "    # Find 5 most common tokens and create dataframe for presentation\n",
    "    top_5_tokens = Counter(tokens).most_common(5)\n",
    "    df = pd.DataFrame(top_5_tokens, columns=['token', 'frequency'])\n",
    "    \n",
    "    if verbose :        \n",
    "        print(f\"There are {num_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "    \n",
    "        # print the five most common tokens\n",
    "        print(\"===========================\")\n",
    "        print(f\"The 5 most common tokens: \")\n",
    "        print(df)        \n",
    "        \n",
    "    return([num_tokens, \n",
    "            num_unique_tokens,\n",
    "            lexical_diversity,\n",
    "            num_characters\n",
    "            ])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59dcf058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 55 characters in the data.\n",
      "The lexical diversity is 0.692 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     text          3\n",
      "1     here          2\n",
      "2  example          2\n",
      "3       is          1\n",
      "4     some          1\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"here is some example text with other example text here in this text\"\"\".split()\n",
    "assert(descriptive_stats(text, verbose=True)[0] == 13)\n",
    "assert(descriptive_stats(text, verbose=False)[1] == 9)\n",
    "assert(abs(descriptive_stats(text, verbose=False)[2] - 0.69) < 0.02)\n",
    "assert(descriptive_stats(text, verbose=False)[3] == 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e7e1a2",
   "metadata": {},
   "source": [
    "Q: Why is it beneficial to use assertion statements in your code? \n",
    "\n",
    "A: Assertions allow you to test your logic for errors and invalid assumptions and explicitly state what we expect to be True or False."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3bf93e",
   "metadata": {},
   "source": [
    "## Data Input\n",
    "\n",
    "Now read in each of the corpora. For the lyrics data, it may be convenient to store the entire contents of the file to make it easier to inspect the titles individually, as you'll do in the last part of the assignment. In the solution, I stored the lyrics data in a dictionary with two dimensions of keys: artist and song. The value was the file contents. A data frame would work equally well. \n",
    "\n",
    "For the Twitter data, we only need the description field for this assignment. Feel free all the descriptions read it into a data structure. In the solution, I stored the descriptions as a dictionary of lists, with the key being the artist. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37d70801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the lyrics data\n",
    "lyrics_path = data_location + lyrics_folder\n",
    "PATH = glob.iglob(f'{lyrics_path}/*')\n",
    "\n",
    "\"\"\" \n",
    "    Example format:\n",
    "    {\n",
    "        'artist': cher,\n",
    "        'filename': \"cher_88degrees.txt\",\n",
    "        'title': \"88 Degrees\",\n",
    "        'lyrics': \"Stuck in L.A., ain't got no friends ...\"\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "lyrics_list = []\n",
    "\n",
    "for subdir in PATH:\n",
    "\n",
    "    # Define subdirectories\n",
    "    path = Path(subdir).glob('*')\n",
    "\n",
    "    # Iterate over song txt files\n",
    "    for file in path:\n",
    "\n",
    "        # Obtain file names, artists, song titles, and lyrics \n",
    "        filename = Path(file).name\n",
    "        artist = filename.split(\"_\")[0]\n",
    "        title = open(file, 'r').readline().strip(\"\\n\")\n",
    "        lyrics = ' '.join([line.strip() for line in open(file, 'r').readlines()[3:]]).lstrip()\n",
    "\n",
    "        # Key-values\n",
    "        lyrics_dict = {\n",
    "            'artist': artist,\n",
    "            'filename': filename,\n",
    "            'title': title,\n",
    "            'lyrics': lyrics\n",
    "            }\n",
    "        lyrics_list.append(lyrics_dict)\n",
    "\n",
    "lyrics_df = pd.DataFrame(lyrics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b4ca564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>filename</th>\n",
       "      <th>title</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cher</td>\n",
       "      <td>cher_88degrees.txt</td>\n",
       "      <td>\"88 Degrees\"</td>\n",
       "      <td>Stuck in L.A., ain't got no friends And so Hol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cher</td>\n",
       "      <td>cher_adifferentkindoflovesong.txt</td>\n",
       "      <td>\"A Different Kind Of Love Song\"</td>\n",
       "      <td>What if the world was crazy and I was sane Wou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cher</td>\n",
       "      <td>cher_afterall.txt</td>\n",
       "      <td>\"After All\"</td>\n",
       "      <td>Well, here we are again I guess it must be fat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cher</td>\n",
       "      <td>cher_again.txt</td>\n",
       "      <td>\"Again\"</td>\n",
       "      <td>Again evening finds me at your door Here to as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cher</td>\n",
       "      <td>cher_alfie.txt</td>\n",
       "      <td>\"Alfie\"</td>\n",
       "      <td>What's it all about, Alfie? Is it just for the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                           filename                            title  \\\n",
       "0   cher                 cher_88degrees.txt                     \"88 Degrees\"   \n",
       "1   cher  cher_adifferentkindoflovesong.txt  \"A Different Kind Of Love Song\"   \n",
       "2   cher                  cher_afterall.txt                      \"After All\"   \n",
       "3   cher                     cher_again.txt                          \"Again\"   \n",
       "4   cher                     cher_alfie.txt                          \"Alfie\"   \n",
       "\n",
       "                                              lyrics  \n",
       "0  Stuck in L.A., ain't got no friends And so Hol...  \n",
       "1  What if the world was crazy and I was sane Wou...  \n",
       "2  Well, here we are again I guess it must be fat...  \n",
       "3  Again evening finds me at your door Here to as...  \n",
       "4  What's it all about, Alfie? Is it just for the...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c9a96fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(file_path, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Batches the input file by a given batch size.\n",
    "    :param file_path: str, path to the followers data txt file\n",
    "    :param batch_size: int, 1000 row default size\n",
    "    :return batch:\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        batch = []\n",
    "        for line in file:\n",
    "            # Split row into parts by whitespaces\n",
    "            parts = re.split(r'\\s+', line.strip())\n",
    "            \n",
    "            # Skip lines with insufficient parts\n",
    "            if len(parts) < 8:\n",
    "                continue\n",
    "\n",
    "            # Extract the 'description' value following the last integer\n",
    "            description_val = ' '.join(parts[8:]) if len(parts) > 8 else ''\n",
    "\n",
    "            # Append the description to the batch\n",
    "            batch.append(description_val)\n",
    "\n",
    "            # Yield the batch when it reaches the specified size\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "\n",
    "        # Yield the remaining batch\n",
    "        if batch:\n",
    "            yield batch\n",
    "\n",
    "def batch_processor(file):     \n",
    "    \"\"\" \n",
    "    Process the batches.\n",
    "    :param file: str, follower data path\n",
    "    :return descriptions: list, description values \n",
    "    \"\"\"   \n",
    "    # Process the file in batches\n",
    "    descriptions = []\n",
    "\n",
    "    for batch_num, batch in enumerate(batch_generator(file)):\n",
    "        # Extend the list of descriptions with the current batch\n",
    "        descriptions.extend(batch)\n",
    "\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "debcac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the twitter data\n",
    "twitter_path = data_location + twitter_folder\n",
    "PATH = glob.iglob(f'{twitter_path}/*')\n",
    "\n",
    "\"\"\" \n",
    "    Example format:\n",
    "    {\n",
    "        'cher': {\n",
    "            'description': [\" \", \"𝙿𝚛𝚘𝚞𝚍 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛 𝚘𝚏 𝚖𝚎𝚜𝚜𝚢 𝚋𝚞𝚗𝚜 & 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜\", ...]\n",
    "            },\n",
    "        'robyn': {\n",
    "            'description': [\"\"I love chill\" ...\", ...]\n",
    "        }\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "descriptions = []\n",
    "\n",
    "# Iterate over Twitter folder and follower data \n",
    "for file in PATH:\n",
    "    # Obtain only the follower data files\n",
    "    filename = Path(file).name\n",
    "    if \"data\" in filename:\n",
    "        # Get descriptions\n",
    "        descriptions.append(batch_processor(file))\n",
    "\n",
    "twitter_dict = {\n",
    "    'cher': descriptions[0],\n",
    "    'robyn': descriptions[1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2b720d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['𝚘𝚏 𝚖𝚎𝚜𝚜𝚢 𝚋𝚞𝚗𝚜 & 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜',\n",
       " '',\n",
       " '@Washinformer @SpelmanCollege alumna #DCnative Award-winning journalist & PR pro @IABC Fellow & Past Chair IG: bcscomm Email: wibsiler@gmail.com',\n",
       " '159 I’m unemployed and live with my parents. MOOPS!',\n",
       " 'healing begin. Let us learn from the past. 🇨🇦 follower of #TheResistance',\n",
       " 'thou wilt. 🖤✨',\n",
       " '163 Curious Canadian Contemplator. She/Her. Sexagenarian. We are here for each other!',\n",
       " 'BLM. Mental health advocate. Do something nice for somebody today :)',\n",
       " \"80'S VIVA MÉXICO\",\n",
       " 'Mudita 🧩 INFP🌻']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display sample output\n",
    "twitter_dict['cher'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f3b12",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Now clean and tokenize your data. Remove punctuation chacters (available in the `punctuation` object in the `string` library), split on whitespace, fold to lowercase, and remove stopwords. Store your cleaned data, which must be accessible as an interable for `descriptive_stats`, in new objects or in new columns in your data frame. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71c73d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = set(punctuation) # speeds up comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76eb20b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(input_list):\n",
    "    \"\"\" \n",
    "    Clean data and return iterable for descriptive_stats.\n",
    "    :param :\n",
    "    :return :\n",
    "    \"\"\"\n",
    "    # Set stop words\n",
    "    stop_words = set(sw)\n",
    "\n",
    "    tokenized_descs = []\n",
    "    for item in input_list:\n",
    "\n",
    "        # remove punctuation, lower, and tokenize\n",
    "        clean_string = re.sub(r'[^\\w\\s]', '', item).lower().split()\n",
    "    \n",
    "        # Store transformed \n",
    "        filtered_sentence = [w for w in clean_string if not w.lower() in stop_words]\n",
    "\n",
    "        # Check for empty lists after transformations\n",
    "        if len(filtered_sentence) >= 1:\n",
    "            tokenized_descs.append(filtered_sentence)\n",
    "\n",
    "    return tokenized_descs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e16a0ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your clean twitter data here\n",
    "cher_twt_clean = data_cleaning(twitter_dict['cher'][:1000])  # Cher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c857fb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "robyn_twt_clean = data_cleaning(twitter_dict['robyn'][:1000])  # Robyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0f22e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your clean lyrics data here\n",
    "lyrics_df['lyrics_tokenized'] = data_cleaning(lyrics_df['lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcc52a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>filename</th>\n",
       "      <th>title</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>lyrics_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cher</td>\n",
       "      <td>cher_88degrees.txt</td>\n",
       "      <td>\"88 Degrees\"</td>\n",
       "      <td>Stuck in L.A., ain't got no friends And so Hol...</td>\n",
       "      <td>[stuck, la, aint, got, friends, hollywood, nut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cher</td>\n",
       "      <td>cher_adifferentkindoflovesong.txt</td>\n",
       "      <td>\"A Different Kind Of Love Song\"</td>\n",
       "      <td>What if the world was crazy and I was sane Wou...</td>\n",
       "      <td>[world, crazy, sane, would, strange, cant, bel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cher</td>\n",
       "      <td>cher_afterall.txt</td>\n",
       "      <td>\"After All\"</td>\n",
       "      <td>Well, here we are again I guess it must be fat...</td>\n",
       "      <td>[well, guess, must, fate, weve, tried, deep, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cher</td>\n",
       "      <td>cher_again.txt</td>\n",
       "      <td>\"Again\"</td>\n",
       "      <td>Again evening finds me at your door Here to as...</td>\n",
       "      <td>[evening, finds, door, ask, could, try, dont, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cher</td>\n",
       "      <td>cher_alfie.txt</td>\n",
       "      <td>\"Alfie\"</td>\n",
       "      <td>What's it all about, Alfie? Is it just for the...</td>\n",
       "      <td>[whats, alfie, moment, live, whats, sort, alfi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                           filename                            title  \\\n",
       "0   cher                 cher_88degrees.txt                     \"88 Degrees\"   \n",
       "1   cher  cher_adifferentkindoflovesong.txt  \"A Different Kind Of Love Song\"   \n",
       "2   cher                  cher_afterall.txt                      \"After All\"   \n",
       "3   cher                     cher_again.txt                          \"Again\"   \n",
       "4   cher                     cher_alfie.txt                          \"Alfie\"   \n",
       "\n",
       "                                              lyrics  \\\n",
       "0  Stuck in L.A., ain't got no friends And so Hol...   \n",
       "1  What if the world was crazy and I was sane Wou...   \n",
       "2  Well, here we are again I guess it must be fat...   \n",
       "3  Again evening finds me at your door Here to as...   \n",
       "4  What's it all about, Alfie? Is it just for the...   \n",
       "\n",
       "                                    lyrics_tokenized  \n",
       "0  [stuck, la, aint, got, friends, hollywood, nut...  \n",
       "1  [world, crazy, sane, would, strange, cant, bel...  \n",
       "2  [well, guess, must, fate, weve, tried, deep, i...  \n",
       "3  [evening, finds, door, ask, could, try, dont, ...  \n",
       "4  [whats, alfie, moment, live, whats, sort, alfi...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd0179",
   "metadata": {},
   "source": [
    "## Basic Descriptive Statistics\n",
    "\n",
    "Call your `descriptive_stats` function on both your lyrics data and your twitter data and for both artists (four total calls). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0bbedd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 19 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0        𝚘𝚏          1\n",
      "1     𝚖𝚎𝚜𝚜𝚢          1\n",
      "2      𝚋𝚞𝚗𝚜          1\n",
      "3  𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜          1\n",
      "There are 16 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 116 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "            token  frequency\n",
      "0    washinformer          1\n",
      "1  spelmancollege          1\n",
      "2          alumna          1\n",
      "3        dcnative          1\n",
      "4    awardwinning          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 31 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0         159          1\n",
      "1          im          1\n",
      "2  unemployed          1\n",
      "3        live          1\n",
      "4     parents          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 47 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  healing          1\n",
      "1    begin          1\n",
      "2      let          1\n",
      "3       us          1\n",
      "4    learn          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 8 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  thou          1\n",
      "1  wilt          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 48 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0           163          1\n",
      "1       curious          1\n",
      "2      canadian          1\n",
      "3  contemplator          1\n",
      "4        sheher          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 49 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0        blm          1\n",
      "1     mental          1\n",
      "2     health          1\n",
      "3   advocate          1\n",
      "4  something          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 13 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     80s          1\n",
      "1    viva          1\n",
      "2  méxico          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  mudita          1\n",
      "1    infp          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 79 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      201          1\n",
      "1       im          1\n",
      "2      guy          1\n",
      "3  married          1\n",
      "4     kids          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   clean          1\n",
      "1  saunas          1\n",
      "2    nice          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 28 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  california          1\n",
      "1         usa          1\n",
      "2         let          1\n",
      "3       truth          1\n",
      "4     prevail          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 24 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      752          1\n",
      "1     like          1\n",
      "2     cars          1\n",
      "3  guitars          1\n",
      "4   sports          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   asf          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 68 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                 token  frequency\n",
      "0               ottawa          1\n",
      "1  1219816548036227073          1\n",
      "2                 1275          1\n",
      "3                  414          1\n",
      "4                   18          1\n",
      "There are 10 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 66 characters in the data.\n",
      "The lexical diversity is 0.900 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0    survivor          2\n",
      "1  vaccinated          1\n",
      "2     cyclist          1\n",
      "3      cancer          1\n",
      "4      caldor          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   guy          1\n",
      "There are 19 tokens in the data.\n",
      "There are 19 unique tokens in the data.\n",
      "There are 120 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0         2132          1\n",
      "1      shehers          1\n",
      "2        queer          1\n",
      "3  progressive          1\n",
      "4    organizer          1\n",
      "There are 18 tokens in the data.\n",
      "There are 18 unique tokens in the data.\n",
      "There are 139 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       kyle          1\n",
      "1  hollywood          1\n",
      "2     137657          1\n",
      "3     143879          1\n",
      "4      actor          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 19 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  actual          1\n",
      "1    real          1\n",
      "2  person          1\n",
      "3     one          1\n",
      "There are 22 tokens in the data.\n",
      "There are 22 unique tokens in the data.\n",
      "There are 142 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                 token  frequency\n",
      "0                 crap          1\n",
      "1  1178663736288120835          1\n",
      "2                texas          1\n",
      "3                  usa          1\n",
      "4                  281          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 58 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0    avalance          1\n",
      "1  worshipper          1\n",
      "2     legends          1\n",
      "3       whore          1\n",
      "4         tua          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 2 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0    af          1\n",
      "There are 14 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 63 characters in the data.\n",
      "The lexical diversity is 0.857 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    state          2\n",
      "1     1988          2\n",
      "2      268          1\n",
      "3     penn          1\n",
      "4  alumnus          1\n",
      "There are 17 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 106 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0        90          1\n",
      "1  crossfit          1\n",
      "2      guru          1\n",
      "3     civil          1\n",
      "4       war          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 51 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    draw          1\n",
      "1    line          1\n",
      "2  stinky          1\n",
      "3    tofu          1\n",
      "4   lives          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 52 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0     fashion          1\n",
      "1      beauty          1\n",
      "2  enthusiast          1\n",
      "3    goofball          1\n",
      "4       proud          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 74 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0     runner          1\n",
      "1      otfer          1\n",
      "2  sometimes          1\n",
      "3     singer          1\n",
      "4      never          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 14 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  perrito          1\n",
      "1  querido          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  choice          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 18 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  always          1\n",
      "1  moving          1\n",
      "2     530          1\n",
      "3     410          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 54 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   could          1\n",
      "1  either          1\n",
      "2   watch          1\n",
      "3  happen          1\n",
      "4    part          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 16 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      3rd          1\n",
      "1   burner          1\n",
      "2  account          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 36 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0  americanstan          1\n",
      "1          many          1\n",
      "2       artists          1\n",
      "3          like          1\n",
      "4     extremely          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 24 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0         17          1\n",
      "1       wood          1\n",
      "2        elf          1\n",
      "3     ranger          1\n",
      "4  filmmaker          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   247          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 52 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "            token  frequency\n",
      "0              24          1\n",
      "1          london          1\n",
      "2         student          1\n",
      "3  theatreartsmdx          1\n",
      "4         shethey          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 14 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     sci          1\n",
      "1      fi          1\n",
      "2  horror          1\n",
      "3     guy          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "             token  frequency\n",
      "0  baskacherlovely          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 34 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      1436          1\n",
      "1        im          1\n",
      "2  disciple          1\n",
      "3     jesus          1\n",
      "4    christ          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 34 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0   niamhyeahjax          1\n",
      "1      instagram          1\n",
      "2  ratgirlratboi          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 97 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  australia          1\n",
      "1        531          1\n",
      "2       2452          1\n",
      "3       fyre          1\n",
      "4   festival          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 47 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0    social          1\n",
      "1  democrat          1\n",
      "2  progress          1\n",
      "3  security          1\n",
      "4    mostly          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 64 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0    thirties          1\n",
      "1     mothers          1\n",
      "2   daughters          1\n",
      "3  songwriter          1\n",
      "4     coowner          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 7 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  nothing          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 27 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0     juicy          1\n",
      "1   couture          1\n",
      "2  princess          1\n",
      "3   scorpio          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 26 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      nft          1\n",
      "1    hftwt          1\n",
      "2  couture          1\n",
      "3   addict          1\n",
      "4    aries          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 5 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  happy          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 39 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0      south          1\n",
      "1   broadway          1\n",
      "2  baltimore          1\n",
      "3         md          1\n",
      "4         15          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   hurry          1\n",
      "1  always          1\n",
      "2    late          1\n",
      "There are 15 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 124 characters in the data.\n",
      "The lexical diversity is 0.933 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0        student          2\n",
      "1            683          1\n",
      "2   religionuiuc          1\n",
      "3         theory          1\n",
      "4  ritualization          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 27 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  christian          1\n",
      "1   democrat          1\n",
      "2        mom          1\n",
      "3    grandma          1\n",
      "There are 13 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 66 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      467          1\n",
      "1      589          1\n",
      "2  creator          1\n",
      "3    black          1\n",
      "4    girls          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 79 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                     token  frequency\n",
      "0               keepitreal          1\n",
      "1              commonsense          1\n",
      "2                 feelings          1\n",
      "3  mediaistherealvirusdont          1\n",
      "4                     care          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 27 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0           instagram          1\n",
      "1  aangelinagutierrez          1\n",
      "There are 11 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 48 characters in the data.\n",
      "The lexical diversity is 0.818 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  working          2\n",
      "1      guy          2\n",
      "2      car          1\n",
      "3   better          1\n",
      "4    mebut          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 7 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  shehers          1\n",
      "There are 10 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 78 characters in the data.\n",
      "The lexical diversity is 0.900 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0    trading          2\n",
      "1  potential          1\n",
      "2      cover          1\n",
      "3      every          1\n",
      "4     aspect          1\n",
      "There are 16 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 87 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0        roll          1\n",
      "1  juggernaut          1\n",
      "2      writer          1\n",
      "3        muse          1\n",
      "4      animal          1\n",
      "There are 18 tokens in the data.\n",
      "There are 18 unique tokens in the data.\n",
      "There are 102 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  male          1\n",
      "1    35          1\n",
      "2   ace          1\n",
      "3   red          1\n",
      "4  wolf          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 7 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  teacher          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 99 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     utah          1\n",
      "1     1146          1\n",
      "2     1763          1\n",
      "3   family          1\n",
      "4  friends          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 41 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0                 236          1\n",
      "1           direttore          1\n",
      "2                  di          1\n",
      "3  httpstcoufl33umy7a          1\n",
      "4           fotografo          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 56 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  played          1\n",
      "1   rules          1\n",
      "2  worked          1\n",
      "3    hard          1\n",
      "4   stood          1\n",
      "There are 7 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 35 characters in the data.\n",
      "The lexical diversity is 0.857 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     lady          2\n",
      "1     mark          1\n",
      "2     nice          1\n",
      "3  meeting          1\n",
      "4   single          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 27 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0          607          1\n",
      "1          470          1\n",
      "2  mississippi          1\n",
      "3         born          1\n",
      "4       sheher          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 39 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     lady          1\n",
      "1     also          1\n",
      "2  kakashi          1\n",
      "3       mi          1\n",
      "4   esposo          1\n",
      "There are 16 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 99 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       564          1\n",
      "1     proud          1\n",
      "2  canadian          1\n",
      "3     loves          1\n",
      "4      sane          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 41 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0      laugh          1\n",
      "1    lovethe          1\n",
      "2      three          1\n",
      "3  important          1\n",
      "4      words          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0     w          1\n",
      "1    bf          1\n",
      "2     0          1\n",
      "3    25          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  love          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 9 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  knowledge          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 44 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0     services          1\n",
      "1  illustrator          1\n",
      "2        clash          1\n",
      "3       comics          1\n",
      "4         star          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 18 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0    former          1\n",
      "1  onetrick          1\n",
      "2      pony          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 54 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0     hobbies          1\n",
      "1     include          1\n",
      "2     reading          1\n",
      "3   financial          1\n",
      "4  statements          1\n",
      "There are 12 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 76 characters in the data.\n",
      "The lexical diversity is 0.917 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0      magic          2\n",
      "1  sometimes          1\n",
      "2      could          1\n",
      "3     always          1\n",
      "4        use          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 63 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      4970          1\n",
      "1     story          1\n",
      "2    seeker          1\n",
      "3      idea          1\n",
      "4  explorer          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   mom          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 17 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  anyone          1\n",
      "1     fan          1\n",
      "2      20          1\n",
      "3  sheher          1\n",
      "There are 9 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 68 characters in the data.\n",
      "The lexical diversity is 0.889 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  absolutely          2\n",
      "1     madness          1\n",
      "2      genius          1\n",
      "3      better          1\n",
      "4  ridiculous          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 1 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0     8          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 23 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      1442          1\n",
      "1     woman          1\n",
      "2  womaning          1\n",
      "3    sheher          1\n",
      "There are 17 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 86 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      tamu          1\n",
      "1        19          1\n",
      "2      make          1\n",
      "3  programs          1\n",
      "4     video          1\n",
      "There are 18 tokens in the data.\n",
      "There are 18 unique tokens in the data.\n",
      "There are 96 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0      91          1\n",
      "1     nfl          1\n",
      "2     nhl          1\n",
      "3     fan          1\n",
      "4  giants          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 35 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    marys          1\n",
      "1  student          1\n",
      "2    cross          1\n",
      "3  country          1\n",
      "4    track          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 44 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0   carpenter          1\n",
      "1    juventus          1\n",
      "2  religiongo          1\n",
      "3        habs          1\n",
      "4     goforza          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 115 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                           token  frequency\n",
      "0                            474          1\n",
      "1                       armchair          1\n",
      "2              psychologistbacon          1\n",
      "3                       lovercat          1\n",
      "4  whispererdeadheadpoetravenous          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  person          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   153          1\n",
      "There are 17 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 83 characters in the data.\n",
      "The lexical diversity is 0.882 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  account          2\n",
      "1       im          2\n",
      "2       17          1\n",
      "3      323          1\n",
      "4    hello          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 80 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     city          1\n",
      "1     doth          1\n",
      "2     like          1\n",
      "3  garment          1\n",
      "4     wear          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 13 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   266          1\n",
      "1  love          1\n",
      "2   dog          1\n",
      "3   man          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 35 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   burnt          1\n",
      "1  crispy          1\n",
      "2    bits          1\n",
      "3   chips          1\n",
      "4    fish          1\n",
      "There are 16 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 111 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     3908          1\n",
      "1      471          1\n",
      "2       im          1\n",
      "3     anba          1\n",
      "4  someone          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  batman          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 12 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0  bambinobecky          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 23 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0               maker          1\n",
      "1  httpstcomyxsnp2fpx          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 23 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       299          1\n",
      "1    animal          1\n",
      "2    rights          1\n",
      "3  advocate          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 47 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                 token  frequency\n",
      "0                 days          1\n",
      "1  1445380103161143304          1\n",
      "2              shethey          1\n",
      "3                  516          1\n",
      "4                  691          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 2 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0    ew          1\n",
      "There are 15 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 72 characters in the data.\n",
      "The lexical diversity is 0.933 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0      sf          2\n",
      "1     bay          1\n",
      "2    area          1\n",
      "3  native          1\n",
      "4   santa          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 100 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0          18459          1\n",
      "1          17911          1\n",
      "2  adoptdontshop          1\n",
      "3              3          1\n",
      "4          black          1\n",
      "There are 17 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 96 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       love          1\n",
      "1    country          1\n",
      "2  grandkids          1\n",
      "3       dogs          1\n",
      "4    husband          1\n",
      "There are 6 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 35 characters in the data.\n",
      "The lexical diversity is 0.667 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  washing          2\n",
      "1  clothes          2\n",
      "2    bitch          1\n",
      "3       im          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 77 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       202          1\n",
      "1       567          1\n",
      "2   english          1\n",
      "3       lit          1\n",
      "4  producer          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  stories          1\n",
      "1      end          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  sheher          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 37 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0         dog          1\n",
      "1  enthusiast          1\n",
      "2     science          1\n",
      "3        nerd          1\n",
      "4         nap          1\n",
      "There are 3 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 37 characters in the data.\n",
      "The lexical diversity is 0.667 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "             token  frequency\n",
      "0  bettiewoodqueen          2\n",
      "1          cashapp          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 70 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0       proud          1\n",
      "1   supporter          1\n",
      "2         msm          1\n",
      "3    moderate          1\n",
      "4  democratic          1\n",
      "There are 17 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 101 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  married          1\n",
      "1   trying          1\n",
      "2    raise          1\n",
      "3   decent          1\n",
      "4   caring          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 27 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0           easygoing          1\n",
      "1  httpstcoqpmgk6wvop          1\n",
      "There are 21 tokens in the data.\n",
      "There are 19 unique tokens in the data.\n",
      "There are 83 characters in the data.\n",
      "The lexical diversity is 0.905 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     que          2\n",
      "1      tu          2\n",
      "2     los          1\n",
      "3  huesos          1\n",
      "4     veo          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 34 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0         dr          1\n",
      "1     pepper          1\n",
      "2     addict          1\n",
      "3  capricorn          1\n",
      "4       estj          1\n",
      "There are 17 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 104 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      3537          1\n",
      "1   husband          1\n",
      "2  musician          1\n",
      "3    artist          1\n",
      "4  producer          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 11 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   think          1\n",
      "1  sneaky          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 36 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  pronouns          1\n",
      "1         7          1\n",
      "2       196          1\n",
      "3  obsessed          1\n",
      "4      drag          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 54 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0         reg          1\n",
      "1       nurse          1\n",
      "2     outdoor          1\n",
      "3  enthusiast          1\n",
      "4      health          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 24 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "             token  frequency\n",
      "0        cofounder          1\n",
      "1  passportvintage          1\n",
      "There are 15 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 75 characters in the data.\n",
      "The lexical diversity is 0.933 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   quit          2\n",
      "1    499          1\n",
      "2   1331          1\n",
      "3  focus          1\n",
      "4   life          1\n",
      "There are 16 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 90 characters in the data.\n",
      "The lexical diversity is 0.938 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0        paint          2\n",
      "1         love          1\n",
      "2    genealogy          1\n",
      "3  genealogist          1\n",
      "4          act          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 66 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  myfamily          1\n",
      "1      free          1\n",
      "2    spirit          1\n",
      "3  creative          1\n",
      "4   whiskey          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 33 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    love          1\n",
      "1  ariana          1\n",
      "2    abba          1\n",
      "3       n          1\n",
      "4   james          1\n",
      "There are 17 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 80 characters in the data.\n",
      "The lexical diversity is 0.941 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  books          2\n",
      "1   2460          1\n",
      "2    get          1\n",
      "3   paid          1\n",
      "4  write          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 7 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  beaches          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0    wu          1\n",
      "1    mt          1\n",
      "2    22          1\n",
      "3  tpwk          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 47 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0   snowflake          1\n",
      "1  registered          1\n",
      "2       nurse          1\n",
      "3        also          1\n",
      "4    referred          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 38 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0      81          1\n",
      "1  nothin          1\n",
      "2  really          1\n",
      "3    also          1\n",
      "4      go          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 18 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    butch          1\n",
      "1  lesbian          1\n",
      "2   sheher          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 75 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "            token  frequency\n",
      "0              29          1\n",
      "1           years          1\n",
      "2          dancer          1\n",
      "3         forever          1\n",
      "4  seattletomiami          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 30 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0        302          1\n",
      "1  asturiana          1\n",
      "2        fan          1\n",
      "3        nº1          1\n",
      "4        del          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 29 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0           129          1\n",
      "1  photographer          1\n",
      "2        people          1\n",
      "3      weddings          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 9 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  minnesota          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0     9          1\n",
      "1   311          1\n",
      "There are 17 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 93 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       red          1\n",
      "1       hat          1\n",
      "2     hater          1\n",
      "3     blues          1\n",
      "4  politics          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   south          1\n",
      "1  jersey          1\n",
      "2    girl          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 35 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  adultsized          1\n",
      "1         90s          1\n",
      "2         kid          1\n",
      "3       الحمد          1\n",
      "4        الله          1\n",
      "There are 13 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 67 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0   retired          1\n",
      "1      dead          1\n",
      "2  feminist          1\n",
      "3   grandma          1\n",
      "4         8          1\n",
      "There are 9 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 56 characters in the data.\n",
      "The lexical diversity is 0.889 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  everything          2\n",
      "1      boston          1\n",
      "2        soul          1\n",
      "3    baseball          1\n",
      "4       heart          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 16 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0     bi          1\n",
      "1     36          1\n",
      "2    232          1\n",
      "3  shout          1\n",
      "4   void          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 24 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                      token  frequency\n",
      "0  artistproducersongwriter          1\n",
      "There are 17 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 90 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     calc          1\n",
      "1   linear          1\n",
      "2      alg          1\n",
      "3  husband          1\n",
      "4     love          1\n",
      "There are 18 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 102 characters in the data.\n",
      "The lexical diversity is 0.833 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       tea          2\n",
      "1  festival          2\n",
      "2      film          2\n",
      "3   gardens          1\n",
      "4    pretty          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 57 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0         two          1\n",
      "1  adultsthat          1\n",
      "2        love          1\n",
      "3        life          1\n",
      "4    daughter          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 18 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   nana          1\n",
      "1   life          1\n",
      "2  coach          1\n",
      "3  nyfla          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 11 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    150          1\n",
      "1  clown          1\n",
      "2    era          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 71 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "              token  frequency\n",
      "0           justice          1\n",
      "1  environmentalist          1\n",
      "2             lbgtq          1\n",
      "3            higher          1\n",
      "4         education          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 27 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "             token  frequency\n",
      "0              769          1\n",
      "1              new          1\n",
      "2           yorker          1\n",
      "3  photojournalist          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 5 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0    38          1\n",
      "1   390          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 74 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0   family          1\n",
      "1  friends          1\n",
      "2  country          1\n",
      "3    fight          1\n",
      "4     help          1\n",
      "There are 19 tokens in the data.\n",
      "There are 19 unique tokens in the data.\n",
      "There are 100 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      988          1\n",
      "1     3394          1\n",
      "2     matt          1\n",
      "3  britton          1\n",
      "4  husband          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 33 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0    cantante          1\n",
      "1   argentina          1\n",
      "2  residiendo          1\n",
      "3          en          1\n",
      "4        cdmx          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 22 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0         23          1\n",
      "1    awkward          1\n",
      "2  extremely          1\n",
      "3       pale          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 114 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "              token  frequency\n",
      "0          business          1\n",
      "1  leadpractitioner          1\n",
      "2   skinnersacademy          1\n",
      "3           hackney          1\n",
      "4           trustee          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 45 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  chaotic          1\n",
      "1   public          1\n",
      "2    diary          1\n",
      "3     love          1\n",
      "4       rl          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 59 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      air          1\n",
      "1    force          1\n",
      "2  veteran          1\n",
      "3  retired          1\n",
      "4   postal          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 37 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0   librarian          1\n",
      "1  florentine          1\n",
      "2        wife          1\n",
      "3      mother          1\n",
      "4    educator          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 54 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  canvas          1\n",
      "1    make          1\n",
      "2   jpegs          1\n",
      "3    fine          1\n",
      "4  artist          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 17 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0   resister          1\n",
      "1  trumpland          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 37 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0                 280          1\n",
      "1  httpstco36bhixaam5          1\n",
      "2                  81          1\n",
      "3               years          1\n",
      "4                 old          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 13 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  voimaan          1\n",
      "1   pahoin          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 23 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  special          1\n",
      "1      one          1\n",
      "2     else          1\n",
      "3    quite          1\n",
      "4     like          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 32 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    away          1\n",
      "1      80          1\n",
      "2     304          1\n",
      "3  little          1\n",
      "4     bit          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 24 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      blm          1\n",
      "1  gorilla          1\n",
      "2     grip          1\n",
      "3  coochie          1\n",
      "4      htx          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 28 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    harry          1\n",
      "1   potter          1\n",
      "2  nirvana          1\n",
      "3   motley          1\n",
      "4     crue          1\n",
      "There are 16 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 113 characters in the data.\n",
      "The lexical diversity is 0.938 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0    chiefs          2\n",
      "1        46          1\n",
      "2  lifelong          1\n",
      "3       fan          1\n",
      "4      true          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 11 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0  awesomeness          1\n",
      "There are 19 tokens in the data.\n",
      "There are 19 unique tokens in the data.\n",
      "There are 107 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0       22332          1\n",
      "1        9266          1\n",
      "2       rnbsn          1\n",
      "3  asu_alumni          1\n",
      "4     liberal          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   get          1\n",
      "1   app          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 77 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0        773          1\n",
      "1       1497          1\n",
      "2        una          1\n",
      "3     mirada          1\n",
      "4  compasiva          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 58 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  healthcare          1\n",
      "1      worker          1\n",
      "2         cat          1\n",
      "3       owner          1\n",
      "4  gratefully          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 99 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  arctophile          1\n",
      "1       music          1\n",
      "2       lover          1\n",
      "3   collector          1\n",
      "4       bears          1\n",
      "There are 13 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 88 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  dreamworld          1\n",
      "1      petite          1\n",
      "2       pixie          1\n",
      "3  girlfriend          1\n",
      "4     booking          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 25 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0      master          1\n",
      "1         boo          1\n",
      "2  bitchcraft          1\n",
      "3      sheher          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 74 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  clothing          1\n",
      "1      line          1\n",
      "2   loading          1\n",
      "3    hebrew          1\n",
      "4      love          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      bias          1\n",
      "1       son          1\n",
      "2  junhyung          1\n",
      "There are 18 tokens in the data.\n",
      "There are 18 unique tokens in the data.\n",
      "There are 103 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0   always          1\n",
      "1       29          1\n",
      "2    think          1\n",
      "3  america          1\n",
      "4     lost          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 13 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   live          1\n",
      "1   love          1\n",
      "2  laugh          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 33 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "            token  frequency\n",
      "0         memetic          1\n",
      "1    epidemiology          1\n",
      "2  psychodynamics          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  sheher          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   old          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   nhs          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 24 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0    trying          1\n",
      "1      stay          1\n",
      "2  hydrated          1\n",
      "3    really          1\n",
      "There are 13 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 74 characters in the data.\n",
      "The lexical diversity is 0.923 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    proud          2\n",
      "1  chicago          1\n",
      "2      919          1\n",
      "3     2286          1\n",
      "4   trying          1\n",
      "There are 20 tokens in the data.\n",
      "There are 20 unique tokens in the data.\n",
      "There are 106 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      usa          1\n",
      "1       38          1\n",
      "2      111          1\n",
      "3    stand          1\n",
      "4  ukraine          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 32 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  simple          1\n",
      "1     man          1\n",
      "2  making          1\n",
      "3     way          1\n",
      "4    life          1\n",
      "There are 14 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 95 characters in the data.\n",
      "The lexical diversity is 0.929 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0   retired          2\n",
      "1    writer          1\n",
      "2  producer          1\n",
      "3     actor          1\n",
      "4       cop          1\n",
      "There are 10 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 52 characters in the data.\n",
      "The lexical diversity is 0.900 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0     loves          2\n",
      "1     lover          1\n",
      "2  humanity          1\n",
      "3    doxies          1\n",
      "4     story          1\n",
      "There are 13 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 69 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    pen          1\n",
      "1  15585          1\n",
      "2   6299          1\n",
      "3  wanna          1\n",
      "4    fly          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  warm          1\n",
      "1    em          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 81 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0           589          1\n",
      "1         sheep          1\n",
      "2  hetheynorsol          1\n",
      "3      jerrytwt          1\n",
      "4   belyfloptwt          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 69 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0    account          1\n",
      "1      often          1\n",
      "2  forgotten          1\n",
      "3      sixth          1\n",
      "4      spice          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 29 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      hate          1\n",
      "1      cats          1\n",
      "2     makes          1\n",
      "3     human          1\n",
      "4  roommate          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 92 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      3103          1\n",
      "1  strongly          1\n",
      "2   believe          1\n",
      "3  everyone          1\n",
      "4  deserves          1\n",
      "There are 15 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 90 characters in the data.\n",
      "The lexical diversity is 0.933 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0   please          2\n",
      "1  friends          1\n",
      "2     need          1\n",
      "3  support          1\n",
      "4      mom          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 113 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0     journalist          1\n",
      "1         writer          1\n",
      "2         editor          1\n",
      "3  hollywoodlife          1\n",
      "4       usweekly          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 73 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  journalist          1\n",
      "1    bbcworld          1\n",
      "2    formerly          1\n",
      "3   bbcbrasil          1\n",
      "4         sao          1\n",
      "There are 15 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 120 characters in the data.\n",
      "The lexical diversity is 0.933 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0          4          2\n",
      "1      lu281          1\n",
      "2      union          1\n",
      "3   activist          1\n",
      "4  chicagono          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 24 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0         dogs          1\n",
      "1    instagram          1\n",
      "2  jadee_white          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 14 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     544          1\n",
      "1  manage          1\n",
      "2   laugh          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 23 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0   living          1\n",
      "1      new          1\n",
      "2  zealand          1\n",
      "3      kia          1\n",
      "4     kaha          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 35 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       158          1\n",
      "1     never          1\n",
      "2   thought          1\n",
      "3        id          1\n",
      "4  actually          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 9 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  stream          1\n",
      "1     elo          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 44 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0  jimmieinnes          1\n",
      "1      waiting          1\n",
      "2         cake          1\n",
      "3      hairpin          1\n",
      "4       inside          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 33 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     bek          1\n",
      "1    todd          1\n",
      "2  alwynn          1\n",
      "3    mask          1\n",
      "4   youre          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 2 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0    34          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 36 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       grad          1\n",
      "1  certified          1\n",
      "2   licensed          1\n",
      "3   athletic          1\n",
      "4    trainer          1\n",
      "There are 18 tokens in the data.\n",
      "There are 18 unique tokens in the data.\n",
      "There are 87 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    girl          1\n",
      "1    love          1\n",
      "2    wine          1\n",
      "3  oolong          1\n",
      "4     tea          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  retweeting          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 12 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0  philadelphia          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 38 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0      theythem          1\n",
      "1      creative          1\n",
      "2  technologist          1\n",
      "3          make          1\n",
      "4        things          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 41 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0    theythem          1\n",
      "1          25          1\n",
      "2  antibinary          1\n",
      "3        drag          1\n",
      "4      artist          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 69 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0     mighty          1\n",
      "1      union          1\n",
      "2   movement          1\n",
      "3  melbourne          1\n",
      "4  australia          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 48 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    עורכת          1\n",
      "1    חדשות          1\n",
      "2   ומגישה          1\n",
      "3  n12news          1\n",
      "4     הוגת          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 25 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    dont          1\n",
      "1    pull          1\n",
      "2  ladder          1\n",
      "3  behind          1\n",
      "4   hehim          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 64 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0      radio          1\n",
      "1        gal          1\n",
      "2    middays          1\n",
      "3    cfoxvan          1\n",
      "4  sometimes          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 78 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  songwriter          1\n",
      "1       music          1\n",
      "2    producer          1\n",
      "3    engineer          1\n",
      "4          ig          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 58 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0    attorney          1\n",
      "1   political          1\n",
      "2      junkie          1\n",
      "3  consultant          1\n",
      "4  descendant          1\n",
      "There are 13 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 85 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                  token  frequency\n",
      "0                  1201          1\n",
      "1                  hope          1\n",
      "2            multimedia          1\n",
      "3  writerproducereditor          1\n",
      "4                  paid          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 13 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  kitty          1\n",
      "1    hes          1\n",
      "2  batty          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 41 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  university          1\n",
      "1   cambridge          1\n",
      "2       plant          1\n",
      "3     science          1\n",
      "4  department          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 53 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   hard          1\n",
      "1  worry          1\n",
      "2    way          1\n",
      "3   much          1\n",
      "4   want          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 87 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0    272579          1\n",
      "1      2030          1\n",
      "2      read          1\n",
      "3  customer          1\n",
      "4   service          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 33 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    soul          1\n",
      "1      19          1\n",
      "2     199          1\n",
      "3      20          1\n",
      "4  sheher          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 35 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   bunch          1\n",
      "1  things          1\n",
      "2    like          1\n",
      "3  follow          1\n",
      "4  mostly          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 8 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0      im          1\n",
      "1  trying          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 46 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0      53          1\n",
      "1     188          1\n",
      "2      16          1\n",
      "3    tpwk          1\n",
      "4  pretty          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 52 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0       leeeave          1\n",
      "1  ooooohwwwnnn          1\n",
      "2       mrhehim          1\n",
      "3            ig          1\n",
      "4        tiktok          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 1 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0     9          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 30 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0         22          1\n",
      "1        170          1\n",
      "2       turn          1\n",
      "3      radio          1\n",
      "4  listening          1\n",
      "There are 17 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 98 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0  independent          1\n",
      "1          alt          1\n",
      "2        rbpop          1\n",
      "3       artist          1\n",
      "4    realistic          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  hope          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  souls          1\n",
      "1     60          1\n",
      "2    274          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 52 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0      never          1\n",
      "1      timid          1\n",
      "2      enjoy          1\n",
      "3  worldlove          1\n",
      "4     sports          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 39 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0        1272          1\n",
      "1         mom          1\n",
      "2        wife          1\n",
      "3  registered          1\n",
      "4         dem          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 25 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  washington          1\n",
      "1       state          1\n",
      "2  leadership          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 24 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    video          1\n",
      "1    grrrl          1\n",
      "2   rhymes          1\n",
      "3        w          1\n",
      "4  messily          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 22 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  retiring          1\n",
      "1    foodie          1\n",
      "2       dog          1\n",
      "3     lover          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 82 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0        193          1\n",
      "1   husbands          1\n",
      "2    husband          1\n",
      "3  childrens          1\n",
      "4      daddy          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 113 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  studies          1\n",
      "1      phd          1\n",
      "2   global          1\n",
      "3  history          1\n",
      "4   womens          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 100 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0          mom          1\n",
      "1  grandmother          1\n",
      "2     democrat          1\n",
      "3     activist          1\n",
      "4    volunteer          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0     90          1\n",
      "1    437          1\n",
      "2     sé          1\n",
      "3    que          1\n",
      "4  poner          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 13 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  hehim          1\n",
      "1     22          1\n",
      "2    asd          1\n",
      "3    yeg          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  jeremy          1\n",
      "1  strong          1\n",
      "2     247          1\n",
      "There are 18 tokens in the data.\n",
      "There are 18 unique tokens in the data.\n",
      "There are 100 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0        cat          1\n",
      "1     herder          1\n",
      "2  emergency          1\n",
      "3    program          1\n",
      "4    manager          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 64 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0      sheher          1\n",
      "1         blm          1\n",
      "2      social          1\n",
      "3       media          1\n",
      "4  dominatrix          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 46 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0            fl          1\n",
      "1         music          1\n",
      "2  architecture          1\n",
      "3           art          1\n",
      "4   progressive          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 35 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "             token  frequency\n",
      "0               18          1\n",
      "1              198          1\n",
      "2  unhrcaustralian          1\n",
      "3          penguin          1\n",
      "4         delegate          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 53 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0      father          1\n",
      "1      writer          1\n",
      "2    musician          1\n",
      "3  naturalist          1\n",
      "4    rational          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 11 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     811          1\n",
      "1      27          1\n",
      "2  sheher          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 22 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0   retweets          1\n",
      "1  expressed          1\n",
      "2      views          1\n",
      "There are 16 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 90 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0          562          1\n",
      "1     chemicaw          1\n",
      "2      womance          1\n",
      "3     commonwy          1\n",
      "4  abbweviated          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 17 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  girlfriend          1\n",
      "1          im          1\n",
      "2       bored          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 47 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    tengo          1\n",
      "1     ojos          1\n",
      "2  grandes          1\n",
      "3       pq          1\n",
      "4      soy          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 16 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0    tonight          1\n",
      "1  ukrainian          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 16 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  dont          1\n",
      "1  care          1\n",
      "2  aint          1\n",
      "3  lost          1\n",
      "There are 20 tokens in the data.\n",
      "There are 20 unique tokens in the data.\n",
      "There are 110 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0          222          1\n",
      "1            3          1\n",
      "2          fur          1\n",
      "3       babies          1\n",
      "4  westvillenj          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  came          1\n",
      "There are 9 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 61 characters in the data.\n",
      "The lexical diversity is 0.889 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0     bigger          2\n",
      "1       goal          1\n",
      "2   setbacks          1\n",
      "3       stay          1\n",
      "4  resilient          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 25 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0          loud          1\n",
      "1         funny          1\n",
      "2          much          1\n",
      "3  landonslipke          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 69 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0   nature          1\n",
      "1       tv          1\n",
      "2    shows          1\n",
      "3       og          1\n",
      "4  sweetee          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 16 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       cute          1\n",
      "1        cat          1\n",
      "2  privilege          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 60 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      ela          1\n",
      "1  teacher          1\n",
      "2       ed          1\n",
      "3   techie          1\n",
      "4    world          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 118 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0      living          1\n",
      "1       lands          1\n",
      "2   landscape          1\n",
      "3      design          1\n",
      "4  consultant          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 13 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       love          1\n",
      "1  athletics          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 36 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0        14          1\n",
      "1   burning          1\n",
      "2   lentils          1\n",
      "3  tweeting          1\n",
      "4     every          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 54 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      419          1\n",
      "1      671          1\n",
      "2    wanna          1\n",
      "3  defined          1\n",
      "4   things          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 20 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     410          1\n",
      "1   drink          1\n",
      "2  bloody          1\n",
      "3  coffee          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 26 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0         way          1\n",
      "1  worldscifi          1\n",
      "2      horror          1\n",
      "3     fantasy          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 26 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     isnt          1\n",
      "1  touting          1\n",
      "2    tired          1\n",
      "3   failed          1\n",
      "4     isms          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 83 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       1212          1\n",
      "1       1108          1\n",
      "2        mph          1\n",
      "3    disease          1\n",
      "4  detective          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 35 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   know          1\n",
      "1   sure          1\n",
      "2   give          1\n",
      "3  comes          1\n",
      "4   back          1\n",
      "There are 4 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 0.750 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    bad          2\n",
      "1  youre          1\n",
      "2   good          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 18 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0         wife          1\n",
      "1          mom          1\n",
      "2  grandmother          1\n",
      "There are 17 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 127 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0          10766          1\n",
      "1          11837          1\n",
      "2     nomandates          1\n",
      "3  freedomconvoy          1\n",
      "4            gyo          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 13 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   keep          1\n",
      "1   head          1\n",
      "2  water          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 49 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  learning          1\n",
      "1   support          1\n",
      "2  services          1\n",
      "3     poway          1\n",
      "4   unified          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 59 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    glass          1\n",
      "1  strings          1\n",
      "2      old          1\n",
      "3   growly          1\n",
      "4     best          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 42 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0      human          1\n",
      "1      woman          1\n",
      "2   engineer          1\n",
      "3    respond          1\n",
      "4  anonymous          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 67 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0           389          1\n",
      "1      theythem          1\n",
      "2  nevertheless          1\n",
      "3     persisted          1\n",
      "4         level          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 22 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   know          1\n",
      "1  worth          1\n",
      "2   also          1\n",
      "3  smoke          1\n",
      "4   weed          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 32 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   dont          1\n",
      "1   come          1\n",
      "2  often          1\n",
      "3   sure          1\n",
      "4    ill          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       priv          1\n",
      "1  nvrblikeu          1\n",
      "2         18          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 44 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0    remodels          1\n",
      "1     general          1\n",
      "2  contractor          1\n",
      "3       based          1\n",
      "4      denver          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 14 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "            token  frequency\n",
      "0  brookeporter53          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 53 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0      writer          1\n",
      "1      mayhem          1\n",
      "2     library          1\n",
      "3       lover          1\n",
      "4  insatiable          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 14 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "            token  frequency\n",
      "0  obatalaeyiogbe          1\n",
      "There are 17 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 98 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0         876          1\n",
      "1         193          1\n",
      "2     actress          1\n",
      "3        poet          1\n",
      "4  marathoner          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 9 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     wig          1\n",
      "1  sheher          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 66 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0      security          1\n",
      "1          guru          1\n",
      "2          avid          1\n",
      "3  photographer          1\n",
      "4         coder          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 42 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0        18          1\n",
      "1       gay          1\n",
      "2    single          1\n",
      "3   english          1\n",
      "4  studying          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 33 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0  httpstcorcmh1tcnrn          1\n",
      "1               phone          1\n",
      "2                 366          1\n",
      "3                 978          1\n",
      "4                2958          1\n",
      "There are 11 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 69 characters in the data.\n",
      "The lexical diversity is 0.818 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0    writer          3\n",
      "1   cymraes          1\n",
      "2  parttime          1\n",
      "3  londoner          1\n",
      "4  polyglot          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  life          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 18 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0    complain          1\n",
      "1  patriarchy          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 12 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0        soy          1\n",
      "1  rebeldeﾟﾟ          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 98 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      100          1\n",
      "1  phoenix          1\n",
      "2   rising          1\n",
      "3    ashes          1\n",
      "4     onto          1\n",
      "There are 7 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 33 characters in the data.\n",
      "The lexical diversity is 0.857 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      love          2\n",
      "1      cant          1\n",
      "2      hell          1\n",
      "3     gonna          1\n",
      "4  somebody          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 43 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0      pacific          1\n",
      "1           nw          1\n",
      "2       native          1\n",
      "3  progressive          1\n",
      "4       follow          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 9 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     160          1\n",
      "1  making          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 7 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   541          1\n",
      "1  mtbd          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 32 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  fellowship          1\n",
      "1     friends          1\n",
      "2      family          1\n",
      "3       loved          1\n",
      "4        ones          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 12 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   make          1\n",
      "1  today          1\n",
      "2    day          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 68 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   new          1\n",
      "1  york          1\n",
      "2  city          1\n",
      "3     3          1\n",
      "4    26          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 5 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  watch          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 24 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0        home          1\n",
      "1  automation          1\n",
      "2  programmer          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 28 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  tweeting          1\n",
      "1     every          1\n",
      "2  negative          1\n",
      "3   thought          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 17 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0      81          1\n",
      "1   molly          1\n",
      "2  danger          1\n",
      "3    girl          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 34 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0     50          1\n",
      "1    221          1\n",
      "2  wanna          1\n",
      "3    tea          1\n",
      "4   shop          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 20 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  sheher          1\n",
      "1     leo          1\n",
      "2  gemini          1\n",
      "3   virgo          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 18 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  cachorros          1\n",
      "1       luis          1\n",
      "2      fonsi          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 45 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   bored          1\n",
      "1       n          1\n",
      "2  lonely          1\n",
      "3  really          1\n",
      "4    need          1\n",
      "There are 17 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 105 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       1022          1\n",
      "1     sheher          1\n",
      "2  paralegal          1\n",
      "3     living          1\n",
      "4    unceded          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 23 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0     truth          1\n",
      "1  kindness          1\n",
      "2      love          1\n",
      "3    action          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  mantra          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 9 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   seek          1\n",
      "1  truth          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 21 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0        say          1\n",
      "1  something          1\n",
      "2       real          1\n",
      "3      quick          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 25 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0       maman          1\n",
      "1          de          1\n",
      "2           3          1\n",
      "3  magnifique          1\n",
      "4     enfants          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 5 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  human          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 13 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  friends          1\n",
      "1   family          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 18 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   work          1\n",
      "1   free          1\n",
      "2  smoke          1\n",
      "3  place          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 35 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       soul          1\n",
      "1       sees          1\n",
      "2     beauty          1\n",
      "3        may          1\n",
      "4  sometimes          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 26 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  passionate          1\n",
      "1    learning          1\n",
      "2        mnps          1\n",
      "3        star          1\n",
      "There are 17 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 100 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0          2346          1\n",
      "1       twitter          1\n",
      "2       fascist          1\n",
      "3     communist          1\n",
      "4  organization          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 9 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  melodrama          1\n",
      "There are 16 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 110 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "            token  frequency\n",
      "0           autor          1\n",
      "1      übersetzer          1\n",
      "2   synchronautor          1\n",
      "3          berlin          1\n",
      "4  prenzlschwäbin          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 27 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0          find          1\n",
      "1        reason          1\n",
      "2  unreasonable          1\n",
      "3         world          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 42 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0   listening          1\n",
      "1  interested          1\n",
      "2        also          1\n",
      "3         fyi          1\n",
      "4        dont          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 8 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   bac          1\n",
      "1   lax          1\n",
      "2    19          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 25 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   good          1\n",
      "1  laugh          1\n",
      "2   dont          1\n",
      "3   take          1\n",
      "4   life          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 23 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       yes          1\n",
      "1      join          1\n",
      "2      crab          1\n",
      "3      cult          1\n",
      "4  facebook          1\n",
      "There are 15 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 91 characters in the data.\n",
      "The lexical diversity is 0.933 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    myers          2\n",
      "1       fl          1\n",
      "2        0          1\n",
      "3        8          1\n",
      "4  laurels          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 27 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  advocate          1\n",
      "1   justice          1\n",
      "2    foodie          1\n",
      "3       cat          1\n",
      "4       mom          1\n",
      "There are 18 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 113 characters in the data.\n",
      "The lexical diversity is 0.944 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    twins          2\n",
      "1  capital          1\n",
      "2      brd          1\n",
      "3   member          1\n",
      "4     love          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 38 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     call          1\n",
      "1  chizzle          1\n",
      "2      cuz          1\n",
      "3       im          1\n",
      "4   harder          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  help          1\n",
      "There are 16 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 90 characters in the data.\n",
      "The lexical diversity is 0.875 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "            token  frequency\n",
      "0             ray          2\n",
      "1            bans          2\n",
      "2        formerly          1\n",
      "3           known          1\n",
      "4  bluthmodelhome          1\n",
      "There are 7 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 34 characters in the data.\n",
      "The lexical diversity is 0.857 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    like          2\n",
      "1   write          1\n",
      "2  things          1\n",
      "3    dont          1\n",
      "4    talk          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 78 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0      charm          1\n",
      "1  rejoining          1\n",
      "2        3rd          1\n",
      "3       time          1\n",
      "4       yrbk          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 7 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  real          1\n",
      "1   day          1\n",
      "There are 12 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 94 characters in the data.\n",
      "The lexical diversity is 0.917 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0            pinklash          2\n",
      "1                  ig          1\n",
      "2         pinklash_cb          1\n",
      "3            facebook          1\n",
      "4  httpstcoweipxfzlkq          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 16 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     726          1\n",
      "1  pastry          1\n",
      "2    chef          1\n",
      "3     mom          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 11 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0      15          1\n",
      "1     226          1\n",
      "2  sheher          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  ourple          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 24 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     beer          1\n",
      "1     nerd          1\n",
      "2      bit          1\n",
      "3  rubbish          1\n",
      "4   tennis          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 92 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0   teacher          1\n",
      "1     books          1\n",
      "2  uniquely          1\n",
      "3  portable          1\n",
      "4     magic          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 30 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0        gay          1\n",
      "1    witches          1\n",
      "2      gotta          1\n",
      "3  favourite          1\n",
      "4     gender          1\n",
      "There are 18 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 87 characters in the data.\n",
      "The lexical diversity is 0.889 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0       today          2\n",
      "1        gods          2\n",
      "2        gift          1\n",
      "3  appreciate          1\n",
      "4       learn          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 51 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                   token  frequency\n",
      "0               enjoying          1\n",
      "1                   time          1\n",
      "2                husband          1\n",
      "3                   love          1\n",
      "4  dogsreadingneedlework          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 7 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  clothes          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 8 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  right          1\n",
      "1    eye          1\n",
      "There are 16 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 100 characters in the data.\n",
      "The lexical diversity is 0.938 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0            im          2\n",
      "1   superlurker          1\n",
      "2           mom          1\n",
      "3  selfemployed          1\n",
      "4      gardener          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 29 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  software          1\n",
      "1  engineer          1\n",
      "2  investor          1\n",
      "3     gamer          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 26 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0       52          1\n",
      "1     ever          1\n",
      "2    doubt          1\n",
      "3  ability          1\n",
      "4     find          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 7 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  shopify          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 116 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0        1406          1\n",
      "1   community          1\n",
      "2  engagement          1\n",
      "3  strategist          1\n",
      "4        talk          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 9 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   soul          1\n",
      "1  quake          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 48 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       791          1\n",
      "1      1332          1\n",
      "2  politics          1\n",
      "3        ir          1\n",
      "4   student          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 27 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       cats          1\n",
      "1       life          1\n",
      "2  certified          1\n",
      "3       shit          1\n",
      "4     talker          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 78 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0   reject          1\n",
      "1     food          1\n",
      "2   ignore          1\n",
      "3  customs          1\n",
      "4     fear          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 85 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                     token  frequency\n",
      "0                      251          1\n",
      "1                     1486          1\n",
      "2  httpstcoocxjht62l2video          1\n",
      "3                   editor          1\n",
      "4                 producer          1\n",
      "There are 20 tokens in the data.\n",
      "There are 19 unique tokens in the data.\n",
      "There are 114 characters in the data.\n",
      "The lexical diversity is 0.950 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  levels          2\n",
      "1    2349          1\n",
      "2    born          1\n",
      "3   31764          1\n",
      "4  418420          1\n",
      "There are 17 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 94 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0        im          1\n",
      "1   lifting          1\n",
      "2   weights          1\n",
      "3  spending          1\n",
      "4      time          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 68 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                               token  frequency\n",
      "0  introvertanimegamerfoodietraveler          1\n",
      "1                          collected          1\n",
      "2                              every          1\n",
      "3                              stone          1\n",
      "4                             thrown          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 20 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      gets          1\n",
      "1  prettier          1\n",
      "2     every          1\n",
      "3       day          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  friend          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 41 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0        96          1\n",
      "1        20          1\n",
      "2  tweeting          1\n",
      "3     every          1\n",
      "4   thought          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 11 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0      28          1\n",
      "1     101          1\n",
      "2  movies          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 105 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       real          1\n",
      "1  patriotic          1\n",
      "2   american          1\n",
      "3        way          1\n",
      "4   lifelong          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 74 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0  vantandenburg          1\n",
      "1        average          1\n",
      "2          basic          1\n",
      "3         raging          1\n",
      "4       feminist          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 23 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0       im          1\n",
      "1    bored          1\n",
      "2   sheher          1\n",
      "3  goavsgo          1\n",
      "4      blm          1\n",
      "There are 19 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 112 characters in the data.\n",
      "The lexical diversity is 0.895 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0        fibromyalgia          2\n",
      "1                real          2\n",
      "2  832665845922832384          1\n",
      "3              canada          1\n",
      "4                7441          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 85 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0         3113          1\n",
      "1    publicist          1\n",
      "2      problem          1\n",
      "3       solver          1\n",
      "4  rocknroller          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 22 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0      figment          1\n",
      "1  imagination          1\n",
      "2           23          1\n",
      "3           uk          1\n",
      "There are 12 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 76 characters in the data.\n",
      "The lexical diversity is 0.917 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0           get          2\n",
      "1  professional          1\n",
      "2        leader          1\n",
      "3    healthcare          1\n",
      "4       fitness          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 16 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       idk          1\n",
      "1       man          1\n",
      "2        im          1\n",
      "3  existing          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 30 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  dabbler          1\n",
      "1     plus          1\n",
      "2       ou          1\n",
      "3    moins          1\n",
      "4    слава          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 53 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  angeles          1\n",
      "1       ca          1\n",
      "2       15          1\n",
      "3      100          1\n",
      "4   theyhe          1\n",
      "There are 16 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 98 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   dont          1\n",
      "1   want          1\n",
      "2  visit          1\n",
      "3    643          1\n",
      "4    994          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 69 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0            951          1\n",
      "1            482          1\n",
      "2  thalassophile          1\n",
      "3       parttime          1\n",
      "4       aspiring          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 41 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  landscaper          1\n",
      "1        pool          1\n",
      "2     shooter          1\n",
      "3         dad          1\n",
      "4      health          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 16 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0      sheher          1\n",
      "1  vaxedwaxed          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 44 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  jackson          1\n",
      "1   stevie          1\n",
      "2   wonder          1\n",
      "3     stan          1\n",
      "4     wall          1\n",
      "There are 15 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 75 characters in the data.\n",
      "The lexical diversity is 0.933 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       code          2\n",
      "1  temporary          1\n",
      "2       love          1\n",
      "3         im          1\n",
      "4     fucker          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 5 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0    68          1\n",
      "1   179          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 54 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0      nonsense          1\n",
      "1        mostly          1\n",
      "2          film          1\n",
      "3  tuscanybased          1\n",
      "4    translator          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 36 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0        people          1\n",
      "1  northwestern          1\n",
      "2    university          1\n",
      "3      pritzker          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 34 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0          hons          1\n",
      "1         amsrb          1\n",
      "2         ocean          1\n",
      "3      wildlife          1\n",
      "4  photographer          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 13 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       free          1\n",
      "1  palestine          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 7 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  mother          1\n",
      "1       3          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 88 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      love          1\n",
      "1  politics          1\n",
      "2    golden          1\n",
      "3     state          1\n",
      "4  warriors          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   286          1\n",
      "There are 13 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 44 characters in the data.\n",
      "The lexical diversity is 0.846 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0           𝓮          3\n",
      "1           𝓡          1\n",
      "2           𝔂          1\n",
      "3           𝓼          1\n",
      "4  2246430780          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 118 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                           token  frequency\n",
      "0                            182          1\n",
      "1                            932          1\n",
      "2  artistmusicianidiottechnician          1\n",
      "3                    philosopher          1\n",
      "4                          gamer          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 62 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  vampire          1\n",
      "1   slayer          1\n",
      "2    angel          1\n",
      "3   series          1\n",
      "4   sheher          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 55 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0         211          1\n",
      "1       wanna          1\n",
      "2  experience          1\n",
      "3        epic          1\n",
      "4       highs          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 21 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  comfort          1\n",
      "1  digital          1\n",
      "2    world          1\n",
      "3       23          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  enthusiast          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  proud          1\n",
      "1  balls          1\n",
      "2  haver          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  news          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 7 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   345          1\n",
      "1  2088          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  wrong          1\n",
      "1  bitch          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 120 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "             token  frequency\n",
      "0             1272          1\n",
      "1             born          1\n",
      "2          glasgow          1\n",
      "3    whballardnews          1\n",
      "4  churchill_hwdsb          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 87 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "            token  frequency\n",
      "0             mom          1\n",
      "1  virgogeminisag          1\n",
      "2          vocals          1\n",
      "3         special          1\n",
      "4           event          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 32 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0        wife          1\n",
      "1        boho          1\n",
      "2         fur          1\n",
      "3      babies          1\n",
      "4  optimistic          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 55 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0      39          1\n",
      "1      42          1\n",
      "2     ive          1\n",
      "3  turned          1\n",
      "4    hard          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 83 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0       pisces          1\n",
      "1  telecomtech          1\n",
      "2       policy          1\n",
      "3         ctia          1\n",
      "4         alum          1\n",
      "There are 15 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 67 characters in the data.\n",
      "The lexical diversity is 0.933 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0     im          2\n",
      "1  music          1\n",
      "2      2          1\n",
      "3   cute          1\n",
      "4   cats          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 11 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    693          1\n",
      "1    big          1\n",
      "2  daddy          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 57 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  blonde          1\n",
      "1    hair          1\n",
      "2    blue          1\n",
      "3   eyesi          1\n",
      "4    love          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 66 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  louisiana          1\n",
      "1        usa          1\n",
      "2        160          1\n",
      "3       1566          1\n",
      "4        lil          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 2 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0    22          1\n",
      "There are 16 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 94 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  theythem          1\n",
      "1       blm          1\n",
      "2    writer          1\n",
      "3  reporter          1\n",
      "4       hip          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 19 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0           96          1\n",
      "1       tiktok          1\n",
      "2  kimberrnach          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 11 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   fancy          1\n",
      "1  seeing          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 27 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       247          1\n",
      "1     bored          1\n",
      "2     binge          1\n",
      "3  watching          1\n",
      "4    movies          1\n",
      "There are 16 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 98 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0      proud          1\n",
      "1        mom          1\n",
      "2   daughter          1\n",
      "3     rocket          1\n",
      "4  scientist          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 13 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0   always          1\n",
      "1  nothing          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 9 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   kind          1\n",
      "1  heart          1\n",
      "There are 16 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 87 characters in the data.\n",
      "The lexical diversity is 0.938 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  writing          2\n",
      "1     poet          1\n",
      "2     word          1\n",
      "3    witch          1\n",
      "4   cohost          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 11 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  world          1\n",
      "1    107          1\n",
      "2    166          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 36 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  maternity          1\n",
      "1       ward          1\n",
      "2   standing          1\n",
      "3  democracy          1\n",
      "4     canada          1\n",
      "There are 10 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 79 characters in the data.\n",
      "The lexical diversity is 0.900 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  specialist          2\n",
      "1    strategy          1\n",
      "2  innovation          1\n",
      "3     climate          1\n",
      "4         erp          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 31 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       6175          1\n",
      "1     health          1\n",
      "2   medicine          1\n",
      "3  happylife          1\n",
      "4       love          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 31 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  satisfied          1\n",
      "1    husband          1\n",
      "2      mixed          1\n",
      "3       race          1\n",
      "4     couple          1\n",
      "There are 6 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 29 characters in the data.\n",
      "The lexical diversity is 0.833 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    like          2\n",
      "1   arrow          1\n",
      "2   fruit          1\n",
      "3   flies          1\n",
      "4  banana          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 74 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0         898          1\n",
      "1  collective          1\n",
      "2       space          1\n",
      "3        nfts          1\n",
      "4  exhibiting          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 76 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0           577          1\n",
      "1       retired          1\n",
      "2        ffemtp          1\n",
      "3       30years          1\n",
      "4  intermittent          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 36 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0     stuff          1\n",
      "1    others          1\n",
      "2  pronouns          1\n",
      "3      thee          1\n",
      "4      thou          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   meme          1\n",
      "1  qveen          1\n",
      "2    aka          1\n",
      "3    ace          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 77 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "             token  frequency\n",
      "0          ukraine          1\n",
      "1         resister          1\n",
      "2           sister          1\n",
      "3  bidenharris2020          1\n",
      "4              blm          1\n",
      "There are 17 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 105 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  10453          1\n",
      "1   1131          1\n",
      "2  kinda          1\n",
      "3    hot          1\n",
      "4  funny          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 29 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0        0          1\n",
      "1      127          1\n",
      "2  gracias          1\n",
      "3      por          1\n",
      "4  revisar          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 47 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0  transgender          1\n",
      "1          pnw          1\n",
      "2      gayming          1\n",
      "3      watches          1\n",
      "4     horology          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 74 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "            token  frequency\n",
      "0       committed          1\n",
      "1        creating          1\n",
      "2        ultimate          1\n",
      "3  celebritytofan          1\n",
      "4      engagement          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 23 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    hehim          1\n",
      "1  account          1\n",
      "2     stan          1\n",
      "3  dualipa          1\n",
      "There are 18 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 80 characters in the data.\n",
      "The lexical diversity is 0.944 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0           go          2\n",
      "1           67          1\n",
      "2        years          1\n",
      "3          old          1\n",
      "4  transgender          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   makes          1\n",
      "1    life          1\n",
      "2  funner          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 51 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "              token  frequency\n",
      "0  kobycampbellsoup          1\n",
      "1            roblox          1\n",
      "2              like          1\n",
      "3              star          1\n",
      "4              wars          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 106 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       act          1\n",
      "1  kindness          1\n",
      "2    matter          1\n",
      "3     small          1\n",
      "4      ever          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 37 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0         cook          1\n",
      "1  housekeeper          1\n",
      "2          445          1\n",
      "3        women          1\n",
      "4         dont          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 102 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0          817          1\n",
      "1           tv          1\n",
      "2   enthusiast          1\n",
      "3  campaigning          1\n",
      "4         mike          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 7 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  warlock          1\n",
      "There are 10 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 52 characters in the data.\n",
      "The lexical diversity is 0.900 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      love          2\n",
      "1     nurse          1\n",
      "2  democrat          1\n",
      "3    resist          1\n",
      "4       blm          1\n",
      "There are 12 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 89 characters in the data.\n",
      "The lexical diversity is 0.917 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0         also          2\n",
      "1  steelworker          1\n",
      "2      atheist          1\n",
      "3    heathenim          1\n",
      "4     cannabis          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 38 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0       im          1\n",
      "1   spunky          1\n",
      "2     like          1\n",
      "3  oatmeal          1\n",
      "4    lumpy          1\n",
      "There are 19 tokens in the data.\n",
      "There are 19 unique tokens in the data.\n",
      "There are 113 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  politeness          1\n",
      "1         694          1\n",
      "2        4588          1\n",
      "3       happy          1\n",
      "4     twitter          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 72 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0   scrolls          1\n",
      "1    online          1\n",
      "2       fan          1\n",
      "3  believer          1\n",
      "4   justice          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  manchester          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 27 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  sailormoon          1\n",
      "1        baby          1\n",
      "2       spice          1\n",
      "3    daughter          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 70 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0       lover          1\n",
      "1  technology          1\n",
      "2       freak          1\n",
      "3       music          1\n",
      "4      junkie          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 32 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "              token  frequency\n",
      "0  elinorcoakley123          1\n",
      "1            vinted          1\n",
      "2        elinorcoak          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 12 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  writer          1\n",
      "1  nytmag          1\n",
      "There are 19 tokens in the data.\n",
      "There are 19 unique tokens in the data.\n",
      "There are 116 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  married          1\n",
      "1      mom          1\n",
      "2    three          1\n",
      "3    girls          1\n",
      "4    loves          1\n",
      "There are 15 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 89 characters in the data.\n",
      "The lexical diversity is 0.867 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     last          2\n",
      "1  captain          2\n",
      "2     1561          1\n",
      "3     make          1\n",
      "4   movies          1\n",
      "There are 19 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 115 characters in the data.\n",
      "The lexical diversity is 0.789 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0     lives          3\n",
      "1    matter          3\n",
      "2        15          1\n",
      "3       208          1\n",
      "4  theythem          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 19 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     girl          1\n",
      "1  jariana          1\n",
      "2     stan          1\n",
      "3     hfmb          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 9 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  could          1\n",
      "1   stan          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   206          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   tbh          1\n",
      "There are 11 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 81 characters in the data.\n",
      "The lexical diversity is 0.909 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0      health          2\n",
      "1     account          1\n",
      "2         ms3          1\n",
      "3  interested          1\n",
      "4    scalpels          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 44 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0      womens          1\n",
      "1      gender          1\n",
      "2   sexuality          1\n",
      "3     studies          1\n",
      "4  university          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 36 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  sugartown          1\n",
      "1      lover          1\n",
      "2    married          1\n",
      "3       best          1\n",
      "4        guy          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 32 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  religious          1\n",
      "1  extremist          1\n",
      "2   autistic          1\n",
      "3     savant          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 12 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   soon          1\n",
      "1    mrs          1\n",
      "2  chinn          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 45 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "               token  frequency\n",
      "0         orthopedic          1\n",
      "1            surgeon          1\n",
      "2  orthopedicsurgeon          1\n",
      "3        animallover          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 20 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0         gamer          1\n",
      "1           boy          1\n",
      "2  denkusdeluxe          1\n",
      "There are 16 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 115 characters in the data.\n",
      "The lexical diversity is 0.938 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0             dj          2\n",
      "1  international          1\n",
      "2         artist          1\n",
      "3      relations          1\n",
      "4        izotope          1\n",
      "There are 16 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 93 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  content          1\n",
      "1  creator          1\n",
      "2    gamer          1\n",
      "3    model          1\n",
      "4       10          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 71 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  shetheyboo          1\n",
      "1         334          1\n",
      "2        3380          1\n",
      "3         eat          1\n",
      "4        dirt          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 41 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "             token  frequency\n",
      "0  behavioristavid          1\n",
      "1      readermusic          1\n",
      "2         junkieon          1\n",
      "3          journey          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 55 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0      retired          1\n",
      "1         wife          1\n",
      "2  grandmother          1\n",
      "3       animal          1\n",
      "4        lover          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 25 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   back          1\n",
      "1   lick          1\n",
      "2  pussy          1\n",
      "3    say          1\n",
      "4  crack          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 54 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  teacher          1\n",
      "1    bucks          1\n",
      "2      fan          1\n",
      "3   guitar          1\n",
      "4   player          1\n",
      "There are 16 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 106 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      4971          1\n",
      "1    artist          1\n",
      "2  painting          1\n",
      "3     style          1\n",
      "4    called          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     龍宮寺          1\n",
      "1       堅          1\n",
      "2  𖥧𖤣𖡼𖥧𖤣𖡼          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  soul          1\n",
      "There are 9 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 26 characters in the data.\n",
      "The lexical diversity is 0.667 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0      u          3\n",
      "1  femme          2\n",
      "2      4          1\n",
      "3  think          1\n",
      "4    kno          1\n",
      "There are 20 tokens in the data.\n",
      "There are 20 unique tokens in the data.\n",
      "There are 98 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0         151          1\n",
      "1       spicy          1\n",
      "2     italian          1\n",
      "3        iowa          1\n",
      "4  journalist          1\n",
      "There are 16 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 104 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "               token  frequency\n",
      "0             former          1\n",
      "1  newstrafficsports          1\n",
      "2                guy          1\n",
      "3               wtop          1\n",
      "4               cbs3          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 40 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   actor          1\n",
      "1  writer          1\n",
      "2    real          1\n",
      "3    life          1\n",
      "4    mean          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 87 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0   movement          1\n",
      "1     actors          1\n",
      "2     mother          1\n",
      "3          2          1\n",
      "4  brilliant          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 16 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  beach          1\n",
      "1   beer          1\n",
      "2     im          1\n",
      "3  happy          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   blm          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 5 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  birds          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 24 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0          leo          1\n",
      "1  independent          1\n",
      "2          kim          1\n",
      "3            k          1\n",
      "4       family          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 22 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  musician          1\n",
      "1    trying          1\n",
      "2      make          1\n",
      "3      mark          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       254          1\n",
      "1  happened          1\n",
      "2      life          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  bodies          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 2 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0    56          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 5 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  phase          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 20 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       324          1\n",
      "1       745          1\n",
      "2  lonesome          1\n",
      "3    cowboy          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 16 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     love          1\n",
      "1  country          1\n",
      "2    music          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 9 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  marine          1\n",
      "1     mom          1\n",
      "There are 15 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 103 characters in the data.\n",
      "The lexical diversity is 0.733 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  register          3\n",
      "1      vote          3\n",
      "2       304          1\n",
      "3   friends          1\n",
      "4    family          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 37 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  resistance          1\n",
      "1    resistor          1\n",
      "2       awohl          1\n",
      "3        life          1\n",
      "4       crazy          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 13 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0       sc          1\n",
      "1  glira91          1\n",
      "2       ca          1\n",
      "3       co          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   190          1\n",
      "There are 17 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 88 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                 token  frequency\n",
      "0  1362876575968993281          1\n",
      "1                  san          1\n",
      "2              antonio          1\n",
      "3                   tx          1\n",
      "4                   38          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   ʸᵘʰ          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 38 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    536          1\n",
      "1    ask          1\n",
      "2   many          1\n",
      "3  times          1\n",
      "4  cried          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 12 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  pretty          1\n",
      "1  inside          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 33 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0      tayloru          1\n",
      "1           22          1\n",
      "2      learner          1\n",
      "3       writer          1\n",
      "4  storyteller          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 26 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    bath          1\n",
      "1      24          1\n",
      "2    full          1\n",
      "3    time          1\n",
      "4  potato          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 62 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0           yes          1\n",
      "1          real          1\n",
      "2          name          1\n",
      "3           gig          1\n",
      "4  photographer          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 18 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0     uk          1\n",
      "1  bring          1\n",
      "2   back          1\n",
      "3    new          1\n",
      "4   girl          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 61 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0          19          1\n",
      "1      alumna          1\n",
      "2       treat          1\n",
      "3      people          1\n",
      "4  reflection          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   317          1\n",
      "1   368          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 14 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     mom          1\n",
      "1  better          1\n",
      "2   thing          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 78 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0        2447          1\n",
      "1  recovering          1\n",
      "2        feed          1\n",
      "3    salesman          1\n",
      "4   ndsubison          1\n",
      "There are 11 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 66 characters in the data.\n",
      "The lexical diversity is 0.909 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0     lover          2\n",
      "1  business          1\n",
      "2     owner          1\n",
      "3    people          1\n",
      "4    animal          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 12 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0          ig          1\n",
      "1  bhullarirl          1\n",
      "There are 13 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 94 characters in the data.\n",
      "The lexical diversity is 0.923 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     firm          2\n",
      "1    proud          1\n",
      "2      mom          1\n",
      "3        5          1\n",
      "4  fighter          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 8 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  downtown          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 88 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0     teaches          1\n",
      "1  polarizing          1\n",
      "2      topics          1\n",
      "3        play          1\n",
      "4        nice          1\n",
      "There are 5 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 28 characters in the data.\n",
      "The lexical diversity is 0.800 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  grandma          2\n",
      "1   loving          1\n",
      "2      mom          1\n",
      "3    great          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 41 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "              token  frequency\n",
      "0             treat          1\n",
      "1            people          1\n",
      "2          kindness          1\n",
      "3            sheher          1\n",
      "4  blacklivesmatter          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0         2010          1\n",
      "1  sagittarius          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      cat          1\n",
      "1  content          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 23 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      care          1\n",
      "1    worker          1\n",
      "2  24973530          1\n",
      "3        38          1\n",
      "4       451          1\n",
      "There are 17 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 85 characters in the data.\n",
      "The lexical diversity is 0.941 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  wife          2\n",
      "1  blue          1\n",
      "2  eyed          1\n",
      "3    50          1\n",
      "4  year          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 92 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0      proud          1\n",
      "1   american          1\n",
      "2        cat          1\n",
      "3  obsession          1\n",
      "4        bad          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 41 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      477          1\n",
      "1  welcome          1\n",
      "2   circus          1\n",
      "3   dexter          1\n",
      "4      fan          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 16 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0  bidenharris          1\n",
      "1        voter          1\n",
      "There are 13 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 77 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "             token  frequency\n",
      "0           couple          1\n",
      "1           adults          1\n",
      "2            adult          1\n",
      "3           things          1\n",
      "4  adultssometimes          1\n",
      "There are 10 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 59 characters in the data.\n",
      "The lexical diversity is 0.900 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0        2          2\n",
      "1    owner          1\n",
      "2      mom          1\n",
      "3  grandma          1\n",
      "4     semi          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 63 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   zone          1\n",
      "1    198          1\n",
      "2    468          1\n",
      "3   live          1\n",
      "4  house          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 90 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  finalist          1\n",
      "1     state          1\n",
      "2    nation          1\n",
      "3     blind          1\n",
      "4       eye          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 30 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   mama          1\n",
      "1   nana          1\n",
      "2   wife          1\n",
      "3   year          1\n",
      "4  award          1\n",
      "There are 14 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 84 characters in the data.\n",
      "The lexical diversity is 0.929 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0       privacy          2\n",
      "1  differential          1\n",
      "2            ml          1\n",
      "3      lifelong          1\n",
      "4       learner          1\n",
      "There are 18 tokens in the data.\n",
      "There are 18 unique tokens in the data.\n",
      "There are 136 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0      ca          1\n",
      "1    1068          1\n",
      "2    1126          1\n",
      "3  doctor          1\n",
      "4  angela          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 68 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0    loving          1\n",
      "1   grandma          1\n",
      "2    former          1\n",
      "3  designer          1\n",
      "4   forever          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 11 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   speak          1\n",
      "1  listen          1\n",
      "There are 16 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 66 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0      ut          1\n",
      "1     404          1\n",
      "2     330          1\n",
      "3  hethey          1\n",
      "4     aka          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 28 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0    delusions          1\n",
      "1  cottagecore          1\n",
      "2     grandeur          1\n",
      "There are 19 tokens in the data.\n",
      "There are 19 unique tokens in the data.\n",
      "There are 96 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0         ro          1\n",
      "1  listening          1\n",
      "2  favorites          1\n",
      "3        run          1\n",
      "4        dmc          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 22 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "            token  frequency\n",
      "0  barbereducator          1\n",
      "1            gods          1\n",
      "2            love          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 99 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0         1064          1\n",
      "1  literateuse          1\n",
      "2          los          1\n",
      "3      angeles          1\n",
      "4        stone          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   lol          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  adhd          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 26 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0        16          1\n",
      "1       170          1\n",
      "2  kirkland          1\n",
      "3  purified          1\n",
      "4     water          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 130 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "            token  frequency\n",
      "0       spiritual          1\n",
      "1  intellectually          1\n",
      "2     introverted          1\n",
      "3      innovative          1\n",
      "4        problack          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 9 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   make          1\n",
      "1  laugh          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 13 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0    22          1\n",
      "1  cest          1\n",
      "2  pour          1\n",
      "3   toi          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 8 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  girl          1\n",
      "1    xu          1\n",
      "2    25          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 7 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  nuclear          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 2 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0    95          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 70 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0    alcoholic          1\n",
      "1      retired          1\n",
      "2  firefighter          1\n",
      "3       please          1\n",
      "4         dont          1\n",
      "There are 30 tokens in the data.\n",
      "There are 30 unique tokens in the data.\n",
      "There are 159 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                 token  frequency\n",
      "0                  aka          1\n",
      "1                  장용진          1\n",
      "2  1002554566800494593          1\n",
      "3             savannah          1\n",
      "4                   ga          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 56 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0    history          1\n",
      "1     sexual          1\n",
      "2     health          1\n",
      "3  ageingold          1\n",
      "4        age          1\n",
      "There are 10 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 44 characters in the data.\n",
      "The lexical diversity is 0.900 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     live          2\n",
      "1       29          1\n",
      "2    dream          1\n",
      "3  forever          1\n",
      "4    youll          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   439          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 44 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0     animal          1\n",
      "1      lover          1\n",
      "2  volunteer          1\n",
      "3    looking          1\n",
      "4    forward          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 79 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0                 400          1\n",
      "1                3008          1\n",
      "2  httpstcoyiderqaj31          1\n",
      "3  httpstco3saq6ihvj3          1\n",
      "4  httpstcobwkwsrmbuu          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 65 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0    matter          1\n",
      "1   fascism          1\n",
      "2   fantasy          1\n",
      "3  football          1\n",
      "4     board          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  1020          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 30 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0     first          1\n",
      "1      dont          1\n",
      "2   succeed          1\n",
      "3       fix          1\n",
      "4  ponytail          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 12 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      talk          1\n",
      "1  godzilla          1\n",
      "There are 17 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 111 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0    futbolera          1\n",
      "1  multifandom          1\n",
      "2         club          1\n",
      "3      américa          1\n",
      "4    selección          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 36 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   167          1\n",
      "1  dont          1\n",
      "2  even          1\n",
      "3  know          1\n",
      "4   use          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 75 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0     queer          1\n",
      "1  lebanese          1\n",
      "2       leo          1\n",
      "3     funny          1\n",
      "4   product          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 16 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  distancing          1\n",
      "1      please          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 74 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      many          1\n",
      "1  remember          1\n",
      "2  everyone          1\n",
      "3  fighting          1\n",
      "4  personal          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 12 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0  𝙗𝙖𝙗𝙮8usomuch          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  peachy          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  bottom          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 18 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  theyshehe          1\n",
      "1  nonbinary          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 24 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0    twitter          1\n",
      "1         im          1\n",
      "2  political          1\n",
      "3     junkie          1\n",
      "There are 15 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 61 characters in the data.\n",
      "The lexical diversity is 0.933 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    mom          2\n",
      "1    379          1\n",
      "2   moon          1\n",
      "3  child          1\n",
      "4    blm          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 29 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    stand          1\n",
      "1      gop          1\n",
      "2    chump          1\n",
      "3  ovarian          1\n",
      "4   cancer          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 40 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0       market          1\n",
      "1   strategist          1\n",
      "2      program          1\n",
      "3       change          1\n",
      "4  leaderhuman          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 37 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0                16th          1\n",
      "1             century          1\n",
      "2            bookings          1\n",
      "3  nmarquinhagmailcom          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 39 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0    potential          1\n",
      "1     employer          1\n",
      "2       please          1\n",
      "3        leave          1\n",
      "4  immediately          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 41 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0        media          1\n",
      "1  artistgamer          1\n",
      "2    root_b_83          1\n",
      "3       122517          1\n",
      "4   aemediaart          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 92 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       long          1\n",
      "1      grift          1\n",
      "2     weasel          1\n",
      "3       zero          1\n",
      "4  character          1\n",
      "There are 20 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 55 characters in the data.\n",
      "The lexical diversity is 0.600 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0     u          4\n",
      "1  know          3\n",
      "2  love          2\n",
      "3  yuki          2\n",
      "4     4          2\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 21 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  father          1\n",
      "1     80s          1\n",
      "2    baby          1\n",
      "3     420          1\n",
      "4   hehim          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 99 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0     since          1\n",
      "1      1974          1\n",
      "2  football          1\n",
      "3     music          1\n",
      "4      food          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 12 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0  impersonator          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 12 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  youtube          1\n",
      "1    check          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 13 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  fanboy          1\n",
      "1     del          1\n",
      "2    amor          1\n",
      "There are 18 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 99 characters in the data.\n",
      "The lexical diversity is 0.944 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  twitter          2\n",
      "1   thinks          1\n",
      "2       im          1\n",
      "3  ireland          1\n",
      "4       11          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 55 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      2012          1\n",
      "1      mama          1\n",
      "2   swiftie          1\n",
      "3  daughter          1\n",
      "4        uk          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 69 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0  complaining          1\n",
      "1       talent          1\n",
      "2        would          1\n",
      "3     talented          1\n",
      "4      lotmost          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 115 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "             token  frequency\n",
      "0          passion          1\n",
      "1         teaching          1\n",
      "2  entrepreneurial          1\n",
      "3    communication          1\n",
      "4         stanford          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0        de          1\n",
      "1  personne          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  curator          1\n",
      "1      bad          1\n",
      "2    taste          1\n",
      "There are 8 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 30 characters in the data.\n",
      "The lexical diversity is 0.875 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0      im          2\n",
      "1    2762          1\n",
      "2    2151          1\n",
      "3  crying          1\n",
      "4  cummin          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  666999          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    hgtv          1\n",
      "1   later          1\n",
      "2  sheher          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 13 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   heart          1\n",
      "1  prelaw          1\n",
      "2      21          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 11 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   happy          1\n",
      "1  person          1\n",
      "There are 13 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 73 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0        566          1\n",
      "1      proud          1\n",
      "2  represent          1\n",
      "3         nw          1\n",
      "4        okc          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    bus          1\n",
      "1      7          1\n",
      "2    146          1\n",
      "3  loyal          1\n",
      "4    bit          1\n",
      "There are 18 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 85 characters in the data.\n",
      "The lexical diversity is 0.833 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0     im          3\n",
      "1   love          2\n",
      "2    610          1\n",
      "3  hello          1\n",
      "4  world          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 78 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     1294          1\n",
      "1      mom          1\n",
      "2  grandma          1\n",
      "3     mimi          1\n",
      "4     maam          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 19 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0        𝘤𝘰𝘻          1\n",
      "1          𝘐          1\n",
      "2  𝘣𝘳𝘦𝘢𝘬𝘥𝘰𝘸𝘯          1\n",
      "3     𝘦𝘢𝘴𝘪𝘭𝘺          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 37 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      come          1\n",
      "1   retweet          1\n",
      "2    sheher          1\n",
      "3    stream          1\n",
      "4  iichliwp          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 87 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0         caring          1\n",
      "1           love          1\n",
      "2        animals          1\n",
      "3           care          1\n",
      "4  environmental          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 46 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0     france          1\n",
      "1         47          1\n",
      "2        155          1\n",
      "3  screaming          1\n",
      "4     vulgar          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 54 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0       sense          1\n",
      "1   curiosity          1\n",
      "2      almost          1\n",
      "3  everything          1\n",
      "4          ok          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 19 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    sins          1\n",
      "1  doesnt          1\n",
      "2    know          1\n",
      "3   rules          1\n",
      "There are 5 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 24 characters in the data.\n",
      "The lexical diversity is 0.800 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  little          2\n",
      "1   means          1\n",
      "2     bit          1\n",
      "3    mean          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   121          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 33 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    truck          1\n",
      "1   driver          1\n",
      "2  liberal          1\n",
      "3    bible          1\n",
      "4     belt          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   225          1\n",
      "There are 16 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 136 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     396          1\n",
      "1     451          1\n",
      "2   white          1\n",
      "3     man          1\n",
      "4  horses          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 120 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "             token  frequency\n",
      "0             esrc          1\n",
      "1              phd          1\n",
      "2          student          1\n",
      "3  corpussocialsci          1\n",
      "4     lancasteruni          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 5 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  toker          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0    18          1\n",
      "1    91          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 13 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  completely          1\n",
      "1         cry          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 28 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0          witch          1\n",
      "1  hertfordshire          1\n",
      "2             80          1\n",
      "3       prosecco          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 21 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     love          1\n",
      "1       38          1\n",
      "2      244          1\n",
      "3    cries          1\n",
      "4  giggles          1\n",
      "There are 18 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 84 characters in the data.\n",
      "The lexical diversity is 0.889 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0     sobre          2\n",
      "1     mundo          2\n",
      "2       320          1\n",
      "3  noticias          1\n",
      "4        la          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 48 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0  storyteller          1\n",
      "1     explorer          1\n",
      "2        lgbtq          1\n",
      "3     novelist          1\n",
      "4       coffee          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 68 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  associate          1\n",
      "1   director          1\n",
      "2    digital          1\n",
      "3    library          1\n",
      "4      north          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  giants          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 24 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "            token  frequency\n",
      "0  keeganbedinger          1\n",
      "1         biggest          1\n",
      "2             fan          1\n",
      "There are 14 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 74 characters in the data.\n",
      "The lexical diversity is 0.929 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0      𝚋𝚢          2\n",
      "1     456          1\n",
      "2  𝙱𝚎𝚛𝚋𝚎𝚛          1\n",
      "3   𝚀𝚞𝚎𝚎𝚗          1\n",
      "4       ⵣ          1\n",
      "There are 5 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 18 characters in the data.\n",
      "The lexical diversity is 0.800 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0     el          2\n",
      "1  entre          1\n",
      "2  desig          1\n",
      "3   tedi          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 53 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0    internets          1\n",
      "1            1          1\n",
      "2  alternative          1\n",
      "3          pop          1\n",
      "4         punk          1\n",
      "There are 16 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 71 characters in the data.\n",
      "The lexical diversity is 0.875 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  podemos          2\n",
      "1      que          2\n",
      "2       al          1\n",
      "3    final          1\n",
      "4      del          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 48 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0        115          1\n",
      "1     aerith          1\n",
      "2       stan          1\n",
      "3    account          1\n",
      "4  certified          1\n",
      "There are 16 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 78 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    1009          1\n",
      "1    1391          1\n",
      "2    lula          1\n",
      "3    2023          1\n",
      "4  mulher          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   ilf          1\n",
      "There are 24 tokens in the data.\n",
      "There are 22 unique tokens in the data.\n",
      "There are 126 characters in the data.\n",
      "The lexical diversity is 0.917 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0  bidenharris          2\n",
      "1       vaxxed          2\n",
      "2   california          1\n",
      "3          usa          1\n",
      "4          540          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       need          1\n",
      "1         iv          1\n",
      "2  starbucks          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 23 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0     sunrise          1\n",
      "1  organizing          1\n",
      "2         lab          1\n",
      "3         sol          1\n",
      "There are 18 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 70 characters in the data.\n",
      "The lexical diversity is 0.889 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    soy          2\n",
      "1     yo          2\n",
      "2  cerca          1\n",
      "3     12          1\n",
      "4    228          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   day          1\n",
      "1  like          1\n",
      "2   one          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 29 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0      former          1\n",
      "1  journalist          1\n",
      "2    annoying          1\n",
      "3       hehim          1\n",
      "There are 13 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 61 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  anime          1\n",
      "1   star          1\n",
      "2   wars          1\n",
      "3  video          1\n",
      "4  games          1\n",
      "There are 18 tokens in the data.\n",
      "There are 18 unique tokens in the data.\n",
      "There are 105 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  zealand          1\n",
      "1      128          1\n",
      "2      203          1\n",
      "3   sheher          1\n",
      "4    tryna          1\n",
      "There are 18 tokens in the data.\n",
      "There are 18 unique tokens in the data.\n",
      "There are 113 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0   friend          1\n",
      "1    lover          1\n",
      "2     life          1\n",
      "3   empire          1\n",
      "4  builder          1\n",
      "There are 17 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 106 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0      hehimhis          1\n",
      "1  transplanted          1\n",
      "2     desertrat          1\n",
      "3     architect          1\n",
      "4         proud          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 21 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       fan          1\n",
      "1       pop          1\n",
      "2   culture          1\n",
      "3  handbags          1\n",
      "There are 17 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 87 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0       rn          1\n",
      "1  forever          1\n",
      "2     blue          1\n",
      "3   mental          1\n",
      "4   health          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 17 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     much          1\n",
      "1  happier          1\n",
      "2       im          1\n",
      "3     dead          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 109 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                 token  frequency\n",
      "0  1288283504393027585          1\n",
      "1             denobula          1\n",
      "2                 5233          1\n",
      "3                 1679          1\n",
      "4                   28          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  1049          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 20 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0   german          1\n",
      "1    expat          1\n",
      "2  sliving          1\n",
      "3       us          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 16 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   frank          1\n",
      "1   ocean          1\n",
      "2  tattoo          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 34 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   adore          1\n",
      "1  movies          1\n",
      "2   music          1\n",
      "3      tv          1\n",
      "4     pop          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 20 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    kedua          1\n",
      "1     yang          1\n",
      "2  pertama          1\n",
      "3     kamu          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 24 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     420          1\n",
      "1   three          1\n",
      "2  queers          1\n",
      "3  trench          1\n",
      "4    coat          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 71 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0          2580          1\n",
      "1        signed          1\n",
      "2  epic_records          1\n",
      "3         world          1\n",
      "4     visionary          1\n",
      "There are 13 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 88 characters in the data.\n",
      "The lexical diversity is 0.923 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0         lover          2\n",
      "1       amateur          1\n",
      "2  photographer          1\n",
      "3        animal          1\n",
      "4         movie          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   491          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 88 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0   enthusiast          1\n",
      "1       animal          1\n",
      "2        lover          1\n",
      "3     democrat          1\n",
      "4  bidenharris          1\n",
      "There are 15 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 84 characters in the data.\n",
      "The lexical diversity is 0.800 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0       today          4\n",
      "1  yesterdays          1\n",
      "2       grasp          1\n",
      "3   weakening          1\n",
      "4   tomorrows          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 52 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    393          1\n",
      "1   2497          1\n",
      "2    sir          1\n",
      "3   shit          1\n",
      "4  stain          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 41 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  daydreamer          1\n",
      "1      writer          1\n",
      "2    believer          1\n",
      "3      things          1\n",
      "4          go          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 54 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   dogs          1\n",
      "1    242          1\n",
      "2    298          1\n",
      "3  place          1\n",
      "4  trump          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 18 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     eat          1\n",
      "1    well          1\n",
      "2  travel          1\n",
      "3   often          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 90 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       land          1\n",
      "1        246          1\n",
      "2       1203          1\n",
      "3      hehim          1\n",
      "4  abolition          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 48 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  45318642          1\n",
      "1       744          1\n",
      "2      1866          1\n",
      "3   rainbow          1\n",
      "4    haired          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 37 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      210          1\n",
      "1      big          1\n",
      "2  chaotic          1\n",
      "3  neutral          1\n",
      "4   energy          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 40 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "              token  frequency\n",
      "0  singersongwriter          1\n",
      "1            donate          1\n",
      "2              help          1\n",
      "3         ukrainian          1\n",
      "4             women          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 47 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  television          1\n",
      "1    reporter          1\n",
      "2   executive          1\n",
      "3    producer          1\n",
      "4        news          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 19 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0  simplesmente          1\n",
      "1       simples          1\n",
      "There are 17 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 98 characters in the data.\n",
      "The lexical diversity is 0.941 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   shea          2\n",
      "1    370          1\n",
      "2    681          1\n",
      "3  lking          1\n",
      "4   life          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 5 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  tired          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 17 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "            token  frequency\n",
      "0             739          1\n",
      "1  foodietraveler          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 8 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  prepared          1\n",
      "There are 13 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 81 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0     redskin          1\n",
      "1         fan          1\n",
      "2    attorney          1\n",
      "3    graduate          1\n",
      "4  georgetown          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 27 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0        2819          1\n",
      "1  ᴍᴀᴄǫᴜᴀяɪҽˢ          1\n",
      "2        ᴍᴄᴏᴍ          1\n",
      "3   ᴍᴀяᴋҽᴛɪɴɢ          1\n",
      "There are 16 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 121 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     3729          1\n",
      "1  liberal          1\n",
      "2  atheist          1\n",
      "3   animal          1\n",
      "4    lover          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 29 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  lydibabyyy          1\n",
      "1        july          1\n",
      "2      babyyy          1\n",
      "3          18          1\n",
      "4     indiana          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 83 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   never          1\n",
      "1     met          1\n",
      "2  animal          1\n",
      "3   didnt          1\n",
      "4    want          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 21 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    hope          1\n",
      "1  tweets          1\n",
      "2    dont          1\n",
      "3    make          1\n",
      "4     sad          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  stand          1\n",
      "1  still          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 35 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0         596          1\n",
      "1    personal          1\n",
      "2     trainer          1\n",
      "3     fitness          1\n",
      "4  specialist          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 60 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      497          1\n",
      "1  liberal          1\n",
      "2    proud          1\n",
      "3     ally          1\n",
      "4      blm          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 9 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     guy          1\n",
      "1  twitch          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 62 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0    student          1\n",
      "1   creative          1\n",
      "2       mass          1\n",
      "3      media          1\n",
      "4  political          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 19 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  american          1\n",
      "1     loves          1\n",
      "2    cinema          1\n",
      "There are 19 tokens in the data.\n",
      "There are 18 unique tokens in the data.\n",
      "There are 111 characters in the data.\n",
      "The lexical diversity is 0.947 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  things          2\n",
      "1     240          1\n",
      "2     390          1\n",
      "3    best          1\n",
      "4    life          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 28 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       else          1\n",
      "1     except          1\n",
      "2  listening          1\n",
      "3       good          1\n",
      "4      music          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 49 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    lead          1\n",
      "1   horse          1\n",
      "2  roller          1\n",
      "3    cant          1\n",
      "4    make          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 59 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0       biden          1\n",
      "1      harris          1\n",
      "2   supporter          1\n",
      "3         got          1\n",
      "4  vaccinated          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 84 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0         304          1\n",
      "1        2642          1\n",
      "2        stop          1\n",
      "3  amplifying          1\n",
      "4      stupid          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   830          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 5 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  peace          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 14 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     470          1\n",
      "1     unt          1\n",
      "2      23          1\n",
      "3  sheher          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 29 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "              token  frequency\n",
      "0             order          1\n",
      "1          opinions          1\n",
      "2  standwithukraine          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 87 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                 token  frequency\n",
      "0      contentmarketer          1\n",
      "1  socialmediamarketer          1\n",
      "2             starwars          1\n",
      "3                 nerd          1\n",
      "4               trivia          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 32 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "             token  frequency\n",
      "0           brings          1\n",
      "1              joy          1\n",
      "2               go          1\n",
      "3           snapig          1\n",
      "4  kaitlynalexiiss          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 25 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0     dumpster          1\n",
      "1       shared          1\n",
      "2  proletariat          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 9 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  laugh          1\n",
      "1   love          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  1643          1\n",
      "There are 13 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 69 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0     person          1\n",
      "1     messed          1\n",
      "2      sense          1\n",
      "3      humor          1\n",
      "4  sarcastic          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 100 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0      believe          1\n",
      "1     laughter          1\n",
      "2    tolerance          1\n",
      "3  irreverence          1\n",
      "4   blathering          1\n",
      "There are 13 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 64 characters in the data.\n",
      "The lexical diversity is 0.923 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  sports          2\n",
      "1    film          1\n",
      "2    buff          1\n",
      "3  native          1\n",
      "4   texan          1\n",
      "There are 12 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 99 characters in the data.\n",
      "The lexical diversity is 0.917 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0  populations          2\n",
      "1     educator          1\n",
      "2   supporting          1\n",
      "3       public          1\n",
      "4    education          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 76 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      nice          1\n",
      "1    listen          1\n",
      "2   science          1\n",
      "3      help          1\n",
      "4  whenever          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 48 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  political          1\n",
      "1       void          1\n",
      "2       yell          1\n",
      "3       help          1\n",
      "4    relieve          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 46 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     best          1\n",
      "1       dc          1\n",
      "2    metro          1\n",
      "3    speed          1\n",
      "4  cameras          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  hardly          1\n",
      "1    know          1\n",
      "There are 12 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 72 characters in the data.\n",
      "The lexical diversity is 0.833 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  financial          3\n",
      "1       fair          1\n",
      "2     market          1\n",
      "3    advisor          1\n",
      "4   anything          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 16 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    one          1\n",
      "1  would          1\n",
      "2   give          1\n",
      "3   shit          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 24 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0    citizen          1\n",
      "1  architect          1\n",
      "2        dog          1\n",
      "3      lover          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 9 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  teamcolts          1\n",
      "There are 19 tokens in the data.\n",
      "There are 19 unique tokens in the data.\n",
      "There are 122 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0        1109          1\n",
      "1    lifelong          1\n",
      "2  restaurant          1\n",
      "3    advocate          1\n",
      "4         bay          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 26 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0  engineering          1\n",
      "1        uclan          1\n",
      "2   basketball          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 26 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0        endless          1\n",
      "1         stream          1\n",
      "2  consciousness          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 14 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   bad          1\n",
      "1  news          1\n",
      "2  good          1\n",
      "3   way          1\n",
      "There are 15 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 71 characters in the data.\n",
      "The lexical diversity is 0.733 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  people          3\n",
      "1    make          2\n",
      "2     fun          2\n",
      "3  ignore          1\n",
      "4    like          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 54 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0     middle          1\n",
      "1       road          1\n",
      "2  extremist          1\n",
      "3    married          1\n",
      "4         44          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 5 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  hehim          1\n",
      "There are 20 tokens in the data.\n",
      "There are 19 unique tokens in the data.\n",
      "There are 104 characters in the data.\n",
      "The lexical diversity is 0.950 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       love          2\n",
      "1    animals          1\n",
      "2      birds          1\n",
      "3    amateur          1\n",
      "4  historian          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 47 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      old          1\n",
      "1  college          1\n",
      "2  student          1\n",
      "3    likes          1\n",
      "4   fanart          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 85 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0  wifemomhuman          1\n",
      "1          love          1\n",
      "2        planet          1\n",
      "3     traveling          1\n",
      "4     socialism          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 25 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  normal          1\n",
      "1  spider          1\n",
      "2   chaos          1\n",
      "3     fly          1\n",
      "4   hehim          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 17 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0     heart          1\n",
      "1  breaking          1\n",
      "2      mine          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 39 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0      martha          1\n",
      "1  washington          1\n",
      "2        love          1\n",
      "3     husband          1\n",
      "4      george          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 19 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0     tweet          1\n",
      "1    things          1\n",
      "2  interest          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 14 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    girl          1\n",
      "1  living          1\n",
      "2    life          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 7 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0     50          1\n",
      "1  japan          1\n",
      "There are 17 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 78 characters in the data.\n",
      "The lexical diversity is 0.941 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0     de          2\n",
      "1     78          1\n",
      "2     21          1\n",
      "3    yrs          1\n",
      "4  adult          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 30 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  humans          1\n",
      "1   leave          1\n",
      "2   often          1\n",
      "3   scars          1\n",
      "4    john          1\n",
      "There are 16 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 80 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       ohio          1\n",
      "1      widow          1\n",
      "2        boy          1\n",
      "3        mom          1\n",
      "4  supporter          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0       queen          1\n",
      "1  apparently          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 21 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   645          1\n",
      "1  love          1\n",
      "2   way          1\n",
      "3   ill          1\n",
      "4  ever          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 28 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     123          1\n",
      "1     210          1\n",
      "2   black          1\n",
      "3   lives          1\n",
      "4  matter          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 33 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0           nyer          1\n",
      "1             pa          1\n",
      "2  longsuffering          1\n",
      "3           mets          1\n",
      "4            fan          1\n",
      "There are 22 tokens in the data.\n",
      "There are 22 unique tokens in the data.\n",
      "There are 134 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0     planet          1\n",
      "1      boone          1\n",
      "2   20599212          1\n",
      "3  milwaukee          1\n",
      "4         wi          1\n",
      "There are 22 tokens in the data.\n",
      "There are 22 unique tokens in the data.\n",
      "There are 111 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0          291          1\n",
      "1          686          1\n",
      "2       marcom          1\n",
      "3          pro          1\n",
      "4  umassmedcwm          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 54 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  personal          1\n",
      "1   account          1\n",
      "2   sharing          1\n",
      "3     stuff          1\n",
      "4      like          1\n",
      "There are 11 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 64 characters in the data.\n",
      "The lexical diversity is 0.545 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       get          3\n",
      "1  positive          2\n",
      "2  whatever          2\n",
      "3     think          2\n",
      "4     vibes          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 16 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    moon          1\n",
      "1  gemini          1\n",
      "2  rising          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 8 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  like          1\n",
      "1  jazz          1\n",
      "There are 16 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 95 characters in the data.\n",
      "The lexical diversity is 0.812 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    kind          2\n",
      "1  people          2\n",
      "2   tough          2\n",
      "3    fine          1\n",
      "4    used          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 42 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       keep          1\n",
      "1      whats          1\n",
      "2  happening          1\n",
      "3     agenda          1\n",
      "4     trying          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 2 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0    24          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   315          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 58 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0    special          1\n",
      "1  education          1\n",
      "2    teacher          1\n",
      "3   advocate          1\n",
      "4        niu          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  pret          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  fucking          1\n",
      "1     idea          1\n",
      "2     dude          1\n",
      "There are 14 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 87 characters in the data.\n",
      "The lexical diversity is 0.929 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0      kittens          2\n",
      "1          358          1\n",
      "2          768          1\n",
      "3  indivisible          1\n",
      "4      citizen          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 47 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  enrolled          1\n",
      "1     agent          1\n",
      "2       irs          1\n",
      "3       tax          1\n",
      "4      help          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 7 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   let          1\n",
      "1  live          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 63 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0     wifey          1\n",
      "1    mental          1\n",
      "2    health          1\n",
      "3  advocate          1\n",
      "4    makeup          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 76 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0         756          1\n",
      "1    musician          1\n",
      "2         yac          1\n",
      "3  furmannews          1\n",
      "4        mmx2          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 71 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0           3098          1\n",
      "1      endurance          1\n",
      "2  athletewomens          1\n",
      "3         rights          1\n",
      "4    activistlaw          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 67 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   new          1\n",
      "1  york          1\n",
      "2    ny          1\n",
      "3   246          1\n",
      "4   882          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 50 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0   halifax          1\n",
      "1        ns          1\n",
      "2     omaha          1\n",
      "3  nebraska          1\n",
      "4       517          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   moi          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 66 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      adhd          1\n",
      "1     brain          1\n",
      "2      many          1\n",
      "3  thoughts          1\n",
      "4       may          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 8 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  full          1\n",
      "1  love          1\n",
      "There are 14 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 85 characters in the data.\n",
      "The lexical diversity is 0.929 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0   things          2\n",
      "1  anxious          1\n",
      "2  nothing          1\n",
      "3     good          1\n",
      "4     come          1\n",
      "There are 20 tokens in the data.\n",
      "There are 20 unique tokens in the data.\n",
      "There are 117 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    1136          1\n",
      "1  juiced          1\n",
      "2   thong          1\n",
      "3  speedo          1\n",
      "4  posing          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 38 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "            token  frequency\n",
      "0         shehers          1\n",
      "1          policy          1\n",
      "2  communications          1\n",
      "3         twitter          1\n",
      "4            apac          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 72 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0        public          1\n",
      "1        health          1\n",
      "2  professional          1\n",
      "3         juris          1\n",
      "4        doctor          1\n",
      "There are 15 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 104 characters in the data.\n",
      "The lexical diversity is 0.933 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0          life          2\n",
      "1           101          1\n",
      "2       retired          1\n",
      "3          sort          1\n",
      "4  relentlessly          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 62 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0    devereux          1\n",
      "1    advanced          1\n",
      "2  behavioral          1\n",
      "3      health          1\n",
      "4    national          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 27 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0      89          1\n",
      "1  living          1\n",
      "2    life          1\n",
      "3    much          1\n",
      "4     fun          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 81 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0   broadcast          1\n",
      "1       media          1\n",
      "2   marketing          1\n",
      "3  previously          1\n",
      "4        ndtv          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 9 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     idk          1\n",
      "1  either          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 16 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0      spread          1\n",
      "1  positivity          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 43 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0        904          1\n",
      "1  beautiful          1\n",
      "2    totally          1\n",
      "3    perfect          1\n",
      "4       home          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 79 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "              token  frequency\n",
      "0  musiclakefamcats          1\n",
      "1           support          1\n",
      "2             local          1\n",
      "3             music          1\n",
      "4              kind          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 20 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  upcoming          1\n",
      "1    fergie          1\n",
      "2    custom          1\n",
      "There are 14 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 80 characters in the data.\n",
      "The lexical diversity is 0.929 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0      lover          2\n",
      "1        571          1\n",
      "2  nonbinary          1\n",
      "3      queer          1\n",
      "4   theythem          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 68 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "             token  frequency\n",
      "0            media          1\n",
      "1             nerd          1\n",
      "2            comic          1\n",
      "3  writerperformer          1\n",
      "4          chortle          1\n",
      "There are 3 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 13 characters in the data.\n",
      "The lexical diversity is 0.667 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  tweet          2\n",
      "1    425          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 23 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                 token  frequency\n",
      "0  1497466648013406218          1\n",
      "1                    4          1\n",
      "2                  116          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 22 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     são          1\n",
      "1   paulo          1\n",
      "2     new          1\n",
      "3  yorker          1\n",
      "4   heart          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 25 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  friends          1\n",
      "1  improve          1\n",
      "2  english          1\n",
      "3     yolo          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 51 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    lot          1\n",
      "1  stuff          1\n",
      "2   find          1\n",
      "3  funny          1\n",
      "4   also          1\n",
      "There are 17 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 116 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       3848          1\n",
      "1  columnist          1\n",
      "2  freelance          1\n",
      "3     writer          1\n",
      "4      music          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0    go          1\n",
      "1  life          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 24 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    fresh          1\n",
      "1   fweets          1\n",
      "2   silver          1\n",
      "3  platter          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 21 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  person          1\n",
      "1   loves          1\n",
      "2   smile          1\n",
      "3   laugh          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 20 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0     record          1\n",
      "1  scratcher          1\n",
      "2         dj          1\n",
      "3        blm          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    118          1\n",
      "1   spam          1\n",
      "2    acc          1\n",
      "3  memes          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  brutal          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 28 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "            token  frequency\n",
      "0  army20wildsoul          1\n",
      "1      confidence          1\n",
      "2            sexy          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 40 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0    father          1\n",
      "1        im          1\n",
      "2       doc          1\n",
      "3     gamer          1\n",
      "4  stubborn          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 27 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    apple          1\n",
      "1    juice          1\n",
      "2    party          1\n",
      "3    youre          1\n",
      "4  invited          1\n",
      "There are 13 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 58 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  blankie          1\n",
      "1        w          1\n",
      "2   switch          1\n",
      "3       47          1\n",
      "4      192          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  winter          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   mah          1\n",
      "1  tipo          1\n",
      "2   meh          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  enough          1\n",
      "1    city          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 16 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    hair          1\n",
      "1  makeup          1\n",
      "2  artist          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 32 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0       sheher          1\n",
      "1  illustrator          1\n",
      "2      student          1\n",
      "3           ux          1\n",
      "4       design          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 23 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                 token  frequency\n",
      "0  1464905664682020866          1\n",
      "1                    8          1\n",
      "2                  142          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 44 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0      2          1\n",
      "1  great          1\n",
      "2   kids          1\n",
      "3    one          1\n",
      "4    ms2          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 93 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0                  15          1\n",
      "1                 183          1\n",
      "2    cciixxmmmmxxiicc          1\n",
      "3  httpstcofr2xznq5t7          1\n",
      "4  httpstcog7xigzcqca          1\n",
      "There are 18 tokens in the data.\n",
      "There are 18 unique tokens in the data.\n",
      "There are 136 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0   potawatomie          1\n",
      "1         lands          1\n",
      "2           388          1\n",
      "3          1038          1\n",
      "4  entomologist          1\n",
      "There are 12 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 89 characters in the data.\n",
      "The lexical diversity is 0.917 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  breathing          2\n",
      "1      tweet          1\n",
      "2   kindness          1\n",
      "3       love          1\n",
      "4  gratitude          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      pee          1\n",
      "1  outside          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 82 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0        hannah          1\n",
      "1        loudon          1\n",
      "2          take          1\n",
      "3  lighthearted          1\n",
      "4          look          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 12 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    one          1\n",
      "1  tweet          1\n",
      "2   time          1\n",
      "There are 20 tokens in the data.\n",
      "There are 20 unique tokens in the data.\n",
      "There are 131 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0              closed          1\n",
      "1  695803377234608129          1\n",
      "2              united          1\n",
      "3             kingdom          1\n",
      "4                 576          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 8 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  youre          1\n",
      "1    gay          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 38 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "            token  frequency\n",
      "0           vaxup          1\n",
      "1          maskup          1\n",
      "2  seriouslynodms          1\n",
      "3   turntexasblue          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 20 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0         989          1\n",
      "1         art          1\n",
      "2  commission          1\n",
      "3        open          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 105 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0       gamerstreamer          1\n",
      "1     twitchaffiliate          1\n",
      "2  httpstcowjl8ajtoic          1\n",
      "3                  im          1\n",
      "4               nurse          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 51 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    7120          1\n",
      "1    7665          1\n",
      "2   equal          1\n",
      "3   human          1\n",
      "4  rights          1\n",
      "There are 18 tokens in the data.\n",
      "There are 18 unique tokens in the data.\n",
      "There are 96 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0     humour          1\n",
      "1        wit          1\n",
      "2      music          1\n",
      "3        kid          1\n",
      "4  grandkids          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  everyone          1\n",
      "1        ia          1\n",
      "There are 18 tokens in the data.\n",
      "There are 18 unique tokens in the data.\n",
      "There are 120 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "               token  frequency\n",
      "0               3485          1\n",
      "1  writeractivistmum          1\n",
      "2                 md          1\n",
      "3        alternative          1\n",
      "4           national          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 12 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      golf          1\n",
      "1  baseball          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 20 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      docs          1\n",
      "1  director          1\n",
      "2        uu          1\n",
      "3    sheher          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 24 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      103          1\n",
      "1      mom          1\n",
      "2     wife          1\n",
      "3  nursing          1\n",
      "4  student          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 44 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "              token  frequency\n",
      "0              gray          1\n",
      "1             hairs          1\n",
      "2              like          1\n",
      "3             think          1\n",
      "4  organicallygrown          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 25 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  reinvent          1\n",
      "1    needed          1\n",
      "2       dms          1\n",
      "3      vote          1\n",
      "4      blue          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   136          1\n",
      "There are 18 tokens in the data.\n",
      "There are 18 unique tokens in the data.\n",
      "There are 113 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0   nashville          1\n",
      "1  songwriter          1\n",
      "2         ron          1\n",
      "3    hellards          1\n",
      "4       debut          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 92 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       1541          1\n",
      "1        639          1\n",
      "2     things          1\n",
      "3  political          1\n",
      "4   business          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  unwell          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 30 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       stay          1\n",
      "1   positive          1\n",
      "2      midst          1\n",
      "3       much          1\n",
      "4  suckiness          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 79 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      156          1\n",
      "1     news          1\n",
      "2     feed          1\n",
      "3  reading          1\n",
      "4  writing          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 55 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0  photographer          1\n",
      "1        visual          1\n",
      "2   storyteller          1\n",
      "3         based          1\n",
      "4  bergennorway          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 16 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "            token  frequency\n",
      "0  unapproachable          1\n",
      "1              21          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 14 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      man          1\n",
      "1  ancient          1\n",
      "2     race          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 30 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      1483          1\n",
      "1  rolypoly          1\n",
      "2    little          1\n",
      "3       bat          1\n",
      "4     faced          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 28 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      410          1\n",
      "1    older          1\n",
      "2   pretty          1\n",
      "3  version          1\n",
      "4     tina          1\n",
      "There are 10 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 55 characters in the data.\n",
      "The lexical diversity is 0.700 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0      blame          3\n",
      "1       dont          2\n",
      "2   sunshine          1\n",
      "3  moonlight          1\n",
      "4       good          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 72 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  democrati          1\n",
      "1       hate          1\n",
      "2     45fuck          1\n",
      "3   favorite          1\n",
      "4       word          1\n",
      "There are 17 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 96 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       6262          1\n",
      "1  lightning          1\n",
      "2    broncos          1\n",
      "3   sunshine          1\n",
      "4     smiles          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 66 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0        guess          1\n",
      "1     somewhat          1\n",
      "2  attrractive          1\n",
      "3         well          1\n",
      "4        built          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 95 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "            token  frequency\n",
      "0        resister          1\n",
      "1       persister          1\n",
      "2  presidentbiden          1\n",
      "3             blm          1\n",
      "4         science          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 84 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0    loiterer          1\n",
      "1  malcontent          1\n",
      "2  neerdowell          1\n",
      "3       piney          1\n",
      "4   barrensin          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 75 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0   estate          1\n",
      "1    agent          1\n",
      "2     high          1\n",
      "3   school          1\n",
      "4  special          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 8 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  pics          1\n",
      "1  vids          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 17 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  writer          1\n",
      "1   remus          1\n",
      "2  kinnie          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 47 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0           nerd          1\n",
      "1          loves          1\n",
      "2      traveling          1\n",
      "3      languages          1\n",
      "4  international          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 39 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0           128          1\n",
      "1           299          1\n",
      "2  selfloathing          1\n",
      "3          poet          1\n",
      "4        mosaic          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 59 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0     certified          1\n",
      "1      football          1\n",
      "2         coach          1\n",
      "3  𝒮𝒸𝓇ℯℯ𝓃𝓌𝓇𝒾𝓉ℯ𝓇          1\n",
      "4            ms          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  1375          1\n",
      "There are 11 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 80 characters in the data.\n",
      "The lexical diversity is 0.909 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  perfection          2\n",
      "1      sports          1\n",
      "2       minus          1\n",
      "3      hockey          1\n",
      "4  attainable          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 25 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      573          1\n",
      "1  husband          1\n",
      "2      dad          1\n",
      "3  grandpa          1\n",
      "4    views          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 60 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    elect          1\n",
      "1   people          1\n",
      "2  willing          1\n",
      "3    fight          1\n",
      "4  rampant          1\n",
      "There are 18 tokens in the data.\n",
      "There are 18 unique tokens in the data.\n",
      "There are 130 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0       85          1\n",
      "1      168          1\n",
      "2       im          1\n",
      "3  captain          1\n",
      "4    brent          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 5 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  saved          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 12 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0      69          1\n",
      "1  coffee          1\n",
      "2    life          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 14 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "            token  frequency\n",
      "0  themorganmason          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 86 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      born          1\n",
      "1        us          1\n",
      "2  educated          1\n",
      "3     proud          1\n",
      "4       mom          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   ifb          1\n",
      "There are 8 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 34 characters in the data.\n",
      "The lexical diversity is 0.875 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  throw          2\n",
      "1     im          1\n",
      "2  gonna          1\n",
      "3   dont          1\n",
      "4   like          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 43 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0    kindness          1\n",
      "1        love          1\n",
      "2  compassion          1\n",
      "3      repeat          1\n",
      "4       irish          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   mom          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 81 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                  token  frequency\n",
      "0                  5543          1\n",
      "1             womenlead          1\n",
      "2        justicematters          1\n",
      "3  ρηwrescuerstuntkiter          1\n",
      "4            gardenbees          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 11 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     like          1\n",
      "1  economy          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 17 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0    em          1\n",
      "1  cant          1\n",
      "2  take          1\n",
      "3  joke          1\n",
      "4   blm          1\n",
      "There are 11 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 73 characters in the data.\n",
      "The lexical diversity is 0.909 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0        heart          2\n",
      "1   delightful          1\n",
      "2       weirdo          1\n",
      "3        pagan          1\n",
      "4  philosopher          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 86 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0        policy          1\n",
      "1       analyst          1\n",
      "2  specialising          1\n",
      "3       climate          1\n",
      "4        change          1\n",
      "There are 13 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 75 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       jane          1\n",
      "1     trades          1\n",
      "2   theythem          1\n",
      "3  burlesque          1\n",
      "4  performer          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   119          1\n",
      "There are 9 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 48 characters in the data.\n",
      "The lexical diversity is 0.889 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      say          2\n",
      "1    wrong          1\n",
      "2    thing          1\n",
      "3  nothing          1\n",
      "4   meghan          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 14 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0     96          1\n",
      "1    196          1\n",
      "2   рыба          1\n",
      "3  божия          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 43 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0     orakel          1\n",
      "1         al          1\n",
      "2  ingenomen          1\n",
      "3        dus          1\n",
      "4       ging          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 103 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0       livin          1\n",
      "1     florida          1\n",
      "2  mapleleafs          1\n",
      "3    bluejays          1\n",
      "4       49ers          1\n",
      "There are 14 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 82 characters in the data.\n",
      "The lexical diversity is 0.929 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0           dr          2\n",
      "1         dean          1\n",
      "2  gamermodder          1\n",
      "3         like          1\n",
      "4         jill          1\n",
      "There are 12 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 68 characters in the data.\n",
      "The lexical diversity is 0.917 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     good          2\n",
      "1    actor          1\n",
      "2  teacher          1\n",
      "3  dreamer          1\n",
      "4      dog          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 35 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0    twitch          1\n",
      "1  streamer          1\n",
      "2    living          1\n",
      "3      like          1\n",
      "4     trash          1\n",
      "There are 15 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 72 characters in the data.\n",
      "The lexical diversity is 0.667 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  books          6\n",
      "1    377          1\n",
      "2   stay          1\n",
      "3   home          1\n",
      "4    mom          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 34 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    wife          1\n",
      "1  mother          1\n",
      "2  grammy          1\n",
      "3  sister          1\n",
      "4    twin          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 9 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   love          1\n",
      "1  songs          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 32 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0               lives          1\n",
      "1  fairytalesomewhere          1\n",
      "2                 far          1\n",
      "3                  us          1\n",
      "4                find          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 18 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0  redesigning          1\n",
      "1      bitmoji          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 77 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0  schizophrenia          1\n",
      "1     researcher          1\n",
      "2         vision          1\n",
      "3      treatment          1\n",
      "4       musician          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 21 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   seen          1\n",
      "1     tv          1\n",
      "2  local          1\n",
      "3  vegan          1\n",
      "4  joint          1\n",
      "There are 10 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 40 characters in the data.\n",
      "The lexical diversity is 0.900 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0      us          2\n",
      "1    1217          1\n",
      "2    1985          1\n",
      "3    life          1\n",
      "4  leaves          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 106 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "             token  frequency\n",
      "0              420          1\n",
      "1          digital          1\n",
      "2      illustrator          1\n",
      "3        procreate          1\n",
      "4  realismanimanga          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 57 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0    hookladen          1\n",
      "1     hypnotic          1\n",
      "2         west          1\n",
      "3        coast          1\n",
      "4  psychedelic          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 24 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       194          1\n",
      "1     sound          1\n",
      "2  designer          1\n",
      "3  composer          1\n",
      "There are 17 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 112 characters in the data.\n",
      "The lexical diversity is 0.941 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0         2022          2\n",
      "1  bestselling          1\n",
      "2        award          1\n",
      "3      winning          1\n",
      "4       author          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 63 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0  gorillagang          1\n",
      "1          nft          1\n",
      "2    community          1\n",
      "3      polygon          1\n",
      "4        matic          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 31 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0     im          1\n",
      "1    big          1\n",
      "2  waste          1\n",
      "3   time          1\n",
      "4   full          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 35 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  childrens          1\n",
      "1    destiny          1\n",
      "2   remember          1\n",
      "3    choices          1\n",
      "4       made          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 63 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      609          1\n",
      "1       hi          1\n",
      "2       im          1\n",
      "3     back          1\n",
      "4  twitter          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 16 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0      hunger          1\n",
      "1  exhaustion          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 13 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  hibiscus          1\n",
      "1     jerry          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 39 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0      71          1\n",
      "1  little          1\n",
      "2     fox          1\n",
      "3  trying          1\n",
      "4   reach          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 25 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0          make          1\n",
      "1  questionable          1\n",
      "2     decisions          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 76 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0   investor          1\n",
      "1       self          1\n",
      "2  important          1\n",
      "3    toddler          1\n",
      "4   streamer          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 65 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0        love          1\n",
      "1       chill          1\n",
      "2    facebook          1\n",
      "3   instagram          1\n",
      "4  soundcloud          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 69 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0   nature          1\n",
      "1       tv          1\n",
      "2    shows          1\n",
      "3       og          1\n",
      "4  sweetee          1\n",
      "There are 19 tokens in the data.\n",
      "There are 18 unique tokens in the data.\n",
      "There are 85 characters in the data.\n",
      "The lexical diversity is 0.947 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0       en          2\n",
      "1    herbe          1\n",
      "2  juriste          1\n",
      "3   paille          1\n",
      "4      ami          1\n",
      "There are 20 tokens in the data.\n",
      "There are 19 unique tokens in the data.\n",
      "There are 107 characters in the data.\n",
      "The lexical diversity is 0.950 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     dont          2\n",
      "1     1258          1\n",
      "2     3444          1\n",
      "3  twitter          1\n",
      "4  profile          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 82 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0   blogger          1\n",
      "1   founder          1\n",
      "2   posting          1\n",
      "3    review          1\n",
      "4  articles          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   173          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 68 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  songwriter          1\n",
      "1     fashion          1\n",
      "2     stylist          1\n",
      "3      floral          1\n",
      "4       house          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 27 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  1529689122          1\n",
      "1         new          1\n",
      "2     zealand          1\n",
      "3         101          1\n",
      "4        1449          1\n",
      "There are 12 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 80 characters in the data.\n",
      "The lexical diversity is 0.917 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0          lover          2\n",
      "1  snoopdogememe          1\n",
      "2              n          1\n",
      "3          music          1\n",
      "4     enthusiast          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 11 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     upon          1\n",
      "1  request          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 17 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      210          1\n",
      "1       dj          1\n",
      "2  cristal          1\n",
      "3    chiou          1\n",
      "There are 15 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 71 characters in the data.\n",
      "The lexical diversity is 0.933 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    trek          2\n",
      "1     mad          1\n",
      "2    jade          1\n",
      "3  shabby          1\n",
      "4  cabbie          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 44 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0      im          1\n",
      "1     one          1\n",
      "2    side          1\n",
      "3  pillow          1\n",
      "4     got          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 25 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      539          1\n",
      "1    hehim          1\n",
      "2   things          1\n",
      "3  opening          1\n",
      "4     back          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 11 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0  alternativa          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 59 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0           1253          1\n",
      "1          wrong          1\n",
      "2           note          1\n",
      "3  insignificant          1\n",
      "4           play          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 92 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "             token  frequency\n",
      "0            water          1\n",
      "1  worldofwarcraft          1\n",
      "2           hiphop          1\n",
      "3      underground          1\n",
      "4       mainstream          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 18 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0    בדיגיטל          1\n",
      "1         של          1\n",
      "2  kann_news          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 57 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      133          1\n",
      "1  pokemon          1\n",
      "2       go          1\n",
      "3  trailer          1\n",
      "4    levei          1\n",
      "There are 6 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 33 characters in the data.\n",
      "The lexical diversity is 0.833 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       art          2\n",
      "1  director          1\n",
      "2    design          1\n",
      "3   graphic          1\n",
      "4    motion          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 104 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      320          1\n",
      "1     host          1\n",
      "2  stickin          1\n",
      "3  anthony          1\n",
      "4  youtube          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 11 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   kat          1\n",
      "1  love          1\n",
      "2  lamp          1\n",
      "There are 13 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 75 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  metaverse          1\n",
      "1      money          1\n",
      "2       mule          1\n",
      "3  traveling          1\n",
      "4     around          1\n",
      "There are 16 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 125 characters in the data.\n",
      "The lexical diversity is 0.938 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0               voice          2\n",
      "1                2701          1\n",
      "2  httpstcok2ckzbkxvq          1\n",
      "3           voiceover          1\n",
      "4              artist          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 12 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0       81          1\n",
      "1  working          1\n",
      "2      man          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    4678          1\n",
      "1  sheher          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 5 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  posts          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0        dá          1\n",
      "1  gastrite          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 14 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     day          1\n",
      "1  mother          1\n",
      "2   night          1\n",
      "There are 10 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 47 characters in the data.\n",
      "The lexical diversity is 0.900 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       one          2\n",
      "1  jahrgang          1\n",
      "2     thats          1\n",
      "3     small          1\n",
      "4      step          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 41 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0     minor          1\n",
      "1        64          1\n",
      "2       506          1\n",
      "3    stream          1\n",
      "4  sawayama          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 31 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  macintosh          1\n",
      "1      apple          1\n",
      "2     granny          1\n",
      "3      smith          1\n",
      "4     choice          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0     2          1\n",
      "1  meet          1\n",
      "2     u          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 33 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                 token  frequency\n",
      "0  1247099510393798656          1\n",
      "1                   69          1\n",
      "2                 4258          1\n",
      "3                 pink          1\n",
      "4                 life          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 5 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  hehim          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 19 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                 token  frequency\n",
      "0  kendrathehumanbeing          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 9 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  positions          1\n",
      "There are 10 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 57 characters in the data.\n",
      "The lexical diversity is 0.900 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0        poche          2\n",
      "1          129          1\n",
      "2     desidero          1\n",
      "3  condividere          1\n",
      "4         miei          1\n",
      "There are 13 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 126 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                    token  frequency\n",
      "0                    2853          1\n",
      "1  screenwriterplaywright          1\n",
      "2               tallagent          1\n",
      "3                    work          1\n",
      "4             development          1\n",
      "There are 20 tokens in the data.\n",
      "There are 20 unique tokens in the data.\n",
      "There are 130 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       9606          1\n",
      "1       1937          1\n",
      "2  historian          1\n",
      "3   19th20th          1\n",
      "4    century          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 17 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  simply          1\n",
      "1   great          1\n",
      "2  person          1\n",
      "There are 13 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 89 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0       music          1\n",
      "1  journalist          1\n",
      "2      making          1\n",
      "3        good          1\n",
      "4       words          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 78 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  blaydon          1\n",
      "1    lover          1\n",
      "2     nufc          1\n",
      "3    sport          1\n",
      "4    music          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0    82          1\n",
      "1  3079          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 16 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    314          1\n",
      "1  crash          1\n",
      "2   burn          1\n",
      "3   girl          1\n",
      "There are 10 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 71 characters in the data.\n",
      "The lexical diversity is 0.900 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0       provoke          2\n",
      "1      thoughts          1\n",
      "2  conversation          1\n",
      "3         truth          1\n",
      "4         women          1\n",
      "There are 16 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 116 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0        364          1\n",
      "1       3028          1\n",
      "2    podcast          1\n",
      "3   chatting          1\n",
      "4  musicians          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 95 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "            token  frequency\n",
      "0       historian          1\n",
      "1       funkateer          1\n",
      "2           grant          1\n",
      "3          writer          1\n",
      "4  librarycompany          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 57 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0    volunteer          1\n",
      "1  spontaneous          1\n",
      "2       dancer          1\n",
      "3     frequent          1\n",
      "4       poster          1\n",
      "There are 9 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 47 characters in the data.\n",
      "The lexical diversity is 0.889 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0    toronto          2\n",
      "1        572          1\n",
      "2      carpe          1\n",
      "3       diem          1\n",
      "4  brazilian          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 74 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    bona          1\n",
      "1    vada          1\n",
      "2  lovely          1\n",
      "3     eek          1\n",
      "4  antifa          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 29 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0           não          1\n",
      "1         venha          1\n",
      "2           com          1\n",
      "3  positividade          1\n",
      "4        tóxica          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    lost          1\n",
      "1  graces          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 28 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0     channel          1\n",
      "1     youtube          1\n",
      "2  reedanubis          1\n",
      "3        join          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 8 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  make          1\n",
      "1  feel          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 13 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0     music          1\n",
      "1  politics          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 16 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   hehim          1\n",
      "1  gluten          1\n",
      "2   enemy          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 57 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      110          1\n",
      "1      349          1\n",
      "2     data          1\n",
      "3  science          1\n",
      "4   policy          1\n",
      "There are 16 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 83 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0    boston          1\n",
      "1       349          1\n",
      "2       832          1\n",
      "3  overtone          1\n",
      "4    window          1\n",
      "There are 22 tokens in the data.\n",
      "There are 22 unique tokens in the data.\n",
      "There are 112 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       1124          1\n",
      "1  bostonian          1\n",
      "2      proud          1\n",
      "3    xennial          1\n",
      "4   lifelong          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 112 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "               token  frequency\n",
      "0              lover          1\n",
      "1  japansylvianduran          1\n",
      "2             gusgus          1\n",
      "3           shamrock          1\n",
      "4             rovers          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 12 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   mouth          1\n",
      "1  hethey          1\n",
      "2       x          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 122 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                                       token  frequency\n",
      "0                                    fxxking          1\n",
      "1  explorersdirectorproducersingersongwriter          1\n",
      "2                  actresscomedianpositivity          1\n",
      "3                            influencerowner          1\n",
      "4                                       fuck          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0      im          1\n",
      "1     fan          1\n",
      "2    many          1\n",
      "3  things          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 42 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0           33          1\n",
      "1          674          1\n",
      "2  adventurous          1\n",
      "3         life          1\n",
      "4   strategist          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 102 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                        token  frequency\n",
      "0                       lover          1\n",
      "1                  everything          1\n",
      "2              aaliyahempress          1\n",
      "3  edeniatsgoddessbdbassassin          1\n",
      "4                hivedaughter          1\n",
      "There are 14 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 71 characters in the data.\n",
      "The lexical diversity is 0.929 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0        like          2\n",
      "1          76          1\n",
      "2  brantfords          1\n",
      "3        gold          1\n",
      "4        mine          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 60 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     lord          1\n",
      "1  grammar          1\n",
      "2     cool          1\n",
      "3     aunt          1\n",
      "4   writer          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 2 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0    85          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 44 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                                      token  frequency\n",
      "0                                      1304          1\n",
      "1  wifemotherscientistprofessorfriendgymrat          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 34 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0    canadian          1\n",
      "1        film          1\n",
      "2      worker          1\n",
      "3      coffee          1\n",
      "4  enthusiast          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 74 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  4503051134          1\n",
      "1      160827          1\n",
      "2      170909          1\n",
      "3         258          1\n",
      "4         336          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 20 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   happy          1\n",
      "1     fun          1\n",
      "2  person          1\n",
      "3  around          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 17 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0       44          1\n",
      "1      599          1\n",
      "2       𝙳𝚘          1\n",
      "3      𝙽𝚘𝚝          1\n",
      "4  𝙳𝚒𝚜𝚝𝚞𝚛𝚋          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  life          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 38 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0      im          1\n",
      "1   ready          1\n",
      "2   heres          1\n",
      "3  chucky          1\n",
      "4    want          1\n",
      "There are 17 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 111 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      dump          1\n",
      "1        uk          1\n",
      "2      6508          1\n",
      "3      6722          1\n",
      "4  musicpet          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 36 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0         mind          1\n",
      "1       visual          1\n",
      "2  storyteller          1\n",
      "3     contents          1\n",
      "4      creator          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 23 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       flip          1\n",
      "1     follow          1\n",
      "2       flop          1\n",
      "3  oiiiiiiio          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   fsu          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 42 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     finn          1\n",
      "1   abroad          1\n",
      "2  coastal          1\n",
      "3    rower          1\n",
      "4      wep          1\n",
      "There are 9 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 37 characters in the data.\n",
      "The lexical diversity is 0.778 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0       ɪꜱ          2\n",
      "1       ᴍʏ          2\n",
      "2      820          1\n",
      "3  ꜰᴀꜱʜɪᴏɴ          1\n",
      "4    ʙɪᴛᴄʜ          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 16 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  pronouns          1\n",
      "1  theythem          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 26 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0      music          1\n",
      "1  publicist          1\n",
      "2       lime          1\n",
      "3   pictures          1\n",
      "There are 15 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 97 characters in the data.\n",
      "The lexical diversity is 0.933 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  creative          2\n",
      "1       271          1\n",
      "2     owner          1\n",
      "3     coach          1\n",
      "4      csco          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  tell          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 53 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0   things          1\n",
      "1   talent          1\n",
      "2  ontario          1\n",
      "3  digital          1\n",
      "4  service          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 16 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  struggle          1\n",
      "1  progress          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 13 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     new          1\n",
      "1    kind          1\n",
      "2  bright          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 18 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0         10          1\n",
      "1         50          1\n",
      "2  liverpool          1\n",
      "3         fc          1\n",
      "4        fan          1\n",
      "There are 14 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 83 characters in the data.\n",
      "The lexical diversity is 0.929 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      less          2\n",
      "1      dare          1\n",
      "2  powerful          1\n",
      "3       use          1\n",
      "4  strength          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 29 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0          776          1\n",
      "1         2389          1\n",
      "2         jazz          1\n",
      "3  sagittarius          1\n",
      "4      arsenal          1\n",
      "There are 10 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 60 characters in the data.\n",
      "The lexical diversity is 0.900 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0       pt          2\n",
      "1     kate          1\n",
      "2  moseley          1\n",
      "3   walter          1\n",
      "4    murch          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 73 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0   creative          1\n",
      "1  readlearn          1\n",
      "2  something          1\n",
      "3      every          1\n",
      "4        day          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 11 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       moi          1\n",
      "1  victoire          1\n",
      "There are 11 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 66 characters in the data.\n",
      "The lexical diversity is 0.818 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0          law          3\n",
      "1    nonbinary          1\n",
      "2  linguistics          1\n",
      "3       nlproc          1\n",
      "4           ai          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 42 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       live          1\n",
      "1  musicgigs          1\n",
      "2   swfcwhat          1\n",
      "3      hokey          1\n",
      "4      cokey          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 64 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0        weirdo          1\n",
      "1  punkàtisanes          1\n",
      "2    metalheadz          1\n",
      "3            du          1\n",
      "4      dimanche          1\n",
      "There are 17 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 89 characters in the data.\n",
      "The lexical diversity is 0.941 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0          es          2\n",
      "1          el          1\n",
      "2        niño          1\n",
      "3  pandillero          1\n",
      "4        pero          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 17 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0         views          1\n",
      "1  brokenrecord          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 24 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    cant          1\n",
      "1     get          1\n",
      "2  enough          1\n",
      "3  golden          1\n",
      "4   crisp          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  3051          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 18 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    feel          1\n",
      "1    real          1\n",
      "2    life          1\n",
      "3  appeal          1\n",
      "There are 9 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 57 characters in the data.\n",
      "The lexical diversity is 0.889 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0      music          2\n",
      "1  spraffing          1\n",
      "2       void          1\n",
      "3       make          1\n",
      "4       like          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 29 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   acid          1\n",
      "1  fried          1\n",
      "2  disco          1\n",
      "3  freak          1\n",
      "4  punky          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       ony          1\n",
      "1      true          1\n",
      "2  religion          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 34 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0      radio          1\n",
      "1  superstar          1\n",
      "2    general          1\n",
      "3     hottie          1\n",
      "4    shethey          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 47 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  wangal          1\n",
      "1    land          1\n",
      "2     477          1\n",
      "3    1135          1\n",
      "4  sheher          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   524          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 21 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0       03012022          1\n",
      "1  recoveryposse          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 20 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     703          1\n",
      "1   hehim          1\n",
      "2  repost          1\n",
      "3  things          1\n",
      "There are 17 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 101 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  popsugar          1\n",
      "1     music          1\n",
      "2      wiki          1\n",
      "3        ny          1\n",
      "4    native          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 21 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       guru          1\n",
      "1   entering          1\n",
      "2  metaverse          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 44 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                      token  frequency\n",
      "0                   gráfico          1\n",
      "1                 intereses          1\n",
      "2  diseñofotografíadocencia          1\n",
      "3                      moda          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 114 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                  token  frequency\n",
      "0         wordspictures          1\n",
      "1  aplingtonparkersburg          1\n",
      "2               dikenew          1\n",
      "3              hartford          1\n",
      "4     gladbrookreinbeck          1\n",
      "There are 10 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 59 characters in the data.\n",
      "The lexical diversity is 0.900 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0      people          2\n",
      "1       music          1\n",
      "2  literature          1\n",
      "3       scifi          1\n",
      "4        nerd          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 40 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0           151936739          1\n",
      "1             perdido          1\n",
      "2                 231          1\n",
      "3                 274          1\n",
      "4  httpstcomxzbpyvzvq          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 86 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0         fmr          1\n",
      "1    director          1\n",
      "2   marketing          1\n",
      "3  livenation          1\n",
      "4          ea          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 49 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                   token  frequency\n",
      "0  other_other_wes_moore          1\n",
      "1                   hate          1\n",
      "2                  colts          1\n",
      "3                   dead          1\n",
      "4                cashapp          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 72 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "              token  frequency\n",
      "0         quadruple          1\n",
      "1            threat          1\n",
      "2                ny          1\n",
      "3               pop          1\n",
      "4  singersongwriter          1\n",
      "There are 17 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 111 characters in the data.\n",
      "The lexical diversity is 0.941 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  stroemer          2\n",
      "1      4918          1\n",
      "2        fm          1\n",
      "3     frank          1\n",
      "4    marcel          1\n",
      "There are 17 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 90 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   likes          1\n",
      "1   words          1\n",
      "2   human          1\n",
      "3  rights          1\n",
      "4    head          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  fria          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   289          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 14 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  welly          1\n",
      "1   stop          1\n",
      "2     42          1\n",
      "3    295          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 94 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  american          1\n",
      "1      girl          1\n",
      "2   england          1\n",
      "3      love          1\n",
      "4    movies          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 36 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0      זה          1\n",
      "1   לטובת          1\n",
      "2  המדינה          1\n",
      "3     ויש          1\n",
      "4      לי          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  lose          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 7 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   506          1\n",
      "1  4025          1\n",
      "There are 13 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 87 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  chocolate          1\n",
      "1   business          1\n",
      "2      spend          1\n",
      "3    winters          1\n",
      "4  somewhere          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 92 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0     senior          1\n",
      "1     editor          1\n",
      "2      nbclx          1\n",
      "3  todayshow          1\n",
      "4     people          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 52 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "             token  frequency\n",
      "0             film          1\n",
      "1           critic          1\n",
      "2              vap          1\n",
      "3       emory_film          1\n",
      "4  unreconstructed          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 25 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       pair          1\n",
      "1   monsters          1\n",
      "2       love          1\n",
      "3  immencely          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 50 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    awful          1\n",
      "1   person          1\n",
      "2      far          1\n",
      "3  perfect          1\n",
      "4  minimum          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 82 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       1638          1\n",
      "1  relocated          1\n",
      "2   londoner          1\n",
      "3      noise          1\n",
      "4      maker          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   543          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 88 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0     designer          1\n",
      "1         alta          1\n",
      "2      costura          1\n",
      "3   reconocida          1\n",
      "4  trayectoria          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 94 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0         head          1\n",
      "1  development          1\n",
      "2  cooperation          1\n",
      "3      swedish          1\n",
      "4      embassy          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 19 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   1369          1\n",
      "1     ja          1\n",
      "2    jag          1\n",
      "3  säger          1\n",
      "4     då          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 13 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  secretly          1\n",
      "1     welsh          1\n",
      "There are 16 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 109 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                 token  frequency\n",
      "0                  oct          1\n",
      "1  1345837033911103490          1\n",
      "2         philadelphia          1\n",
      "3                   pa          1\n",
      "4                  109          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 22 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     ribs          1\n",
      "1  hundred          1\n",
      "2   dollar          1\n",
      "3    bills          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 19 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     284          1\n",
      "1     123          1\n",
      "2  closet          1\n",
      "3     pop          1\n",
      "4    star          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 62 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0        kart          1\n",
      "1      addict          1\n",
      "2        rice          1\n",
      "3     pudding          1\n",
      "4  enthusiast          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 84 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0         447          1\n",
      "1        1737          1\n",
      "2      writer          1\n",
      "3  cocreative          1\n",
      "4    director          1\n",
      "There are 14 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 68 characters in the data.\n",
      "The lexical diversity is 0.786 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "             token  frequency\n",
      "0             much          4\n",
      "1               97          1\n",
      "2              351          1\n",
      "3            hehim          1\n",
      "4  francoaméricain          1\n",
      "There are 17 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 109 characters in the data.\n",
      "The lexical diversity is 0.882 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   star          2\n",
      "1   wars          2\n",
      "2    usa          1\n",
      "3  18872          1\n",
      "4   1240          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 75 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  would          1\n",
      "1    say          1\n",
      "2   love          1\n",
      "3   hurt          1\n",
      "4    lot          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 41 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0   always          1\n",
      "1    brush          1\n",
      "2    teeth          1\n",
      "3  morning          1\n",
      "4    going          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 83 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0   worldwide          1\n",
      "1       music          1\n",
      "2  collection          1\n",
      "3         get          1\n",
      "4         new          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 89 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0     sciencespo          1\n",
      "1             23          1\n",
      "2  investigative          1\n",
      "3     journalist          1\n",
      "4         taiwan          1\n",
      "There are 16 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 103 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0  storyteller          1\n",
      "1         avid          1\n",
      "2     consumer          1\n",
      "3      current          1\n",
      "4       events          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 17 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0   player          1\n",
      "1     beer          1\n",
      "2  drinker          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 58 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "            token  frequency\n",
      "0         conduce          1\n",
      "1              cj          1\n",
      "2        carballo          1\n",
      "3             por          1\n",
      "4  radiosucesosok          1\n",
      "There are 15 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 106 characters in the data.\n",
      "The lexical diversity is 0.933 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      code          2\n",
      "1  memberof          1\n",
      "2      paul          1\n",
      "3   emanuel          1\n",
      "4     gavin          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 70 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     728          1\n",
      "1    4261          1\n",
      "2  former          1\n",
      "3    body          1\n",
      "4  double          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   n11          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 20 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0      83          1\n",
      "1    1134          1\n",
      "2  heaven          1\n",
      "3     las          1\n",
      "4   vegas          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 41 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "             token  frequency\n",
      "0              ucu          1\n",
      "1  gloucestershire          1\n",
      "2           branch          1\n",
      "3            local          1\n",
      "4         national          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  grimes          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 7 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  setlist          1\n",
      "There are 11 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 97 characters in the data.\n",
      "The lexical diversity is 0.909 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0                  ja          2\n",
      "1            asioiden          1\n",
      "2        asiantuntija          1\n",
      "3      maaltamuuttaja          1\n",
      "4  ammattisaamelainen          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 40 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0     songwriter          1\n",
      "1         cantor          1\n",
      "2     compositor          1\n",
      "3              7          1\n",
      "4  capricorniano          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   raw          1\n",
      "There are 7 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 40 characters in the data.\n",
      "The lexical diversity is 0.714 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0      would          2\n",
      "1     prefer          2\n",
      "2  historian          1\n",
      "3       bike          1\n",
      "4      hehim          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 26 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  think          1\n",
      "1   know          1\n",
      "2   doin          1\n",
      "3   dont          1\n",
      "4   mean          1\n",
      "There are 24 tokens in the data.\n",
      "There are 23 unique tokens in the data.\n",
      "There are 128 characters in the data.\n",
      "The lexical diversity is 0.958 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                 token  frequency\n",
      "0                    e          2\n",
      "1  1226707756771610624          1\n",
      "2                   24          1\n",
      "3                  473          1\n",
      "4                 cest          1\n",
      "There are 8 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 40 characters in the data.\n",
      "The lexical diversity is 0.875 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0            de          2\n",
      "1           soy          1\n",
      "2  sulamericano          1\n",
      "3         feira          1\n",
      "4       santana          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 72 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0       17813          1\n",
      "1        3171          1\n",
      "2           1          1\n",
      "3  throwbacks          1\n",
      "4        call          1\n",
      "There are 21 tokens in the data.\n",
      "There are 21 unique tokens in the data.\n",
      "There are 116 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0        street          1\n",
      "1        travel          1\n",
      "2  photographer          1\n",
      "3          nyer          1\n",
      "4       blogger          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    baby          1\n",
      "1  corner          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 45 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  commercial          1\n",
      "1       music          1\n",
      "2         uws          1\n",
      "3       views          1\n",
      "4         etc          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 39 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0         im          1\n",
      "1       read          1\n",
      "2    peoples          1\n",
      "3     tweets          1\n",
      "4  followers          1\n",
      "There are 10 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 99 characters in the data.\n",
      "The lexical diversity is 0.900 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0           dj          2\n",
      "1  broadcaster          1\n",
      "2        bbcr1          1\n",
      "3      monthur          1\n",
      "4         47am          1\n",
      "There are 16 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 85 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  aussie          1\n",
      "1   likes          1\n",
      "2  travel          1\n",
      "3     fun          1\n",
      "4  quirky          1\n",
      "There are 7 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 82 characters in the data.\n",
      "The lexical diversity is 0.857 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0              editor          2\n",
      "1        theskinnymag          1\n",
      "2  nadiatheskinnycouk          1\n",
      "3           freelance          1\n",
      "4          journalist          1\n",
      "There are 15 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 124 characters in the data.\n",
      "The lexical diversity is 0.800 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  united          2\n",
      "1      20          2\n",
      "2   times          2\n",
      "3     588          1\n",
      "4    till          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 73 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0         mids          1\n",
      "1         9752          1\n",
      "2          692          1\n",
      "3  commentator          1\n",
      "4      chelsea          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  travel          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 7 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  kitchen          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 34 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0   momento          1\n",
      "1  correcto          1\n",
      "2      para          1\n",
      "3   olvidar          1\n",
      "4        el          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 56 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0  973392060958027776          1\n",
      "1                 san          1\n",
      "2                josé          1\n",
      "3               costa          1\n",
      "4                rica          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 35 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  singing          1\n",
      "1  dancing          1\n",
      "2  sneaker          1\n",
      "3     head          1\n",
      "4    funny          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 37 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0  musicpeople          1\n",
      "1         adam          1\n",
      "2      lambert          1\n",
      "3     glambert          1\n",
      "4      forever          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 28 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  scottish          1\n",
      "1     actor          1\n",
      "2    seeing          1\n",
      "3      life          1\n",
      "4     takes          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 24 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0       maths          1\n",
      "1  enthusiast          1\n",
      "2   raconteur          1\n",
      "There are 17 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 109 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       lotr          1\n",
      "1   starwars          1\n",
      "2  bathrugby          1\n",
      "3   mkdonsfc          1\n",
      "4         ex          1\n",
      "There are 11 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 69 characters in the data.\n",
      "The lexical diversity is 0.909 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    music          2\n",
      "1      137          1\n",
      "2      280          1\n",
      "3  podcast          1\n",
      "4   hosted          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 115 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  jornalista          1\n",
      "1    designer          1\n",
      "2     criador          1\n",
      "3     podcast          1\n",
      "4    primeiro          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 9 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     sun          1\n",
      "1  desert          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 33 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  como          1\n",
      "1     é          1\n",
      "2   bom          1\n",
      "3   ser          1\n",
      "4  lelé          1\n",
      "There are 17 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 91 characters in the data.\n",
      "The lexical diversity is 0.882 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0       n          2\n",
      "1     day          2\n",
      "2     602          1\n",
      "3    like          1\n",
      "4  sports          1\n",
      "There are 17 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 111 characters in the data.\n",
      "The lexical diversity is 0.941 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0            cio          2\n",
      "1        bootsuk          1\n",
      "2        advisor          1\n",
      "3  silverbuck_uk          1\n",
      "4          chair          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 7 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   115          1\n",
      "1  1510          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 18 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     solo          1\n",
      "1       rt          1\n",
      "2    cosas          1\n",
      "3  chuscas          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 30 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "               token  frequency\n",
      "0           lafuerza          1\n",
      "1  christinaaguilera          1\n",
      "2              xtina          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 13 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0  consciousness          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 33 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0      reader          1\n",
      "1     trekkie          1\n",
      "2  potterhead          1\n",
      "3  buffyholic          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 35 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0      alias          1\n",
      "1       dont          1\n",
      "2      faith          1\n",
      "3  basically          1\n",
      "4      makes          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   old          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 17 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0        223          1\n",
      "1      blink          1\n",
      "2  therefore          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  student          1\n",
      "1    sewer          1\n",
      "2      rat          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 29 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0    napkins          1\n",
      "1    instead          1\n",
      "2     paying          1\n",
      "3  attention          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 27 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    queen          1\n",
      "1  belfast          1\n",
      "2    based          1\n",
      "3     drag          1\n",
      "4   artist          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 56 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0             un          1\n",
      "1            peu          1\n",
      "2       beaucoup          1\n",
      "3  passionnément          1\n",
      "4            les          1\n",
      "There are 18 tokens in the data.\n",
      "There are 18 unique tokens in the data.\n",
      "There are 98 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  escritor          1\n",
      "1  buscando          1\n",
      "2    editor          1\n",
      "3      para          1\n",
      "4       mis          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 101 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0      bilbao          1\n",
      "1  estudiante          1\n",
      "2          de          1\n",
      "3         phd          1\n",
      "4          en          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0     3          1\n",
      "1   140          1\n",
      "There are 18 tokens in the data.\n",
      "There are 18 unique tokens in the data.\n",
      "There are 110 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0         1334          1\n",
      "1         1736          1\n",
      "2  lancastrian          1\n",
      "3      history          1\n",
      "4       nature          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 63 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0        middle          1\n",
      "1         world          1\n",
      "2       ecuador          1\n",
      "3  hardspinnerz          1\n",
      "4            dj          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 85 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0         780          1\n",
      "1        used          1\n",
      "2  leftypants          1\n",
      "3      sheher          1\n",
      "4       lover          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 35 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       name          1\n",
      "1      scott          1\n",
      "2         im          1\n",
      "3  liverpool          1\n",
      "4  supporter          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 100 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "            token  frequency\n",
      "0              85          1\n",
      "1             303          1\n",
      "2        linguist          1\n",
      "3  anthropologist          1\n",
      "4           rower          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 37 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  brasil          1\n",
      "1      46          1\n",
      "2     185          1\n",
      "3    dele          1\n",
      "4   hehim          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 41 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   heart          1\n",
      "1    hips          1\n",
      "2    body          1\n",
      "3    love          1\n",
      "4  trying          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0     es          1\n",
      "1   amor          1\n",
      "2   nada          1\n",
      "3  falta          1\n",
      "There are 12 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 60 characters in the data.\n",
      "The lexical diversity is 0.917 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0      im          2\n",
      "1    name          1\n",
      "2   tyler          1\n",
      "3     bad          1\n",
      "4  person          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 18 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0        tiktok          1\n",
      "1  sagitari_art          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 30 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0      orphan          1\n",
      "1    gaytimes          1\n",
      "2  everything          1\n",
      "3      memoir          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 19 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  twitter          1\n",
      "1     cant          1\n",
      "2       20          1\n",
      "3   phxphl          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 23 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0             es          1\n",
      "1             un          1\n",
      "2         riesgo          1\n",
      "3  chiefskingdom          1\n",
      "There are 17 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 119 characters in the data.\n",
      "The lexical diversity is 0.941 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0      adult          2\n",
      "1       2552          1\n",
      "2     artist          1\n",
      "3  performer          1\n",
      "4        web          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 54 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0   degular          1\n",
      "1      girl          1\n",
      "2     gamer          1\n",
      "3  fulltime          1\n",
      "4   student          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 41 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0   shibarmy          1\n",
      "1  shibtrain          1\n",
      "2      kishu          1\n",
      "3        feg          1\n",
      "4       shib          1\n",
      "There are 8 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 29 characters in the data.\n",
      "The lexical diversity is 0.875 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0       eu          2\n",
      "1      511          1\n",
      "2      sem          1\n",
      "3  rótulos          1\n",
      "4      sou          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 25 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0        ron          1\n",
      "1     endres          1\n",
      "2    hampton          1\n",
      "3  minnesota          1\n",
      "There are 17 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 74 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     128          1\n",
      "1    1329          1\n",
      "2     act          1\n",
      "3     age          1\n",
      "4  people          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 71 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    alexa          1\n",
      "1    music          1\n",
      "2  bangtan          1\n",
      "3   twtot7          1\n",
      "4  jhope86          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 29 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  family          1\n",
      "1   trust          1\n",
      "2     one          1\n",
      "3    stay          1\n",
      "4       5          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 26 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0     reason          1\n",
      "1       keep          1\n",
      "2  chapstick          1\n",
      "3    bedside          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 37 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0   reckon          1\n",
      "1   tweets          1\n",
      "2     made          1\n",
      "3  copious          1\n",
      "4  amounts          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 7 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  theyshe          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 25 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0    nj          1\n",
      "1  born          1\n",
      "2  bred          1\n",
      "3  cape          1\n",
      "4   cod          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  family          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 51 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0          1130          1\n",
      "1          1900          1\n",
      "2         music          1\n",
      "3  photographer          1\n",
      "4           nft          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 11 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    amo          1\n",
      "1    mis          1\n",
      "2  hijos          1\n",
      "There are 12 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 72 characters in the data.\n",
      "The lexical diversity is 0.917 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  también          2\n",
      "1     fine          1\n",
      "2     pero          1\n",
      "3   really          1\n",
      "4     amix          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 45 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  married          1\n",
      "1   father          1\n",
      "2      two          1\n",
      "3  amazing          1\n",
      "4     boys          1\n",
      "There are 13 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 67 characters in the data.\n",
      "The lexical diversity is 0.923 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0        buy          2\n",
      "1        822          1\n",
      "2  sometimes          1\n",
      "3        big          1\n",
      "4      issue          1\n",
      "There are 15 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 74 characters in the data.\n",
      "The lexical diversity is 0.867 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  horror          2\n",
      "1  movies          2\n",
      "2      76          1\n",
      "3     230          1\n",
      "4   sober          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 47 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0      social          1\n",
      "1       media          1\n",
      "2    activist          1\n",
      "3   extrovert          1\n",
      "4  fearlessly          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 36 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  bloghouse          1\n",
      "1     elders          1\n",
      "2      black          1\n",
      "3      trans          1\n",
      "4      lives          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 19 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  literally          1\n",
      "1     always          1\n",
      "2       work          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 43 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       755          1\n",
      "1   atheist          1\n",
      "2  optimist          1\n",
      "3     quiet          1\n",
      "4  achiever          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 13 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0           30          1\n",
      "1  capricórnio          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 66 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       sun          1\n",
      "1    pisces          1\n",
      "2      moon          1\n",
      "3  aquarius          1\n",
      "4  rising__          1\n",
      "There are 25 tokens in the data.\n",
      "There are 23 unique tokens in the data.\n",
      "There are 115 characters in the data.\n",
      "The lexical diversity is 0.920 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0       la          2\n",
      "1      los          2\n",
      "2      301          1\n",
      "3  abogada          1\n",
      "4    gusta          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 27 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  fingers          1\n",
      "1      got          1\n",
      "2    tired          1\n",
      "3  playing          1\n",
      "4    fetch          1\n",
      "There are 16 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 75 characters in the data.\n",
      "The lexical diversity is 0.875 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0      de          2\n",
      "1     soy          2\n",
      "2   pelis          1\n",
      "3  series          1\n",
      "4      un          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 73 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0     consumer          1\n",
      "1   interviews          1\n",
      "2  songwriters          1\n",
      "3    producers          1\n",
      "4        music          1\n",
      "There are 18 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 109 characters in the data.\n",
      "The lexical diversity is 0.944 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0       de          2\n",
      "1   gentil          1\n",
      "2     mais          1\n",
      "3      pas          1\n",
      "4  commode          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   219          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 9 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   dull          1\n",
      "1  world          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 29 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "               token  frequency\n",
      "0                 22          1\n",
      "1  snapchatinstagram          1\n",
      "2         mattieg706          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 26 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      beta          1\n",
      "1  blockers          1\n",
      "2     large          1\n",
      "3       ice          1\n",
      "4    coffee          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 20 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      1887          1\n",
      "1  maranhão          1\n",
      "2       pro          1\n",
      "3     mundo          1\n",
      "There are 19 tokens in the data.\n",
      "There are 19 unique tokens in the data.\n",
      "There are 110 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0        350          1\n",
      "1  launching          1\n",
      "2        9th          1\n",
      "3        may          1\n",
      "4       2022          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 23 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  always          1\n",
      "1  forget          1\n",
      "2   water          1\n",
      "3  plants          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 92 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       dark          1\n",
      "1        cow          1\n",
      "2    mystery          1\n",
      "3       lord          1\n",
      "4  protector          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 35 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                      token  frequency\n",
      "0                       zum          1\n",
      "1                  hausarzt          1\n",
      "2  energiegenossenschaftler          1\n",
      "There are 16 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 67 characters in the data.\n",
      "The lexical diversity is 0.938 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0    le          2\n",
      "1   qué          1\n",
      "2  hace          1\n",
      "3   una          1\n",
      "4  raya          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 8 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  music          1\n",
      "1    man          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 14 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      quem          1\n",
      "1        vc          1\n",
      "2  esperava          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   eat          1\n",
      "1  shit          1\n",
      "2  live          1\n",
      "3  bill          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 110 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0    graduate          1\n",
      "1  university          1\n",
      "2  nottingham          1\n",
      "3    training          1\n",
      "4    contract          1\n",
      "There are 16 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 84 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0        29          1\n",
      "1       259          1\n",
      "2   average          1\n",
      "3  canadian          1\n",
      "4       pro          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 31 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      1209          1\n",
      "1  lifelong          1\n",
      "2    writer          1\n",
      "3    artist          1\n",
      "4   scholar          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 58 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0                porn          1\n",
      "1               butts          1\n",
      "2               dicks          1\n",
      "3               music          1\n",
      "4  httpstco5ynho3e4d9          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 9 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  beautiful          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 64 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       102          1\n",
      "1       670          1\n",
      "2   villain          1\n",
      "3  someones          1\n",
      "4     story          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 51 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     897          1\n",
      "1    west          1\n",
      "2   texas          1\n",
      "3  native          1\n",
      "4   insta          1\n",
      "There are 26 tokens in the data.\n",
      "There are 26 unique tokens in the data.\n",
      "There are 117 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   741          1\n",
      "1  nsfw          1\n",
      "2  24yr          1\n",
      "3   old          1\n",
      "4  anon          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 43 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0        11          1\n",
      "1       164          1\n",
      "2      know          1\n",
      "3     youre          1\n",
      "4  thinking          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 26 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0       science          1\n",
      "1          ucla          1\n",
      "2  neuroscience          1\n",
      "3           phd          1\n",
      "There are 22 tokens in the data.\n",
      "There are 22 unique tokens in the data.\n",
      "There are 115 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  angeles          1\n",
      "1       ca          1\n",
      "2     2514          1\n",
      "3     2198          1\n",
      "4     ucla          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 8 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0      23          1\n",
      "1  hethey          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 116 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      3377          1\n",
      "1  musician          1\n",
      "2     lover          1\n",
      "3     champ          1\n",
      "4    hutler          1\n",
      "There are 16 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 80 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  diggitall          1\n",
      "1     native          1\n",
      "2        mes          1\n",
      "3     tweets          1\n",
      "4  nenragent          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 49 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      306          1\n",
      "1      998          1\n",
      "2   expres          1\n",
      "3  freedom          1\n",
      "4   golden          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 20 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0          12          1\n",
      "1          49          1\n",
      "2        male          1\n",
      "3          18          1\n",
      "4  homosexual          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 87 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0          schumacher          1\n",
      "1                 mv1          1\n",
      "2           supercars          1\n",
      "3               speed          1\n",
      "4  redbullracinghonda          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 35 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0       baseado          1\n",
      "1            em          1\n",
      "2         fatos          1\n",
      "3  irrelevantes          1\n",
      "4             e          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 56 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0     obsessed          1\n",
      "1  celebrities          1\n",
      "2         band          1\n",
      "3       called          1\n",
      "4    snakeskin          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 19 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   1671          1\n",
      "1   deus          1\n",
      "2  acima          1\n",
      "3     de          1\n",
      "4   tudo          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   bio          1\n",
      "There are 10 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 79 characters in the data.\n",
      "The lexical diversity is 0.900 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   check          2\n",
      "1      im          1\n",
      "2  rapper          1\n",
      "3     nft          1\n",
      "4  artist          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 19 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  repetitive          1\n",
      "1   aesthetic          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 38 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "              token  frequency\n",
      "0               gay          1\n",
      "1               420          1\n",
      "2          advocate          1\n",
      "3          outdoors          1\n",
      "4  blacklivesmatter          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 31 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                   token  frequency\n",
      "0                  ashes          1\n",
      "1                  story          1\n",
      "2  tellistandwithukraine          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 28 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       1553          1\n",
      "1    compton          1\n",
      "2   counting          1\n",
      "3  political          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 45 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  beautiful          1\n",
      "1   children          1\n",
      "2     sophie          1\n",
      "3      lewis          1\n",
      "4        boy          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 21 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  fishing          1\n",
      "1     good          1\n",
      "2     time          1\n",
      "3   cheers          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 11 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  fuck          1\n",
      "1  nypd          1\n",
      "2   pds          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 25 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  hethey          1\n",
      "1  camera          1\n",
      "2    roll          1\n",
      "3   looks          1\n",
      "4    like          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  ames          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 35 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   feel          1\n",
      "1   body          1\n",
      "2  shift          1\n",
      "3  world          1\n",
      "4   next          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 31 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0             espelho          1\n",
      "1  757703878817247232          1\n",
      "2                 106          1\n",
      "3                 164          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 33 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       love          1\n",
      "1       quad          1\n",
      "2     skates          1\n",
      "3       dark          1\n",
      "4  chocolate          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 32 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0    ingeniero          1\n",
      "1  informático          1\n",
      "2         pucp          1\n",
      "3     melómano          1\n",
      "There are 20 tokens in the data.\n",
      "There are 20 unique tokens in the data.\n",
      "There are 101 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "            token  frequency\n",
      "0           bruna          1\n",
      "1          passos          1\n",
      "2          amaral          1\n",
      "3  anjadodeadline          1\n",
      "4          ganhei          1\n",
      "There are 15 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 97 characters in the data.\n",
      "The lexical diversity is 0.933 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0          news          2\n",
      "1          alum          1\n",
      "2  lifeatpurdue          1\n",
      "3        uisedu          1\n",
      "4          siuc          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 82 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     402          1\n",
      "1   scott          1\n",
      "2       k          1\n",
      "3   smith          1\n",
      "4  artist          1\n",
      "There are 7 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 69 characters in the data.\n",
      "The lexical diversity is 0.857 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                        token  frequency\n",
      "0                       throw          2\n",
      "1                      wizard          1\n",
      "2              selfproclaimed          1\n",
      "3  pacifist29heshetheynsfwwhy          1\n",
      "4                   handswhen          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 27 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0                hago          1\n",
      "1                trap          1\n",
      "2                   p          1\n",
      "3  httpstcow7plnh0p8f          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 8 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  hurt          1\n",
      "1  ways          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  wrong          1\n",
      "1  wilde          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 5 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  ganas          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 13 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  check          1\n",
      "1   cool          1\n",
      "2   door          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 21 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    dando          1\n",
      "1       mi          1\n",
      "2  humilde          1\n",
      "3  opinión          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 64 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   actor          1\n",
      "1  writer          1\n",
      "2   manny          1\n",
      "3  cw4400          1\n",
      "4   andre          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  fermenting          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 29 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0       25          1\n",
      "1  writing          1\n",
      "2    music          1\n",
      "3      fan          1\n",
      "4    small          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 13 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0    21          1\n",
      "1   550          1\n",
      "2  well          1\n",
      "3  dumb          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 43 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0   enthusiast          1\n",
      "1   mozzarella          1\n",
      "2        stick          1\n",
      "3  connoisseur          1\n",
      "4           go          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  simp          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 18 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0          insta          1\n",
      "1  eumarcantonio          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 23 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                 token  frequency\n",
      "0  1297531018593931265          1\n",
      "1                    1          1\n",
      "2                  312          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 26 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "             token  frequency\n",
      "0              198          1\n",
      "1  troubleshooting          1\n",
      "2         humanity          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 19 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0        good          1\n",
      "1       vibes          1\n",
      "2  tannedskin          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 64 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  assistindo          1\n",
      "1        vida          1\n",
      "2      passar          1\n",
      "3         por          1\n",
      "4         uma          1\n",
      "There are 11 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 64 characters in the data.\n",
      "The lexical diversity is 0.909 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0           e          2\n",
      "1  militância          1\n",
      "2    biscoito          1\n",
      "3          se          1\n",
      "4    conhecem          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 7 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   ive          1\n",
      "1  seen          1\n",
      "There are 8 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 28 characters in the data.\n",
      "The lexical diversity is 0.750 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0      är          2\n",
      "1    puck          2\n",
      "2     129          1\n",
      "3  bollen          1\n",
      "4    rund          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 8 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  november          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 26 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0         art          1\n",
      "1  apreciator          1\n",
      "2     amateur          1\n",
      "3      dancer          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 83 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0        world          1\n",
      "1      reveals          1\n",
      "2       travel          1\n",
      "3         foot          1\n",
      "4  underground          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 18 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  industrial          1\n",
      "1    designer          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 39 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0         432          1\n",
      "1          um          1\n",
      "2       corpo          1\n",
      "3  nãobinário          1\n",
      "4          em          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 17 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  emperor          1\n",
      "1      egg          1\n",
      "2  feather          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 54 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0  httpstcokpeav2ytdt          1\n",
      "1  httpstcoxgse17ecsy          1\n",
      "2  httpstcocpd3mdkyzu          1\n",
      "There are 9 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 36 characters in the data.\n",
      "The lexical diversity is 0.889 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0       en          2\n",
      "1     1073          1\n",
      "2    ebrio          1\n",
      "3      las          1\n",
      "4  sombras          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 2 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0    sf          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 45 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0   oysters          1\n",
      "1      wine          1\n",
      "2   pending          1\n",
      "3   dipwset          1\n",
      "4  pandemic          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 28 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    glass          1\n",
      "1      179          1\n",
      "2      865          1\n",
      "3  dorinda          1\n",
      "4   singin          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 24 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  kitchen          1\n",
      "1  pretend          1\n",
      "2   coffee          1\n",
      "3     shop          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 9 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0         x          1\n",
      "1  theythem          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 28 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0     closest          1\n",
      "1         ive          1\n",
      "2        ever          1\n",
      "3  practicing          1\n",
      "4        bdsm          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 62 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0  zerahürelai          1\n",
      "1           17          1\n",
      "2          102          1\n",
      "3       artist          1\n",
      "4       writer          1\n",
      "There are 16 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 91 characters in the data.\n",
      "The lexical diversity is 0.938 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0    kristin          2\n",
      "1        way          1\n",
      "2      words          1\n",
      "3  emotional          1\n",
      "4     sultry          1\n",
      "There are 13 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 95 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0          phd          1\n",
      "1    candidate          1\n",
      "2  uoftenglish          1\n",
      "3      ancient          1\n",
      "4        early          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 11 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       gay          1\n",
      "1  opinions          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 11 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  doodoo          1\n",
      "1   jones          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 19 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0       witches          1\n",
      "1  gatopapasfzn          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 12 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    still          1\n",
      "1  emanate          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 71 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     say          1\n",
      "1  wealth          1\n",
      "2    mind          1\n",
      "3  pocket          1\n",
      "4  likely          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 13 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0  saranwrapping          1\n",
      "There are 5 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 39 characters in the data.\n",
      "The lexical diversity is 0.800 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0  firefighter          2\n",
      "1       london          1\n",
      "2           im          1\n",
      "3    australia          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   466          1\n",
      "There are 16 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 110 characters in the data.\n",
      "The lexical diversity is 0.938 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "             token  frequency\n",
      "0            movie          2\n",
      "1             1268          1\n",
      "2  writerperformer          1\n",
      "3          comedic          1\n",
      "4           things          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 8 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  kinozeit          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 12 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    movie          1\n",
      "1  enjoyer          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 9 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  anarchist          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 24 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "              token  frequency\n",
      "0  blacklivesmatter          1\n",
      "1          landback          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 28 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     693          1\n",
      "1      ok          1\n",
      "2   bitch          1\n",
      "3  better          1\n",
      "4  follow          1\n",
      "There are 11 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 49 characters in the data.\n",
      "The lexical diversity is 0.909 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  jameson          2\n",
      "1    santa          1\n",
      "2     cruz          1\n",
      "3      290          1\n",
      "4      481          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 34 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  simbólico          1\n",
      "1    reseñas          1\n",
      "2         de          1\n",
      "3    álbumes          1\n",
      "4         en          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 28 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    402          1\n",
      "1   1340          1\n",
      "2  didnt          1\n",
      "3   even          1\n",
      "4    say          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 5 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  salad          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 41 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  archive          1\n",
      "1  biggest          1\n",
      "2    songs          1\n",
      "3     main          1\n",
      "4      pop          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 19 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     say          1\n",
      "1  except          1\n",
      "2      im          1\n",
      "3    lose          1\n",
      "4    time          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 9 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   male          1\n",
      "1  virgo          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 11 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0  unforgiving          1\n",
      "There are 6 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 29 characters in the data.\n",
      "The lexical diversity is 0.833 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     need          2\n",
      "1     dont          1\n",
      "2     edit          1\n",
      "3   button          1\n",
      "4  forgive          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 62 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       bay          1\n",
      "1       try          1\n",
      "2  discreet          1\n",
      "3   willing          1\n",
      "4       btm          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 7 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  lukas          1\n",
      "1     li          1\n",
      "There are 8 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 25 characters in the data.\n",
      "The lexical diversity is 0.875 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  falo          2\n",
      "1   120          1\n",
      "2   516          1\n",
      "3   mal          1\n",
      "4   mas          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 20 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0     cerrado          1\n",
      "1         por          1\n",
      "2  demolición          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 26 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      rio          1\n",
      "1       de          1\n",
      "2  janeiro          1\n",
      "3   brazil          1\n",
      "4     1998          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 27 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       1008          1\n",
      "1   𝖙𝖗æ𝖛𝖊𝖘𝖙𝖎          1\n",
      "2  𝖋𝖚𝖙𝖚𝖗𝖎𝖘𝖙𝖆          1\n",
      "3     elaelu          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 32 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0          like          1\n",
      "1    rhinestone          1\n",
      "2        cowboy          1\n",
      "3  hehimchicago          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 46 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0       hohum          1\n",
      "1    humanoid          1\n",
      "2  occasional          1\n",
      "3        hoot          1\n",
      "4       hehim          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 38 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0            22          1\n",
      "1        please          1\n",
      "2       contact          1\n",
      "3   information          1\n",
      "4  selfimmolate          1\n",
      "There are 5 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 28 characters in the data.\n",
      "The lexical diversity is 0.800 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0        tired          2\n",
      "1         live          1\n",
      "2          die          1\n",
      "3  igeuphaire_          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 11 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  ainda          1\n",
      "1     tô          1\n",
      "2   aqui          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0     gosto          1\n",
      "1        de          1\n",
      "2  reclamar          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  deprimente          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 9 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  situation          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 24 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "            token  frequency\n",
      "0            gods          1\n",
      "1          docile          1\n",
      "2  creatures24yrs          1\n",
      "There are 13 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 82 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0       e          1\n",
      "1  milito          1\n",
      "2     mas          1\n",
      "3  também          1\n",
      "4   estou          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 13 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0      da          1\n",
      "1   minha          1\n",
      "2  cabeça          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 24 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    rage          1\n",
      "1    like          1\n",
      "2    love          1\n",
      "3  hatred          1\n",
      "4  turned          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 117 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0          plays          1\n",
      "1          drums          1\n",
      "2     gang_signs          1\n",
      "3  actorstheband          1\n",
      "4      bluejband          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 39 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0   tumblr          1\n",
      "1     gurl          1\n",
      "2   living          1\n",
      "3  twitter          1\n",
      "4       ig          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 72 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "             token  frequency\n",
      "0            south          1\n",
      "1          african          1\n",
      "2             film          1\n",
      "3             grad          1\n",
      "4  instrumentalist          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 48 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       swe          1\n",
      "1     fword          1\n",
      "2  comitted          1\n",
      "3     hehim          1\n",
      "4        18          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 18 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0     think          1\n",
      "1  anything          1\n",
      "2     funny          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 77 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0  dermatologist          1\n",
      "1       tweeting          1\n",
      "2       skincare          1\n",
      "3          lgbtq          1\n",
      "4    dermatology          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 18 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0     thirty          1\n",
      "1  something          1\n",
      "2        nhs          1\n",
      "There are 19 tokens in the data.\n",
      "There are 19 unique tokens in the data.\n",
      "There are 101 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    libra          1\n",
      "1   gemini          1\n",
      "2      252          1\n",
      "3      441          1\n",
      "4  classic          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 84 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  march          1\n",
      "1     25          1\n",
      "2    tix          1\n",
      "3   sale          1\n",
      "4     us          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 35 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0     pa          1\n",
      "1    101          1\n",
      "2    460          1\n",
      "3    sad          1\n",
      "4  songs          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0     32          1\n",
      "1   1213          1\n",
      "2  whats          1\n",
      "3   mood          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 11 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0      44          1\n",
      "1     331          1\n",
      "2  sheher          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 32 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "              token  frequency\n",
      "0  blacklivesmatter          1\n",
      "1  translivesmatter          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  napper          1\n",
      "There are 12 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 55 characters in the data.\n",
      "The lexical diversity is 0.833 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    1975          2\n",
      "1  settle          2\n",
      "2      96          1\n",
      "3     406          1\n",
      "4  crying          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   251          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 23 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     like          1\n",
      "1    write          1\n",
      "2  special          1\n",
      "3  moments          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 51 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0           526          1\n",
      "1        family          1\n",
      "2       friends          1\n",
      "3      traveler          1\n",
      "4  occupational          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 37 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    rina          1\n",
      "1  mitski          1\n",
      "2    cure          1\n",
      "3      19          1\n",
      "4  hethey          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  nature          1\n",
      "There are 16 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 84 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0         𝘿𝙖𝙛𝙩          1\n",
      "1  forthcoming          1\n",
      "2         book          1\n",
      "3         daft          1\n",
      "4         punk          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 16 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     self          1\n",
      "1  respect          1\n",
      "2    hehim          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 14 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  truly          1\n",
      "1   king          1\n",
      "2  kings          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    live          1\n",
      "1  inside          1\n",
      "2   dream          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 66 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  producer          1\n",
      "1    singer          1\n",
      "2       guy          1\n",
      "3   friends          1\n",
      "4      know          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 33 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0            892          1\n",
      "1  confrontation          1\n",
      "2     resolution          1\n",
      "3        closure          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 2 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0    61          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 85 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0        dj          1\n",
      "1    artist          1\n",
      "2  musician          1\n",
      "3    making          1\n",
      "4    sounds          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  room          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   936          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 45 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0          7565          1\n",
      "1           465          1\n",
      "2  909worldwide          1\n",
      "3         black          1\n",
      "4         lives          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 27 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "              token  frequency\n",
      "0           twitter          1\n",
      "1              stan          1\n",
      "2  blacklivesmatter          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   tpa          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 38 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     high          1\n",
      "1    femme          1\n",
      "2   hethem          1\n",
      "3      far          1\n",
      "4  leftist          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 14 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0      insta          1\n",
      "1  observado          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      397          1\n",
      "1  eleélhe          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      dog          1\n",
      "1  dumbass          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 27 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0        772          1\n",
      "1       2091          1\n",
      "2  insincere          1\n",
      "3      point          1\n",
      "4     nausea          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 23 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0      26          1\n",
      "1      im          1\n",
      "2  trying          1\n",
      "3     get          1\n",
      "4    lost          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 53 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  wamgrambie          1\n",
      "1       trans          1\n",
      "2     gremlin          1\n",
      "3      brenda          1\n",
      "4     brigade          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 44 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    por          1\n",
      "1  favor          1\n",
      "2    217          1\n",
      "3    450          1\n",
      "4  gosto          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 27 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0  crystalline          1\n",
      "1        amber          1\n",
      "2       guilds          1\n",
      "3        cheek          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 8 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  theythem          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   fun          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 12 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  social          1\n",
      "1  worker          1\n",
      "There are 3 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 0.667 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   lie          2\n",
      "1  next          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  shadow          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 8 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    gay          1\n",
      "1  music          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 28 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0  suffocating          1\n",
      "1        world          1\n",
      "2          die          1\n",
      "3         feel          1\n",
      "4        alive          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 47 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     meme          1\n",
      "1  twitter          1\n",
      "2     bear          1\n",
      "3     play          1\n",
      "4    games          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 48 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0    atlantic          1\n",
      "1         day          1\n",
      "2     varsity          1\n",
      "3  basketball          1\n",
      "4       coach          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 29 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       1008          1\n",
      "1  cocinando          1\n",
      "2       ando          1\n",
      "3        con          1\n",
      "4         mi          1\n",
      "There are 17 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 116 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  4241780062          1\n",
      "1  chromatica          1\n",
      "2          87          1\n",
      "3         706          1\n",
      "4    autistic          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 7 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  inmate          1\n",
      "1       2          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 33 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0      un          1\n",
      "1   perro          1\n",
      "2   cinco          1\n",
      "3   gatos          1\n",
      "4  gustan          1\n",
      "There are 16 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 92 characters in the data.\n",
      "The lexical diversity is 0.875 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0        ed          2\n",
      "1       guy          2\n",
      "2  guardian          1\n",
      "3        b4          1\n",
      "4    editor          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 36 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    178          1\n",
      "1   1810          1\n",
      "2  hehim          1\n",
      "3     17          1\n",
      "4  nelly          1\n",
      "There are 10 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 98 characters in the data.\n",
      "The lexical diversity is 0.900 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       dni          2\n",
      "1       107          1\n",
      "2      4756          1\n",
      "3    minors          1\n",
      "4  antivaxx          1\n",
      "There are 17 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 81 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0  minneapolis          1\n",
      "1           mn          1\n",
      "2          336          1\n",
      "3         1574          1\n",
      "4       reader          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 27 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     top          1\n",
      "1  doggin          1\n",
      "2    thru          1\n",
      "3    life          1\n",
      "4    good          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 75 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       1290          1\n",
      "1     future          1\n",
      "2      music          1\n",
      "3  therapist          1\n",
      "4  caretaker          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 44 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  peter          1\n",
      "1     24          1\n",
      "2  nerdy          1\n",
      "3  proud          1\n",
      "4   part          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 18 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0     medio          1\n",
      "1     gamer          1\n",
      "2  comelona          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 56 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0      pores          1\n",
      "1     bigger          1\n",
      "2    seattle          1\n",
      "3      based          1\n",
      "4  colombian          1\n",
      "There are 16 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 89 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0  nationswell          1\n",
      "1        makes          1\n",
      "2        video          1\n",
      "3        games          1\n",
      "4     americas          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 2 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0    ルネ          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 46 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "              token  frequency\n",
      "0            editor          1\n",
      "1      highsnobiety          1\n",
      "2              lurk          1\n",
      "3          formerly          1\n",
      "4  hypebaehypebeast          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 108 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0        cantora          1\n",
      "1         bêbada          1\n",
      "2              e          1\n",
      "3  hipnotizadora          1\n",
      "4     fracassada          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 8 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  beep          1\n",
      "1  boop          1\n",
      "There are 17 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 117 characters in the data.\n",
      "The lexical diversity is 0.941 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "             token  frequency\n",
      "0               en          2\n",
      "1          abogado          1\n",
      "2     especialista          1\n",
      "3       conflictos          1\n",
      "4  internacionales          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 16 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      pop          1\n",
      "1  culture          1\n",
      "2   savant          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 65 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0         im          1\n",
      "1       ilsa          1\n",
      "2  absurdist          1\n",
      "3      clown          1\n",
      "4      wants          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 70 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  3027376426          1\n",
      "1          62          1\n",
      "2         251          1\n",
      "3        stay          1\n",
      "4        soft          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 39 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      1071          1\n",
      "1        ig          1\n",
      "2  luisgvra          1\n",
      "3     every          1\n",
      "4       day          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 20 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0       broke          1\n",
      "1       queer          1\n",
      "2  millennial          1\n",
      "There are 17 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 103 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0      cantor          1\n",
      "1  compositor          1\n",
      "2    produtor          1\n",
      "3          ex          1\n",
      "4    coroinha          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 60 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      1237          1\n",
      "1       993          1\n",
      "2   nothing          1\n",
      "3       mix          1\n",
      "4  stardust          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 23 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "            token  frequency\n",
      "0             357          1\n",
      "1            baby          1\n",
      "2              im          1\n",
      "3  existentialist          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 26 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  kindred          1\n",
      "1  spirits          1\n",
      "2   janice          1\n",
      "3   rodney          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 66 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0      italy          1\n",
      "1   pursuing          1\n",
      "2   artistic          1\n",
      "3       life          1\n",
      "4  sometimes          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 18 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0    musician          1\n",
      "1  shoplifter          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 41 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0          like          1\n",
      "1         fairy          1\n",
      "2  princessthat          1\n",
      "3       resides          1\n",
      "4          pits          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 94 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0  httpstcor5tooncozc          1\n",
      "1              twitch          1\n",
      "2  httpstcowvht93da8a          1\n",
      "3           instagram          1\n",
      "4  httpstcoco1mhynrbk          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 67 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0     plays          1\n",
      "1    guitar          1\n",
      "2     sings          1\n",
      "3  shopping          1\n",
      "4     trash          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 28 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  trans          1\n",
      "1  gamer          1\n",
      "2  idiot          1\n",
      "3  bitch          1\n",
      "4     cw          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 27 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0      de          1\n",
      "1      mi          1\n",
      "2    vida          1\n",
      "3      yo          1\n",
      "4  caminé          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 19 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      time          1\n",
      "1  illusion          1\n",
      "2       eat          1\n",
      "3      rich          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 20 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  candidate          1\n",
      "1        dog          1\n",
      "2        dad          1\n",
      "3      hehim          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 21 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   176          1\n",
      "1  1572          1\n",
      "2  𝑓𝑖𝑟𝑒          1\n",
      "3  𝑤𝑎𝑙𝑘          1\n",
      "4  𝑤𝑖𝑡𝒉          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 54 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0    striving          1\n",
      "1  mediocrity          1\n",
      "2         one          1\n",
      "3         day          1\n",
      "4        time          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 45 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0   writer          1\n",
      "1   poetry          1\n",
      "2    plays          1\n",
      "3  fiction          1\n",
      "4      big          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 19 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  stylelos          1\n",
      "1    barcos          1\n",
      "2        se          1\n",
      "3       van          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 59 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0               links          1\n",
      "1  httpstcojvvqj77mqz          1\n",
      "2  httpstcoiyladpyslo          1\n",
      "3  httpstcoe9gm53np9c          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 36 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     like          1\n",
      "1  yelling          1\n",
      "2       tv          1\n",
      "3   olsens          1\n",
      "4     kind          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 12 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    witch          1\n",
      "1  midwest          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 11 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  waiting          1\n",
      "1     room          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 73 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     nový          1\n",
      "1  podcast          1\n",
      "2  rozchod          1\n",
      "3       je          1\n",
      "4    venku          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 84 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "             token  frequency\n",
      "0              phd          1\n",
      "1        candidate          1\n",
      "2             uoft          1\n",
      "3  kingscollegelon          1\n",
      "4       interested          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 9 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  petri          1\n",
      "1   dish          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 13 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      487          1\n",
      "1  alright          1\n",
      "2      gig          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  1795          1\n",
      "There are 13 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 92 characters in the data.\n",
      "The lexical diversity is 0.846 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       afro          2\n",
      "1      tunes          2\n",
      "2  afromusic          1\n",
      "3  listening          1\n",
      "4   pleasure          1\n",
      "There are 19 tokens in the data.\n",
      "There are 19 unique tokens in the data.\n",
      "There are 114 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0      podcast          1\n",
      "1          two          1\n",
      "2  30something          1\n",
      "3         best          1\n",
      "4      friends          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  qualquer          1\n",
      "1   pronome          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 39 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "            token  frequency\n",
      "0          sheher          1\n",
      "1              19          1\n",
      "2  uttaraphalguni          1\n",
      "3            ptbr          1\n",
      "4              tw          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 91 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0       store          1\n",
      "1          25          1\n",
      "2       years          1\n",
      "3  experience          1\n",
      "4     working          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 11 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     make          1\n",
      "1  boylike          1\n",
      "There are 16 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 109 characters in the data.\n",
      "The lexical diversity is 0.938 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0      radio          2\n",
      "1  worldwide          1\n",
      "2     sounds          1\n",
      "3      local          1\n",
      "4   audience          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 88 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "              token  frequency\n",
      "0                ga          1\n",
      "1       advertising          1\n",
      "2           twitter          1\n",
      "3            sheher          1\n",
      "4  blacklivesmatter          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  love          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 8 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  white          1\n",
      "1    man          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 71 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     fan          1\n",
      "1    nats          1\n",
      "2   grogu          1\n",
      "3  brunch          1\n",
      "4   small          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 76 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  creadora          1\n",
      "1    memera          1\n",
      "2     aviso          1\n",
      "3    cuando          1\n",
      "4      pasa          1\n",
      "There are 16 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 83 characters in the data.\n",
      "The lexical diversity is 0.938 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0      scary          2\n",
      "1  masochist          1\n",
      "2      likes          1\n",
      "3       crap          1\n",
      "4     scared          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 40 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0  hollowearth          1\n",
      "1    thursdayz          1\n",
      "2           57          1\n",
      "3          pst          1\n",
      "4       hosted          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 17 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0       cosas          1\n",
      "1          yo          1\n",
      "2  transformo          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 41 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      love          1\n",
      "1  watching          1\n",
      "2   youtube          1\n",
      "3    videos          1\n",
      "4       tik          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 92 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0           2968          1\n",
      "1   professional          1\n",
      "2  artistgraphic          1\n",
      "3       designer          1\n",
      "4       creating          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 48 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0    things          1\n",
      "1  changing          1\n",
      "2     world          1\n",
      "3   current          1\n",
      "4   project          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 95 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0  httpstco9zda9umj6q          1\n",
      "1  httpstcopgpaeukbyd          1\n",
      "2            fridaetv          1\n",
      "3              maison          1\n",
      "4               matta          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 9 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   луд          1\n",
      "1   жив          1\n",
      "2   съм          1\n",
      "There are 10 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 112 characters in the data.\n",
      "The lexical diversity is 0.900 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "               token  frequency\n",
      "0                sri          2\n",
      "1              music          1\n",
      "2    producernftsbjj          1\n",
      "3              black          1\n",
      "4  beltyogimeditator          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 5 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  laugh          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 33 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0        84          1\n",
      "1       644          1\n",
      "2  displace          1\n",
      "3     guilt          1\n",
      "4   embrace          1\n",
      "There are 22 tokens in the data.\n",
      "There are 22 unique tokens in the data.\n",
      "There are 123 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  217168147          1\n",
      "1    atlanta          1\n",
      "2         ga          1\n",
      "3      20824          1\n",
      "4       3079          1\n",
      "There are 18 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 100 characters in the data.\n",
      "The lexical diversity is 0.944 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      thy          2\n",
      "1   empire          1\n",
      "2    chaos          1\n",
      "3  restord          1\n",
      "4    light          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 35 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0  independent          1\n",
      "1        music          1\n",
      "2     sneakers          1\n",
      "3          npr          1\n",
      "4          air          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  make          1\n",
      "1  mine          1\n",
      "2    99          1\n",
      "There are 9 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 49 characters in the data.\n",
      "The lexical diversity is 0.889 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0   person          2\n",
      "1      put          1\n",
      "2     rage          1\n",
      "3  average          1\n",
      "4   boring          1\n",
      "There are 23 tokens in the data.\n",
      "There are 23 unique tokens in the data.\n",
      "There are 134 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0   british          1\n",
      "1  columbia          1\n",
      "2       289          1\n",
      "3      1711          1\n",
      "4     expat          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 16 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       210          1\n",
      "1       amc          1\n",
      "2  sundance          1\n",
      "3        tv          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 26 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  dalailama          1\n",
      "1          e          1\n",
      "2         os          1\n",
      "3     monges          1\n",
      "4   budistas          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 14 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   pero          1\n",
      "1  buena          1\n",
      "2  gente          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 19 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0   caralho          1\n",
      "1      tava          1\n",
      "2  dormindo          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 14 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  ethirium          1\n",
      "1       bnb          1\n",
      "2       nft          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 17 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "               token  frequency\n",
      "0  ヒカマニのコラ画像も作ってイキマス          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   718          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 26 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0      vegan          1\n",
      "1    toronto          1\n",
      "2  instagram          1\n",
      "3      liz_l          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 8 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  move          1\n",
      "1  fast          1\n",
      "There are 5 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 26 characters in the data.\n",
      "The lexical diversity is 0.800 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    sweet          2\n",
      "1      234          1\n",
      "2  without          1\n",
      "3   bitter          1\n",
      "There are 6 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 32 characters in the data.\n",
      "The lexical diversity is 0.833 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    comes          2\n",
      "1    𝐋𝐢𝐛𝐞𝐫          1\n",
      "2  freedom          1\n",
      "3     𝐓𝐞𝐞𝐧          1\n",
      "4   spirit          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 14 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   ohsea          1\n",
      "1  bwiatl          1\n",
      "2     chi          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 20 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  billion          1\n",
      "1     year          1\n",
      "2      old          1\n",
      "3   carbon          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 72 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "             token  frequency\n",
      "0             5247          1\n",
      "1         director          1\n",
      "2      citynewsvan          1\n",
      "3  citynewscalgary          1\n",
      "4             amma          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 19 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  confused          1\n",
      "1   someone          1\n",
      "2      else          1\n",
      "There are 17 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 109 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "             token  frequency\n",
      "0             1824          1\n",
      "1        assistant          1\n",
      "2         producer          1\n",
      "3  bbcradioulsters          1\n",
      "4  stephenmainline          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 88 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "             token  frequency\n",
      "0            world          1\n",
      "1        squirrels          1\n",
      "2         director          1\n",
      "3               pr          1\n",
      "4  avidreaderpress          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 82 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0            60          1\n",
      "1            63          1\n",
      "2          love          1\n",
      "3   progressive          1\n",
      "4  technologies          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  close          1\n",
      "1  today          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0   running          1\n",
      "1  scissors          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  killer          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 50 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0             en          1\n",
      "1        someart          1\n",
      "2       cultures          1\n",
      "3  civilizations          1\n",
      "4          light          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  stan          1\n",
      "There are 17 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 97 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0   phoole          1\n",
      "1     gang          1\n",
      "2     live          1\n",
      "3  fridays          1\n",
      "4      6pm          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 14 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  communism          1\n",
      "1      shite          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 41 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0             6          1\n",
      "1           147          1\n",
      "2         honor          1\n",
      "3       sustain          1\n",
      "4  entrepreneur          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 20 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     life          1\n",
      "1     lets          1\n",
      "2       go          1\n",
      "3  brandon          1\n",
      "4      fjb          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 57 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0  switzerland          1\n",
      "1            4          1\n",
      "2            5          1\n",
      "3   ishowspeed          1\n",
      "4           ft          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 70 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     2654          1\n",
      "1  fiction          1\n",
      "2   writer          1\n",
      "3   trader          1\n",
      "4    plant          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  76ers          1\n",
      "1      4          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 66 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                                            token  frequency\n",
      "0  veterinarianmonmouthuniversity24animechapter18          1\n",
      "1                            weebnejiitachisasori          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 72 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0             vasquez          1\n",
      "1              family          1\n",
      "2            bulldogs          1\n",
      "3  bulldogsareawesome          1\n",
      "4             seattle          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 18 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "            token  frequency\n",
      "0  entertainments          1\n",
      "1            mine          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   man          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 9 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   ရယသသ          1\n",
      "1  အသကရည          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 36 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0      reader          1\n",
      "1      runner          1\n",
      "2          hr          1\n",
      "3  technology          1\n",
      "4         man          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 34 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    kylie          1\n",
      "1  minogue          1\n",
      "2     star          1\n",
      "3     trek          1\n",
      "4    theme          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0        old          1\n",
      "1        one          1\n",
      "2  suspended          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 50 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0     stock          1\n",
      "1    brings          1\n",
      "2      best          1\n",
      "3   english          1\n",
      "4  speaking          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 5 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  hehim          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 13 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0  bigbrotherfan          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  father          1\n",
      "1    four          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 22 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  creative          1\n",
      "1        31          1\n",
      "2   glasgow          1\n",
      "3     hehim          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       410          1\n",
      "1  superior          1\n",
      "2      bird          1\n",
      "There are 8 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 50 characters in the data.\n",
      "The lexical diversity is 0.875 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0     trans          2\n",
      "1     woman          1\n",
      "2    sheher          1\n",
      "3  activist          1\n",
      "4   fashion          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 66 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0        game          1\n",
      "1     culture          1\n",
      "2  journalist          1\n",
      "3     pinball          1\n",
      "4  enthusiast          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 71 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0   household          1\n",
      "1        name          1\n",
      "2        work          1\n",
      "3    involved          1\n",
      "4  protecting          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 13 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   based          1\n",
      "1  durham          1\n",
      "2      nc          1\n",
      "There are 16 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 103 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      656          1\n",
      "1  cornish          1\n",
      "2   kernow          1\n",
      "3     born          1\n",
      "4    loves          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 32 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0         normal          1\n",
      "1  circumstances          1\n",
      "2            hot          1\n",
      "3         temper          1\n",
      "4           lost          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 31 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0                 ast          1\n",
      "1                  yn          1\n",
      "2                cael          1\n",
      "3  drostootmcancerleo          1\n",
      "4                cusp          1\n",
      "There are 13 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 75 characters in the data.\n",
      "The lexical diversity is 0.923 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0         call          2\n",
      "1           70          1\n",
      "2          769          1\n",
      "3         made          1\n",
      "4  philippines          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 83 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0        436          1\n",
      "1       1031          1\n",
      "2     amazon          1\n",
      "3  affiliate          1\n",
      "4     member          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  dwomoh          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 22 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0                xxvi          1\n",
      "1  httpstconesu65ixha          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 21 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0          pop          1\n",
      "1      culture          1\n",
      "2  connoisseur          1\n",
      "There are 16 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 86 characters in the data.\n",
      "The lexical diversity is 0.875 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   dont          2\n",
      "1    eat          2\n",
      "2     b1          1\n",
      "3   cook          1\n",
      "4  clean          1\n",
      "There are 13 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 77 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0     living          1\n",
      "1     dreams          1\n",
      "2  observing          1\n",
      "3      music          1\n",
      "4       life          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 25 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0   another          1\n",
      "1  everyday          1\n",
      "2      sane          1\n",
      "3    psycho          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 12 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      para          1\n",
      "1  eskuchar          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 11 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  really          1\n",
      "1   going          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 5 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  hehim          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 37 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       loud          1\n",
      "1    mouthed          1\n",
      "2  obnoxious          1\n",
      "3       dont          1\n",
      "4       care          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   493          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 35 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0         756          1\n",
      "1         ill          1\n",
      "2        take          1\n",
      "3  homosexual          1\n",
      "4     culture          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 38 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0        2139          1\n",
      "1  sheherhers          1\n",
      "2        live          1\n",
      "3        dash          1\n",
      "4          ig          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 17 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0       im          1\n",
      "1     wide          1\n",
      "2  variety          1\n",
      "3     kink          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 33 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      256          1\n",
      "1      489          1\n",
      "2  𝓦𝓮𝓵𝓬𝓸𝓶𝓮          1\n",
      "3       𝓽𝓸          1\n",
      "4       𝓶𝔂          1\n",
      "There are 4 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 16 characters in the data.\n",
      "The lexical diversity is 0.750 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  loser          2\n",
      "1    org          1\n",
      "2    fwm          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 9 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  modern          1\n",
      "1     man          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 5 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  right          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 45 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    music          1\n",
      "1      art          1\n",
      "2  writing          1\n",
      "3  dancing          1\n",
      "4  general          1\n",
      "There are 24 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 93 characters in the data.\n",
      "The lexical diversity is 0.708 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0     en          3\n",
      "1     ik          3\n",
      "2   mijn          2\n",
      "3   site          2\n",
      "4  kefir          2\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 26 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0     𝗦𝗵𝗮𝗿𝗶𝗻𝗴          1\n",
      "1   𝗱𝗶𝗳𝗳𝗲𝗿𝗲𝗻𝘁          1\n",
      "2  𝗵𝗲𝗮𝗿𝘁𝗯𝗲𝗮𝘁𝘀          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 26 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0    follow          1\n",
      "1      drag          1\n",
      "2     queen          1\n",
      "3       dog          1\n",
      "4  accounts          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 57 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0        tech          1\n",
      "1  excitement          1\n",
      "2         run          1\n",
      "3        chat          1\n",
      "4   marketing          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 19 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0         hey          1\n",
      "1      friend          1\n",
      "2  everywhere          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  cats          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  1002          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 23 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      1100          1\n",
      "1      4969          1\n",
      "2   serious          1\n",
      "3  goofball          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 8 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  divinest          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 26 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  associated          1\n",
      "1     britney          1\n",
      "2   celebrity          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 9 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  antitrump          1\n",
      "There are 16 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 104 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0           1418          1\n",
      "1      economist          1\n",
      "2  international          1\n",
      "3       monetary          1\n",
      "4           fund          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 31 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      word          1\n",
      "1      says          1\n",
      "2        im          1\n",
      "3  possible          1\n",
      "4    audrey          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  galera          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 32 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0     trump          1\n",
      "1  congress          1\n",
      "2  assisted          1\n",
      "3   rioters          1\n",
      "4      1621          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 12 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0       2          1\n",
      "1       3          1\n",
      "2       4          1\n",
      "3     let          1\n",
      "4  scream          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  kisses          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 24 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  falling          1\n",
      "1     love          1\n",
      "2  present          1\n",
      "3   loving          1\n",
      "There are 18 tokens in the data.\n",
      "There are 18 unique tokens in the data.\n",
      "There are 87 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0      three          1\n",
      "1  wonderful          1\n",
      "2   children          1\n",
      "3     animal          1\n",
      "4      lover          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 28 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  island          1\n",
      "1     170          1\n",
      "2     967          1\n",
      "3  nicest          1\n",
      "4     guy          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 79 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  otippat          1\n",
      "1   mycket          1\n",
      "2    sport          1\n",
      "3    varit          1\n",
      "4    mamma          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 20 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   311          1\n",
      "1  live          1\n",
      "2   ill          1\n",
      "3  give          1\n",
      "4   ive          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 7 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  scratch          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 104 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0        editor          1\n",
      "1         comms          1\n",
      "2  professional          1\n",
      "3      tweeting          1\n",
      "4          life          1\n",
      "There are 17 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 89 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0         laura          1\n",
      "1         daddy          1\n",
      "2         leila          1\n",
      "3        amelie          1\n",
      "4  philadelphia          1\n",
      "There are 5 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 27 characters in the data.\n",
      "The lexical diversity is 0.800 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    might          2\n",
      "1    tweet          1\n",
      "2  depends          1\n",
      "3    arsed          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0     4          1\n",
      "1   681          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 80 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0         553          1\n",
      "1     pilates          1\n",
      "2  instructor          1\n",
      "3     amateur          1\n",
      "4        chef          1\n",
      "There are 19 tokens in the data.\n",
      "There are 19 unique tokens in the data.\n",
      "There are 93 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  blackpool          1\n",
      "1         uk          1\n",
      "2        265          1\n",
      "3       1619          1\n",
      "4   breaking          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 24 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  accounts          1\n",
      "1  password          1\n",
      "2    anyall          1\n",
      "3        18          1\n",
      "There are 12 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 88 characters in the data.\n",
      "The lexical diversity is 0.917 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0  illustration          2\n",
      "1        expert          1\n",
      "2         stuff          1\n",
      "3       request          1\n",
      "4        please          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 46 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0       derby          1\n",
      "1       lgbtq          1\n",
      "2      issues          1\n",
      "3  disability          1\n",
      "4   sometimes          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 52 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0          18          1\n",
      "1      follow          1\n",
      "2  especially          1\n",
      "3       claim          1\n",
      "4        nsfw          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 57 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0        dj          1\n",
      "1  bookings          1\n",
      "2   contact          1\n",
      "3     ruben          1\n",
      "4  martinez          1\n",
      "There are 20 tokens in the data.\n",
      "There are 20 unique tokens in the data.\n",
      "There are 111 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0         128          1\n",
      "1        1840          1\n",
      "2      mental          1\n",
      "3      health          1\n",
      "4  campaigner          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 55 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "               token  frequency\n",
      "0               1084          1\n",
      "1                aka          1\n",
      "2             martin          1\n",
      "3             duncan          1\n",
      "4  multidisciplinary          1\n",
      "There are 13 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 64 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   voted          1\n",
      "1    tory          1\n",
      "2   often          1\n",
      "3  called          1\n",
      "4  feisty          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  life          1\n",
      "There are 18 tokens in the data.\n",
      "There are 18 unique tokens in the data.\n",
      "There are 106 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     east          1\n",
      "1  england          1\n",
      "2      251          1\n",
      "3      663          1\n",
      "4    woman          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 92 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      verb          1\n",
      "1    1going          1\n",
      "2      life          1\n",
      "3     takes          1\n",
      "4  pursuing          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 35 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0      eternal          1\n",
      "1      loveday          1\n",
      "2  onepostcard          1\n",
      "3      forever          1\n",
      "4          day          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 29 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0           4          1\n",
      "1        hard          1\n",
      "2     working          1\n",
      "3     forever          1\n",
      "4  optimistic          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 35 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0     world          1\n",
      "1    random          1\n",
      "2      crap          1\n",
      "3  internet          1\n",
      "4     kodak          1\n",
      "There are 20 tokens in the data.\n",
      "There are 20 unique tokens in the data.\n",
      "There are 113 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    wife          1\n",
      "1  mother          1\n",
      "2   hippy          1\n",
      "3  empath          1\n",
      "4   lefty          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 83 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      3168          1\n",
      "1   nothing          1\n",
      "2  exciting          1\n",
      "3      tell          1\n",
      "4      cant          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  wife          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 20 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    every          1\n",
      "1      day          1\n",
      "2  towards          1\n",
      "3    death          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 17 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   find          1\n",
      "1  happy          1\n",
      "2   life          1\n",
      "3   make          1\n",
      "There are 13 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 85 characters in the data.\n",
      "The lexical diversity is 0.923 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0           u          2\n",
      "1        love          1\n",
      "2       proud          1\n",
      "3  cleverness          1\n",
      "4         deb          1\n",
      "There are 13 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 70 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      699          1\n",
      "1       29          1\n",
      "2     year          1\n",
      "3      old          1\n",
      "4  suffers          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 22 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0      follow          1\n",
      "1  negativity          1\n",
      "2      thanks          1\n",
      "There are 19 tokens in the data.\n",
      "There are 18 unique tokens in the data.\n",
      "There are 105 characters in the data.\n",
      "The lexical diversity is 0.947 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0     health          2\n",
      "1    swindon          1\n",
      "2  liverpool          1\n",
      "3        fan          1\n",
      "4     family          1\n",
      "There are 8 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 58 characters in the data.\n",
      "The lexical diversity is 0.875 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0       producer          2\n",
      "1  bbceastenders          1\n",
      "2    bbccasualty          1\n",
      "3           soap          1\n",
      "4         addict          1\n",
      "There are 14 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 74 characters in the data.\n",
      "The lexical diversity is 0.929 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0        love          2\n",
      "1          hi          1\n",
      "2        name          1\n",
      "3        emma          1\n",
      "4  fashionwwe          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 14 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   𝒪𝓃𝑒          1\n",
      "1   𝒹𝒶𝓎          1\n",
      "2    𝒶𝓉          1\n",
      "3     𝒶          1\n",
      "4  𝓉𝒾𝓂𝑒          1\n",
      "There are 21 tokens in the data.\n",
      "There are 21 unique tokens in the data.\n",
      "There are 115 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    wales          1\n",
      "1      203          1\n",
      "2     1866          1\n",
      "3  married          1\n",
      "4     phil          1\n",
      "There are 16 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 89 characters in the data.\n",
      "The lexical diversity is 0.938 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0           tg          2\n",
      "1         real          1\n",
      "2  underground          1\n",
      "3         raps          1\n",
      "4          tgs          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 13 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0   beings          1\n",
      "1  leisure          1\n",
      "There are 6 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 43 characters in the data.\n",
      "The lexical diversity is 0.833 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0    candidate          2\n",
      "1           jd          1\n",
      "2  berkeleylaw          1\n",
      "3          phd          1\n",
      "4    bu_tweets          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   man          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 18 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  thinking          1\n",
      "1   sinking          1\n",
      "2       mud          1\n",
      "There are 13 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 101 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                         token  frequency\n",
      "0                     theythem          1\n",
      "1  creativeartistbardcomicpoet          1\n",
      "2                          lil          1\n",
      "3                         blob          1\n",
      "4                        space          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 12 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0     41          1\n",
      "1    149          1\n",
      "2     21          1\n",
      "3  hehim          1\n",
      "There are 13 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 88 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     697          1\n",
      "1    cats          1\n",
      "2  coffee          1\n",
      "3   music          1\n",
      "4  horror          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 51 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0      kdrama          1\n",
      "1       gamer          1\n",
      "2    catholic          1\n",
      "3  geographer          1\n",
      "4        role          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 21 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  loves          1\n",
      "1   arts          1\n",
      "2   love          1\n",
      "3  nikki          1\n",
      "4    xxx          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 52 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0      curator          1\n",
      "1         lost          1\n",
      "2        found          1\n",
      "3  photographs          1\n",
      "4        clark          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 31 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0   doncaster          1\n",
      "1      rovers          1\n",
      "2  manchester          1\n",
      "3      united          1\n",
      "There are 11 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 86 characters in the data.\n",
      "The lexical diversity is 0.909 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0    offended          2\n",
      "1   wifewoman          1\n",
      "2     winegin          1\n",
      "3  drinkercat          1\n",
      "4       momma          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 54 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       119          1\n",
      "1      1856          1\n",
      "2      lone          1\n",
      "3  wanderer          1\n",
      "4   looking          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 30 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0      daddy          1\n",
      "1  aesthetic          1\n",
      "2       good          1\n",
      "3        boy          1\n",
      "4      state          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 36 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "              token  frequency\n",
      "0         esdrúxula          1\n",
      "1        anacrônica          1\n",
      "2                 e          1\n",
      "3  contraproducente          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 91 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0          acp          1\n",
      "1  broadcaster          1\n",
      "2    freelance          1\n",
      "3      content          1\n",
      "4     producer          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 18 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0  httpstcovcg0vrspky          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 43 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       free          1\n",
      "1  palestine          1\n",
      "2       1110          1\n",
      "3        401          1\n",
      "4    bottoms          1\n",
      "There are 18 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 108 characters in the data.\n",
      "The lexical diversity is 0.833 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0         news          4\n",
      "1      science          1\n",
      "2      general          1\n",
      "3  interesting          1\n",
      "4     pictures          1\n",
      "There are 16 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 87 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0         la          1\n",
      "1     tierra          1\n",
      "2        del          1\n",
      "3  emperador          1\n",
      "4    supremo          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 51 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0          es          1\n",
      "1    señorita          1\n",
      "2  periodista          1\n",
      "3   peluquera          1\n",
      "4      humana          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 75 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  mainly          1\n",
      "1   gonna          1\n",
      "2    dumb          1\n",
      "3  tweets          1\n",
      "4    song          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 33 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     3439          1\n",
      "1      gay          1\n",
      "2    otaku          1\n",
      "3  kpopper          1\n",
      "4   amante          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 5 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0    29          1\n",
      "1   388          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 70 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0       zχ          1\n",
      "1  ѕρєтяυм          1\n",
      "2  νєтєяαη          1\n",
      "3      ρѕ1          1\n",
      "4        2          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   324          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 53 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   dont          1\n",
      "1   know          1\n",
      "2   fuck          1\n",
      "3     im          1\n",
      "4  error          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 42 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0    bandcamp          1\n",
      "1  soundcloud          1\n",
      "2       april          1\n",
      "3          22          1\n",
      "4   streaming          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 47 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0        san          1\n",
      "1  francisco          1\n",
      "2         21          1\n",
      "3     expect          1\n",
      "4    nothing          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 42 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   alum          1\n",
      "1     20          1\n",
      "2     ba          1\n",
      "3  music          1\n",
      "4  hehim          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 12 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    youre          1\n",
      "1  amazing          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 48 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0    artist          1\n",
      "1       bfa          1\n",
      "2  educator          1\n",
      "3       mat          1\n",
      "4    social          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 63 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     wars          1\n",
      "1  fanatic          1\n",
      "2     film          1\n",
      "3   horror          1\n",
      "4   scream          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 7 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   190          1\n",
      "1  1611          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 44 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0  geostationary          1\n",
      "1            job          1\n",
      "2         living          1\n",
      "3     affiliated          1\n",
      "4         sadwyw          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   never          1\n",
      "1  looked          1\n",
      "2    glum          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 47 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0  netherlands          1\n",
      "1          274          1\n",
      "2         3839          1\n",
      "3       hijhem          1\n",
      "4        hehim          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 19 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "               token  frequency\n",
      "0                 kg          1\n",
      "1  香港俄羅斯台灣混血在台北長大音樂課          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 28 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0          repurposed          1\n",
      "1  fashionaccessories          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 42 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    read          1\n",
      "1     one          1\n",
      "2  tweets          1\n",
      "3   think          1\n",
      "4      im          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 26 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0    pronouns          1\n",
      "1  statements          1\n",
      "2    opinions          1\n",
      "There are 14 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 67 characters in the data.\n",
      "The lexical diversity is 0.929 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0     18          2\n",
      "1  years          1\n",
      "2  older          1\n",
      "3   view          1\n",
      "4    yes          1\n",
      "There are 17 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 90 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  regular          1\n",
      "1  boricua          1\n",
      "2      guy          1\n",
      "3   living          1\n",
      "4  seattle          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 35 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0     please          1\n",
      "1    explain          1\n",
      "2     reason          1\n",
      "3    strange          1\n",
      "4  behaviour          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 11 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   extra          1\n",
      "1  pixels          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 58 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0       economic          1\n",
      "1        justice          1\n",
      "2            phd          1\n",
      "3  developmental          1\n",
      "4   neurobiology          1\n",
      "There are 16 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 106 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0       friend          1\n",
      "1      dorothy          1\n",
      "2       reader          1\n",
      "3  introverted          1\n",
      "4    extrovert          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 17 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       466          1\n",
      "1    actual          1\n",
      "2  thoughts          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 60 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     well          1\n",
      "1   theres          1\n",
      "2  another          1\n",
      "3    dance          1\n",
      "4    gotta          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 5 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  hehim          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 40 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0  genetically          1\n",
      "1     modified          1\n",
      "2     faggotry          1\n",
      "3    heshethey          1\n",
      "4         acab          1\n",
      "There are 23 tokens in the data.\n",
      "There are 23 unique tokens in the data.\n",
      "There are 130 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0    oneeyed          1\n",
      "1       king          1\n",
      "2  114283529          1\n",
      "3   dortmund          1\n",
      "4        via          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 26 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    judge          1\n",
      "1   people          1\n",
      "2      way          1\n",
      "3    treat          1\n",
      "4  animals          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 7 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  dumb          1\n",
      "1   gay          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 35 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0      42          1\n",
      "1     207          1\n",
      "2    wild          1\n",
      "3    punk          1\n",
      "4  raisin          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 31 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0   artist          1\n",
      "1      cat          1\n",
      "2    lover          1\n",
      "3  knitter          1\n",
      "4     nsfw          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 19 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    314          1\n",
      "1  weird          1\n",
      "2   live          1\n",
      "3   rare          1\n",
      "4    die          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 17 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     moon          1\n",
      "1   planet          1\n",
      "2  darling          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 32 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    2335          1\n",
      "1     632          1\n",
      "2     teu          1\n",
      "3    amor          1\n",
      "4  chamou          1\n",
      "There are 11 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 57 characters in the data.\n",
      "The lexical diversity is 0.909 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   lover          2\n",
      "1     cpa          1\n",
      "2    yawn          1\n",
      "3    know          1\n",
      "4  sports          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 2 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0    21          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 16 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  trisha          1\n",
      "1  paytas          1\n",
      "2    stan          1\n",
      "There are 14 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 90 characters in the data.\n",
      "The lexical diversity is 0.929 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  drink          2\n",
      "1   cant          1\n",
      "2   wear          1\n",
      "3    eat          1\n",
      "4   wont          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 39 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0               white          1\n",
      "1                 234          1\n",
      "2                 325          1\n",
      "3          management          1\n",
      "4  jessicamilestorres          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 33 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0          488          1\n",
      "1       comics          1\n",
      "2        queer          1\n",
      "3  cheetahgirl          1\n",
      "4     hehimhun          1\n",
      "There are 23 tokens in the data.\n",
      "There are 22 unique tokens in the data.\n",
      "There are 143 characters in the data.\n",
      "The lexical diversity is 0.957 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  nature          2\n",
      "1  karens          1\n",
      "2  corner          1\n",
      "3  uokhun          1\n",
      "4    2218          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 24 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      new          1\n",
      "1  account          1\n",
      "2     dumb          1\n",
      "3     btch          1\n",
      "4   flavor          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 27 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       266          1\n",
      "1    deeply          1\n",
      "2  sleeping          1\n",
      "3     still          1\n",
      "4     awake          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 50 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                     token  frequency\n",
      "0                      201          1\n",
      "1  singersongwritertheatre          1\n",
      "2                   artist          1\n",
      "3       httpstco1xjjknbpxt          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 5 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  yikes          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 30 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0         pop          1\n",
      "1        nach          1\n",
      "2           8          1\n",
      "3         der          1\n",
      "4  poppodcast          1\n",
      "There are 16 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 88 characters in the data.\n",
      "The lexical diversity is 0.812 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0         im          3\n",
      "1  locksmith          2\n",
      "2      andim          1\n",
      "3    mixtape          1\n",
      "4      angel          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 29 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  baixinho          1\n",
      "1   barbudo          1\n",
      "2  roqueiro          1\n",
      "3         e          1\n",
      "4     besta          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 41 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "            token  frequency\n",
      "0          stockx          1\n",
      "1              ex          1\n",
      "2  brunswickgroup          1\n",
      "3            akqa          1\n",
      "4         trustee          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 22 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0    𝘈𝘯𝘵𝘶𝘬𝘪𝘯          1\n",
      "1        247          1\n",
      "2        𝙉𝙤𝙩          1\n",
      "3  𝙖𝙫𝙖𝙞𝙡𝙖𝙗𝙡𝙚          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 66 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  celine          1\n",
      "1    brit          1\n",
      "2  little          1\n",
      "3  polish          1\n",
      "4    came          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 42 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0    nacido          1\n",
      "1        un          1\n",
      "2    jueves          1\n",
      "3  nintendo          1\n",
      "4    switch          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 74 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  managing          1\n",
      "1  director          1\n",
      "2    forbes          1\n",
      "3    europe          1\n",
      "4  lecturer          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 14 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  capricorn          1\n",
      "1      hehim          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 52 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    taied          1\n",
      "1      839          1\n",
      "2     2577          1\n",
      "3    nasty          1\n",
      "4  british          1\n",
      "There are 15 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 75 characters in the data.\n",
      "The lexical diversity is 0.933 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0    nursing          2\n",
      "1  assistant          1\n",
      "2        nhs          1\n",
      "3   ayrshire          1\n",
      "4      arran          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 23 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       new          1\n",
      "1    vtuber          1\n",
      "2  streamer          1\n",
      "3    twitch          1\n",
      "There are 13 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 96 characters in the data.\n",
      "The lexical diversity is 0.846 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0          world          2\n",
      "1         awards          2\n",
      "2          music          1\n",
      "3  international          1\n",
      "4       ceremony          1\n",
      "There are 11 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 67 characters in the data.\n",
      "The lexical diversity is 0.909 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  respect          2\n",
      "1   people          1\n",
      "2      law          1\n",
      "3    order          1\n",
      "4    teach          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 97 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  proud          1\n",
      "1  mummy          1\n",
      "2      2          1\n",
      "3   cubs          1\n",
      "4   exec          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 77 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  disney          1\n",
      "1    nerd          1\n",
      "2   crazy          1\n",
      "3     cat          1\n",
      "4    lady          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 21 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       lane          1\n",
      "1   producer          1\n",
      "2  omroepntr          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 38 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   m365          1\n",
      "1    uem          1\n",
      "2   prem          1\n",
      "3  stuff          1\n",
      "4   also          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 65 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "              token  frequency\n",
      "0            comics          1\n",
      "1          cartoons          1\n",
      "2            horror          1\n",
      "3      vectorartist          1\n",
      "4  dctheillustrator          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 9 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     est          1\n",
      "1  facere          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 12 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  naughty          1\n",
      "1    nurse          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 18 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0        61          1\n",
      "1       189          1\n",
      "2     party          1\n",
      "3  wherever          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 14 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    939          1\n",
      "1  watch          1\n",
      "2   life          1\n",
      "3     go          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 22 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     376          1\n",
      "1    open          1\n",
      "2  wallet          1\n",
      "3    full          1\n",
      "4   blood          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 7 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  element          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 38 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   983          1\n",
      "1   496          1\n",
      "2  ekki          1\n",
      "3  eins          1\n",
      "4    og          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 7 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  faleceu          1\n",
      "There are 15 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 80 characters in the data.\n",
      "The lexical diversity is 0.933 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     drag          2\n",
      "1     1151          1\n",
      "2     1523          1\n",
      "3  theyshe          1\n",
      "4      stl          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 31 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0         58          1\n",
      "1        hes          1\n",
      "2    cartoon          1\n",
      "3  character          1\n",
      "4     little          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 14 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   moon          1\n",
      "1  coast          1\n",
      "2  clear          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 22 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0                1302          1\n",
      "1  backbackforthforth          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 38 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  timothee          1\n",
      "1  chalamet          1\n",
      "2      dont          1\n",
      "3   impress          1\n",
      "4      much          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0   spotify          1\n",
      "1  pronouns          1\n",
      "There are 12 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 65 characters in the data.\n",
      "The lexical diversity is 0.833 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      fear          2\n",
      "1      life          2\n",
      "2    effect          1\n",
      "3  instills          1\n",
      "4    rather          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 9 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0       73          1\n",
      "1  superit          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 6 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  gemini          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 17 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  basenji          1\n",
      "1      pup          1\n",
      "2      two          1\n",
      "3     dads          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 46 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0        girl          1\n",
      "1  sheherhers          1\n",
      "2        vote          1\n",
      "3         cat          1\n",
      "4         win          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 31 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0        28          1\n",
      "1   husband          1\n",
      "2       cat          1\n",
      "3       dad          1\n",
      "4  interior          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 26 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  questions          1\n",
      "1   comments          1\n",
      "2  criticism          1\n",
      "There are 16 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 94 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0       12          1\n",
      "1   street          1\n",
      "2  rescued          1\n",
      "3     cats          1\n",
      "4  cooking          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 81 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0        114          1\n",
      "1        fix          1\n",
      "2  computers          1\n",
      "3      money          1\n",
      "4    collect          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 49 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "             token  frequency\n",
      "0             book          1\n",
      "1            comes          1\n",
      "2             2022          1\n",
      "3              via          1\n",
      "4  archwayeditions          1\n",
      "There are 8 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 34 characters in the data.\n",
      "The lexical diversity is 0.875 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0   follow          2\n",
      "1      req          1\n",
      "2     dont          1\n",
      "3        u          1\n",
      "4  already          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 8 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    457          1\n",
      "1  hehim          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 88 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0  entertainment          1\n",
      "1           news          1\n",
      "2    archaeology          1\n",
      "3         degree          1\n",
      "4        jeanluc          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  1140          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 14 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   ser          1\n",
      "1   man          1\n",
      "2  bara          1\n",
      "3  skog          1\n",
      "There are 16 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 85 characters in the data.\n",
      "The lexical diversity is 0.875 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  oxford          2\n",
      "1      uk          2\n",
      "2      30          1\n",
      "3     225          1\n",
      "4   music          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 43 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0          songwriter          1\n",
      "1                book          1\n",
      "2               fiend          1\n",
      "3              sheher          1\n",
      "4  httpstco7v4ibjgs7g          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   raq          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 19 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0        life          1\n",
      "1  veterinary          1\n",
      "2       nurse          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 36 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0   waiting          1\n",
      "1     storm          1\n",
      "2   passits          1\n",
      "3  learning          1\n",
      "4     dance          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 98 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0          kind          1\n",
      "1          make          1\n",
      "2        videos          1\n",
      "3  kylieminogue          1\n",
      "4   photography          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 25 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0         53          1\n",
      "1        may          1\n",
      "2      large          1\n",
      "3  container          1\n",
      "4     coffee          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 83 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   gamer          1\n",
      "1  singer          1\n",
      "2  tweets          1\n",
      "3  random          1\n",
      "4  vulgar          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 80 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0       golden          1\n",
      "1         blue          1\n",
      "2  toofollowed          1\n",
      "3            b          1\n",
      "4  twitterhere          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 11 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0  asfaltsbarn          1\n",
      "There are 16 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 58 characters in the data.\n",
      "The lexical diversity is 0.938 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   för          2\n",
      "1    är          1\n",
      "2  kort          1\n",
      "3   att          1\n",
      "4  bara          1\n",
      "There are 18 tokens in the data.\n",
      "There are 18 unique tokens in the data.\n",
      "There are 113 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  studying          1\n",
      "1  learning          1\n",
      "2    create          1\n",
      "3     black          1\n",
      "4     holes          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 38 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0  friluftsliv          1\n",
      "1        hajka          1\n",
      "2         jakt          1\n",
      "3          och          1\n",
      "4      allmänt          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 59 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0  kthuniversity          1\n",
      "1     scilifelab          1\n",
      "2   illustration          1\n",
      "3        scicomm          1\n",
      "4       genomics          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 76 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0     lawyer          1\n",
      "1  distrusts          1\n",
      "2       laws          1\n",
      "3      legal          1\n",
      "4    systems          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 65 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0     travel          1\n",
      "1      happy          1\n",
      "2  norwegian          1\n",
      "3      music          1\n",
      "4    science          1\n",
      "There are 13 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 70 characters in the data.\n",
      "The lexical diversity is 0.923 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0       ur          2\n",
      "1       yr          1\n",
      "2  trading          1\n",
      "3  vetwill          1\n",
      "4   follow          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 8 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    att          1\n",
      "1  cykla          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 20 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     katy          1\n",
      "1    perry          1\n",
      "2     stan          1\n",
      "3  account          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 51 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     feel          1\n",
      "1     like          1\n",
      "2   gloria          1\n",
      "3  fucking          1\n",
      "4  swanson          1\n",
      "There are 6 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 27 characters in the data.\n",
      "The lexical diversity is 0.833 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0       im          2\n",
      "1     2186          1\n",
      "2    hello          1\n",
      "3  patrick          1\n",
      "4  florida          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 55 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0      debut          1\n",
      "1         ep          1\n",
      "2    liminal          1\n",
      "3      space          1\n",
      "4  streaming          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 22 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   897          1\n",
      "1  1099          1\n",
      "2  call          1\n",
      "3  want          1\n",
      "4   say          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 30 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0   twitter          1\n",
      "1  accounts          1\n",
      "2  pressure          1\n",
      "3       add          1\n",
      "4      back          1\n",
      "There are 2 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 20 characters in the data.\n",
      "The lexical diversity is 0.500 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  disappoint          2\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 26 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  artist          1\n",
      "1    zonk          1\n",
      "2   daddy          1\n",
      "3  always          1\n",
      "4   tired          1\n",
      "There are 13 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 70 characters in the data.\n",
      "The lexical diversity is 0.923 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  beach          2\n",
      "1  tryin          1\n",
      "2   make          1\n",
      "3     la          1\n",
      "4   rock          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 29 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0          de          1\n",
      "1      acordo          1\n",
      "2         com          1\n",
      "3    contexto          1\n",
      "4  geográfico          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 13 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  linktree          1\n",
      "1     right          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 5 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  order          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 25 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   ich          1\n",
      "1   bin          1\n",
      "2   der          1\n",
      "3  eine          1\n",
      "4   von          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 84 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0          och          1\n",
      "1        metal          1\n",
      "2         mitt          1\n",
      "3       hjärta          1\n",
      "4  förmodligen          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 26 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0     whole          1\n",
      "1     lotta          1\n",
      "2      sass          1\n",
      "3      side          1\n",
      "4  sunshine          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 56 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0     software          1\n",
      "1  engineering          1\n",
      "2      student          1\n",
      "3       follow          1\n",
      "4         open          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 14 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   like          1\n",
      "1  music          1\n",
      "2  books          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 17 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0       matter          1\n",
      "1  punctuation          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 66 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0           251          1\n",
      "1           two          1\n",
      "2         hours          1\n",
      "3          best          1\n",
      "4  underexposed          1\n",
      "There are 16 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 102 characters in the data.\n",
      "The lexical diversity is 0.938 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0         och          2\n",
      "1     statlig          1\n",
      "2  tjänsteman          1\n",
      "3      gillar          1\n",
      "4       fakta          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 9 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   real          1\n",
      "1  enemy          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 33 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    4382          1\n",
      "1    1899          1\n",
      "2  charli          1\n",
      "3     xcx          1\n",
      "4  others          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 85 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                          token  frequency\n",
      "0                           991          1\n",
      "1                          sono          1\n",
      "2      simpaticoattivosocievole          1\n",
      "3  generosopassionaleestroverso          1\n",
      "4             intelligenteadoro          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 31 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0        1247          1\n",
      "1     married          1\n",
      "2  antomatmos          1\n",
      "3        kind          1\n",
      "4      ikeanu          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 35 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  welcome          1\n",
      "1   boring          1\n",
      "2     page          1\n",
      "3  twitter          1\n",
      "4   hethem          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 59 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0  construction          1\n",
      "1     machinery          1\n",
      "2           est          1\n",
      "3          1946          1\n",
      "4         hymac          1\n",
      "There are 13 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 82 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0        3238          1\n",
      "1  oneanddone          1\n",
      "2       champ          1\n",
      "3       known          1\n",
      "4  irritating          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 77 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     pop          1\n",
      "1  writer          1\n",
      "2    bops          1\n",
      "3  sheher          1\n",
      "4      ig          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    fear          1\n",
      "1  anyway          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 50 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    love          1\n",
      "1    make          1\n",
      "2   funny          1\n",
      "3   video          1\n",
      "4  shared          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 76 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0   designer          1\n",
      "1    creator          1\n",
      "2  education          1\n",
      "3   20042011          1\n",
      "4    tbilisi          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 43 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     fan          1\n",
      "1  wantin          1\n",
      "2    keep          1\n",
      "3    date          1\n",
      "4   stuff          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 12 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      meat          1\n",
      "1  popsicle          1\n",
      "There are 13 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 90 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0          2006          1\n",
      "1         music          1\n",
      "2        easily          1\n",
      "3    distracted          1\n",
      "4  particularly          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 63 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0    lonely          1\n",
      "1  daffodil          1\n",
      "2   promise          1\n",
      "3       ill          1\n",
      "4      sing          1\n",
      "There are 13 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 53 characters in the data.\n",
      "The lexical diversity is 0.923 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  york          2\n",
      "1   new          1\n",
      "2  9720          1\n",
      "3   587          1\n",
      "4  nycs          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 19 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0   starbucks          1\n",
      "1  enthusiast          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 57 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0  chroniqueuse          1\n",
      "1          arts          1\n",
      "2       culture          1\n",
      "3         music          1\n",
      "4          life          1\n",
      "There are 13 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 127 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0          10          1\n",
      "1         631          1\n",
      "2  thebeatles          1\n",
      "3  osmutantes          1\n",
      "4    bobdylan          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  woke          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  manc          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  enthusiast          1\n",
      "There are 15 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 48 characters in the data.\n",
      "The lexical diversity is 0.867 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0    من          2\n",
      "1    گم          2\n",
      "2  دارم          1\n",
      "3  کمتر          1\n",
      "4    از          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 51 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0                love          1\n",
      "1                  ya          1\n",
      "2           happiness          1\n",
      "3  httpstcobdh8r01t5u          1\n",
      "4  httpstcoo1candatti          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 100 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  guardian          1\n",
      "1     audio          1\n",
      "2  formerly          1\n",
      "3      exec          1\n",
      "4  producer          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 16 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  precise          1\n",
      "1    speak          1\n",
      "2     well          1\n",
      "There are 23 tokens in the data.\n",
      "There are 23 unique tokens in the data.\n",
      "There are 114 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  vocalist          1\n",
      "1    tongue          1\n",
      "2    cancer          1\n",
      "3  survivor          1\n",
      "4    martin          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 56 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    cusp          1\n",
      "1   twunk          1\n",
      "2    make          1\n",
      "3   music          1\n",
      "4  stream          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 53 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0         west          1\n",
      "1         cork          1\n",
      "2    librarian          1\n",
      "3  illustrator          1\n",
      "4       social          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 35 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0  limecoconut          1\n",
      "1    veganbomb          1\n",
      "2     cupcakes          1\n",
      "3          dog          1\n",
      "4         work          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 8 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  salsicha          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 87 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "            token  frequency\n",
      "0            acct          1\n",
      "1  mrsshannon2015          1\n",
      "2           proud          1\n",
      "3            mema          1\n",
      "4               6          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 71 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    pop          1\n",
      "1     rb          1\n",
      "2  onika          1\n",
      "3  maraj          1\n",
      "4   doja          1\n",
      "There are 5 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 34 characters in the data.\n",
      "The lexical diversity is 0.600 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  geronimo          3\n",
      "1       god          1\n",
      "2   country          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 30 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0         work          1\n",
      "1  micronautfw          1\n",
      "2   oraclelabs          1\n",
      "3        views          1\n",
      "There are 19 tokens in the data.\n",
      "There are 18 unique tokens in the data.\n",
      "There are 141 characters in the data.\n",
      "The lexical diversity is 0.947 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0    morocco          2\n",
      "1      rabat          1\n",
      "2       1047          1\n",
      "3       1819          1\n",
      "4  assistant          1\n",
      "There are 17 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 113 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0          loves          1\n",
      "1  stereophonics          1\n",
      "2         kaiser          1\n",
      "3         chiefs          1\n",
      "4            two          1\n",
      "There are 9 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 49 characters in the data.\n",
      "The lexical diversity is 0.778 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    never          2\n",
      "1     knew          2\n",
      "2   facing          1\n",
      "3    fears          1\n",
      "4  existed          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 70 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0         tv          1\n",
      "1      shows          1\n",
      "2  political          1\n",
      "3   nonsense          1\n",
      "4        etc          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 13 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  enthusiast          1\n",
      "1         itu          1\n",
      "There are 18 tokens in the data.\n",
      "There are 18 unique tokens in the data.\n",
      "There are 110 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0          free          1\n",
      "1     neighbors          1\n",
      "2          keep          1\n",
      "3  neighborhood          1\n",
      "4          pets          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 13 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  artist          1\n",
      "1   since          1\n",
      "2      88          1\n",
      "There are 13 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 99 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0  dullesairport          1\n",
      "1  reaganairport          1\n",
      "2            bwi          1\n",
      "3         museum          1\n",
      "4      landmarks          1\n",
      "There are 9 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 75 characters in the data.\n",
      "The lexical diversity is 0.889 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0     newsletter          2\n",
      "1       designer          1\n",
      "2          books          1\n",
      "3        posters          1\n",
      "4  miscellaneous          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  sola          1\n",
      "There are 14 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 54 characters in the data.\n",
      "The lexical diversity is 0.929 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0     un          2\n",
      "1   algo          1\n",
      "2    que          1\n",
      "3  marca          1\n",
      "4  antes          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 30 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                token  frequency\n",
      "0                 fan          1\n",
      "1           playlists          1\n",
      "2  httpstcoeskkipwo3v          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 71 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0     r          1\n",
      "1     w          1\n",
      "2     e          1\n",
      "3     l          1\n",
      "4     c          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 41 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0        yes          1\n",
      "1       live          1\n",
      "2  dangerous          1\n",
      "3       life          1\n",
      "4     sodium          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 22 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     calm          1\n",
      "1       im          1\n",
      "2     also          1\n",
      "3    angry          1\n",
      "4  duality          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 65 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0           para          1\n",
      "1      obnubilar          1\n",
      "2             en          1\n",
      "3             el          1\n",
      "4  espaciotiempo          1\n",
      "There are 19 tokens in the data.\n",
      "There are 18 unique tokens in the data.\n",
      "There are 120 characters in the data.\n",
      "The lexical diversity is 0.947 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0       radio          2\n",
      "1         470          1\n",
      "2        bcit          1\n",
      "3     student          1\n",
      "4  passionate          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  live          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 86 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       live          1\n",
      "1  performer          1\n",
      "2   producer          1\n",
      "3   director          1\n",
      "4    drummer          1\n",
      "There are 13 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 93 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0        vp          1\n",
      "1       abb          1\n",
      "2  robotics          1\n",
      "3   denmark          1\n",
      "4   mission          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 10 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  super          1\n",
      "1  loyal          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 90 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "              token  frequency\n",
      "0             child          1\n",
      "1              king          1\n",
      "2          aquarius          1\n",
      "3  singersongwriter          1\n",
      "4             lover          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 14 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  travelled          1\n",
      "1      world          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 15 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  landmand          1\n",
      "1   madmand          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 31 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "            token  frequency\n",
      "0        forælder          1\n",
      "1  pythonhobbyist          1\n",
      "2       fodgænger          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 62 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                            token  frequency\n",
      "0                           heart          1\n",
      "1                         english          1\n",
      "2                          tongue          1\n",
      "3  singersongwriterscruffylooking          1\n",
      "4                            sing          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 56 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                                  token  frequency\n",
      "0                                    52          1\n",
      "1                                   610          1\n",
      "2  designartshumourdraghollywoodfoodiei          1\n",
      "3                                  like          1\n",
      "4                                  mens          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 35 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0    husband          1\n",
      "1  protector          1\n",
      "2  unboxable          1\n",
      "3      likes          1\n",
      "4      hehim          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 7 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   547          1\n",
      "1  4661          1\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 42 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0         tudo          1\n",
      "1        posso          1\n",
      "2           na          1\n",
      "3  venlafaxina          1\n",
      "4          que          1\n",
      "There are 6 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 32 characters in the data.\n",
      "The lexical diversity is 0.833 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   lover          2\n",
      "1    1380          1\n",
      "2  coffee          1\n",
      "3  animal          1\n",
      "4  friend          1\n",
      "There are 5 tokens in the data.\n",
      "There are 5 unique tokens in the data.\n",
      "There are 24 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  capture          1\n",
      "1      put          1\n",
      "2      bad          1\n",
      "3    light          1\n",
      "4   sheher          1\n",
      "There are 17 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 85 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0  journalistin          1\n",
      "1          gern          1\n",
      "2           mit          1\n",
      "3      bewegten          1\n",
      "4       bildern          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 54 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0     humankind          1\n",
      "1  slaughtering          1\n",
      "2     innocence          1\n",
      "3   indignantly          1\n",
      "4          full          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 32 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "                  token  frequency\n",
      "0  blueshirttheoriginal          1\n",
      "1              original          1\n",
      "2                  even          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 67 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0      watch          1\n",
      "1  wrestling          1\n",
      "2       like          1\n",
      "3    detroit          1\n",
      "4     sports          1\n",
      "There are 4 tokens in the data.\n",
      "There are 4 unique tokens in the data.\n",
      "There are 16 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   1122          1\n",
      "1  olsen          1\n",
      "2  proxy          1\n",
      "3     sf          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 66 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0        quote          1\n",
      "1       suffer          1\n",
      "2        often          1\n",
      "3  imagination          1\n",
      "4      reality          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 16 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  asshole          1\n",
      "1    since          1\n",
      "2     1982          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 33 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     city          1\n",
      "1      new          1\n",
      "2  zealand          1\n",
      "3     1925          1\n",
      "4     2602          1\n",
      "There are 23 tokens in the data.\n",
      "There are 21 unique tokens in the data.\n",
      "There are 108 characters in the data.\n",
      "The lexical diversity is 0.913 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     ewe          2\n",
      "1     fuk          2\n",
      "2  around          1\n",
      "3     bye          1\n",
      "4     den          1\n",
      "There are 14 tokens in the data.\n",
      "There are 14 unique tokens in the data.\n",
      "There are 96 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0     music          1\n",
      "1     lover          1\n",
      "2    coffee          1\n",
      "3   drinker          1\n",
      "4  broadway          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 11 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  poverty          1\n",
      "1     fred          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 41 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0          737          1\n",
      "1        music          1\n",
      "2      singing          1\n",
      "3         life          1\n",
      "4  09186695881          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 56 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0        kind          1\n",
      "1       sweet          1\n",
      "2      unless          1\n",
      "3        mean          1\n",
      "4  vindictive          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 14 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  living          1\n",
      "1    best          1\n",
      "2    life          1\n",
      "There are 3 tokens in the data.\n",
      "There are 3 unique tokens in the data.\n",
      "There are 16 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  ساختمان          1\n",
      "1  راهسازی          1\n",
      "2       پل          1\n",
      "There are 20 tokens in the data.\n",
      "There are 18 unique tokens in the data.\n",
      "There are 94 characters in the data.\n",
      "The lexical diversity is 0.900 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  teacher          2\n",
      "1      usw          2\n",
      "2     1441          1\n",
      "3      mrs          1\n",
      "4        f          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 22 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    one          1\n",
      "1  knows          1\n",
      "2    abt          1\n",
      "3   lets          1\n",
      "4   keep          1\n",
      "There are 15 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 102 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0        thing          1\n",
      "1       mainly          1\n",
      "2  shitposting          1\n",
      "3        kinda          1\n",
      "4      earnest          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 9 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  expressed          1\n",
      "There are 12 tokens in the data.\n",
      "There are 12 unique tokens in the data.\n",
      "There are 87 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      meme          1\n",
      "1       ass          1\n",
      "2  musician          1\n",
      "3      mama          1\n",
      "4      seen          1\n",
      "There are 16 tokens in the data.\n",
      "There are 16 unique tokens in the data.\n",
      "There are 99 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0        latina          1\n",
      "1  representing          1\n",
      "2       bayarea          1\n",
      "3            ba          1\n",
      "4         music          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 71 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "              token  frequency\n",
      "0                18          1\n",
      "1  blacklivesmatter          1\n",
      "2  translivesmatter          1\n",
      "3               pan          1\n",
      "4              poly          1\n",
      "There are 7 tokens in the data.\n",
      "There are 7 unique tokens in the data.\n",
      "There are 49 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0     throuple          1\n",
      "1  complicated          1\n",
      "2        lover          1\n",
      "3        music          1\n",
      "4       portly          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 21 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0  dreamersmusik          1\n",
      "1       getdecor          1\n",
      "There are 6 tokens in the data.\n",
      "There are 6 unique tokens in the data.\n",
      "There are 37 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0        hehim          1\n",
      "1         nsfw          1\n",
      "2  personality          1\n",
      "3          mix          1\n",
      "4      crochet          1\n",
      "There are 17 tokens in the data.\n",
      "There are 17 unique tokens in the data.\n",
      "There are 86 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  chiquito          1\n",
      "1       por          1\n",
      "2     dónde          1\n",
      "3      veas          1\n",
      "4       162          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 45 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     cant          1\n",
      "1  believe          1\n",
      "2      ive          1\n",
      "3  account          1\n",
      "4       10          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 13 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  believe          1\n",
      "1   demons          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 3 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   109          1\n",
      "There are 9 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 81 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "            token  frequency\n",
      "0          rights          1\n",
      "1        advocate          1\n",
      "2       currently          1\n",
      "3  omidyarnetwork          1\n",
      "4        formerly          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 5 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  aries          1\n",
      "There are 1 tokens in the data.\n",
      "There are 1 unique tokens in the data.\n",
      "There are 4 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  nosy          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 65 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  activist          1\n",
      "1      love          1\n",
      "2   chelsea          1\n",
      "3   diehard          1\n",
      "4      live          1\n",
      "There are 10 tokens in the data.\n",
      "There are 10 unique tokens in the data.\n",
      "There are 76 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0    ecosystem          1\n",
      "1       growth          1\n",
      "2  livepeerorg          1\n",
      "3       hacker          1\n",
      "4    ethglobal          1\n",
      "There are 11 tokens in the data.\n",
      "There are 11 unique tokens in the data.\n",
      "There are 53 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0   pretty          1\n",
      "1     sure          1\n",
      "2       ai          1\n",
      "3  winning          1\n",
      "4      war          1\n",
      "There are 21 tokens in the data.\n",
      "There are 18 unique tokens in the data.\n",
      "There are 128 characters in the data.\n",
      "The lexical diversity is 0.857 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  sound          3\n",
      "1  proud          2\n",
      "2   beat          1\n",
      "3    267          1\n",
      "4    193          1\n",
      "There are 13 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 68 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    3247          1\n",
      "1   proud          1\n",
      "2     dad          1\n",
      "3   cmsgt          1\n",
      "4  usafmi          1\n",
      "There are 2 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 18 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0  housecleaning          1\n",
      "1          music          1\n"
     ]
    }
   ],
   "source": [
    "# calls to descriptive_stats here\n",
    "# Perform descriptive stats on Twitter data\n",
    "cher_twt_desc_stats = [descriptive_stats(item) for item in cher_twt_clean]\n",
    "robyn_twt_desc_stats = [descriptive_stats(item) for item in robyn_twt_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea0c1b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store full descriptive stats to csv\n",
    "cher_dict = {'statistics': cher_twt_desc_stats}\n",
    "robyn_dict = {'statistics': robyn_twt_desc_stats}\n",
    "cher_df = pd.DataFrame(cher_dict)\n",
    "robyn_df = pd.DataFrame(robyn_dict)\n",
    "cher_df.to_csv('data/twitter/cher_statistics')\n",
    "robyn_df.to_csv('data/twitter/robyn_statistics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "375771b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 180 tokens in the data.\n",
      "There are 82 unique tokens in the data.\n",
      "There are 822 characters in the data.\n",
      "The lexical diversity is 0.456 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  cause          9\n",
      "1    hot          8\n",
      "2     im          8\n",
      "3   yeah          8\n",
      "4     88          5\n",
      "There are 133 tokens in the data.\n",
      "There are 41 unique tokens in the data.\n",
      "There are 670 characters in the data.\n",
      "The lexical diversity is 0.308 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       kind         16\n",
      "1  different         15\n",
      "2       love         15\n",
      "3       song         15\n",
      "4        ooh         14\n",
      "There are 120 tokens in the data.\n",
      "There are 59 unique tokens in the data.\n",
      "There are 603 characters in the data.\n",
      "The lexical diversity is 0.492 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    two          8\n",
      "1   weve          6\n",
      "2   back          6\n",
      "3  guess          5\n",
      "4  stops          4\n",
      "There are 34 tokens in the data.\n",
      "There are 28 unique tokens in the data.\n",
      "There are 143 characters in the data.\n",
      "The lexical diversity is 0.824 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   dont          3\n",
      "1   door          2\n",
      "2   know          2\n",
      "3  never          2\n",
      "4    see          2\n",
      "There are 66 tokens in the data.\n",
      "There are 46 unique tokens in the data.\n",
      "There are 334 characters in the data.\n",
      "The lexical diversity is 0.697 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    alfie         10\n",
      "1     love          4\n",
      "2  believe          3\n",
      "3    whats          2\n",
      "4    meant          2\n",
      "There are 103 tokens in the data.\n",
      "There are 72 unique tokens in the data.\n",
      "There are 486 characters in the data.\n",
      "The lexical diversity is 0.699 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   need          7\n",
      "1  wanna          6\n",
      "2   like          5\n",
      "3   rain          3\n",
      "4  leave          3\n",
      "There are 94 tokens in the data.\n",
      "There are 54 unique tokens in the data.\n",
      "There are 461 characters in the data.\n",
      "The lexical diversity is 0.574 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  heart          6\n",
      "1   feel          6\n",
      "2   cant          5\n",
      "3    way          5\n",
      "4  wants          5\n",
      "There are 81 tokens in the data.\n",
      "There are 48 unique tokens in the data.\n",
      "There are 452 characters in the data.\n",
      "The lexical diversity is 0.593 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     baby         10\n",
      "1  friends         10\n",
      "2     want          7\n",
      "3   really          5\n",
      "4     aint          3\n",
      "There are 133 tokens in the data.\n",
      "There are 43 unique tokens in the data.\n",
      "There are 656 characters in the data.\n",
      "The lexical diversity is 0.323 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    youre         11\n",
      "1     baby         10\n",
      "2  nothing         10\n",
      "3     dont          8\n",
      "4    wanna          8\n",
      "There are 67 tokens in the data.\n",
      "There are 29 unique tokens in the data.\n",
      "There are 237 characters in the data.\n",
      "The lexical diversity is 0.433 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  blue          8\n",
      "1    im          8\n",
      "2   one          6\n",
      "3   hes          5\n",
      "4  time          4\n",
      "There are 99 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 477 characters in the data.\n",
      "The lexical diversity is 0.576 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  thing          8\n",
      "1   know          5\n",
      "2    bad          5\n",
      "3    let          4\n",
      "4     go          4\n",
      "There are 132 tokens in the data.\n",
      "There are 74 unique tokens in the data.\n",
      "There are 621 characters in the data.\n",
      "The lexical diversity is 0.561 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  apples          6\n",
      "1    dont          6\n",
      "2    fall          6\n",
      "3     far          6\n",
      "4    tree          6\n",
      "There are 108 tokens in the data.\n",
      "There are 54 unique tokens in the data.\n",
      "There are 479 characters in the data.\n",
      "The lexical diversity is 0.500 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   love         12\n",
      "1   cant          7\n",
      "2    one          5\n",
      "3    way          4\n",
      "4  spend          4\n",
      "There are 70 tokens in the data.\n",
      "There are 33 unique tokens in the data.\n",
      "There are 372 characters in the data.\n",
      "The lexical diversity is 0.471 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  without         10\n",
      "1    world          7\n",
      "2   heroes          6\n",
      "3     dont          6\n",
      "4     know          6\n",
      "There are 119 tokens in the data.\n",
      "There are 91 unique tokens in the data.\n",
      "There are 648 characters in the data.\n",
      "The lexical diversity is 0.765 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  young          6\n",
      "1   girl          5\n",
      "2   left          4\n",
      "3   love          4\n",
      "4   made          2\n",
      "There are 108 tokens in the data.\n",
      "There are 55 unique tokens in the data.\n",
      "There are 488 characters in the data.\n",
      "The lexical diversity is 0.509 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0      im         12\n",
      "1    back         12\n",
      "2  street          6\n",
      "3    feet          6\n",
      "4     get          3\n",
      "There are 173 tokens in the data.\n",
      "There are 70 unique tokens in the data.\n",
      "There are 744 characters in the data.\n",
      "The lexical diversity is 0.405 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    bang         62\n",
      "1    shot         10\n",
      "2    baby          9\n",
      "3  ground          6\n",
      "4     hit          4\n",
      "There are 96 tokens in the data.\n",
      "There are 53 unique tokens in the data.\n",
      "There are 429 characters in the data.\n",
      "The lexical diversity is 0.552 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    bang         24\n",
      "1    shot          5\n",
      "2     hit          3\n",
      "3  ground          3\n",
      "4   awful          3\n",
      "There are 91 tokens in the data.\n",
      "There are 54 unique tokens in the data.\n",
      "There are 478 characters in the data.\n",
      "The lexical diversity is 0.593 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   every          9\n",
      "1  asking          4\n",
      "2  behind          3\n",
      "3    door          3\n",
      "4   house          3\n",
      "There are 176 tokens in the data.\n",
      "There are 52 unique tokens in the data.\n",
      "There are 878 characters in the data.\n",
      "The lexical diversity is 0.295 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     dont         12\n",
      "1     love         11\n",
      "2    youre         10\n",
      "3   strong         10\n",
      "4  believe          9\n",
      "There are 153 tokens in the data.\n",
      "There are 56 unique tokens in the data.\n",
      "There are 650 characters in the data.\n",
      "The lexical diversity is 0.366 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   want         16\n",
      "1   dont         12\n",
      "2   fade          9\n",
      "3   away          9\n",
      "4  wanna          6\n",
      "There are 94 tokens in the data.\n",
      "There are 47 unique tokens in the data.\n",
      "There are 439 characters in the data.\n",
      "The lexical diversity is 0.500 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    many         10\n",
      "1     yes          7\n",
      "2  answer          6\n",
      "3  blowin          6\n",
      "4    wind          6\n",
      "There are 109 tokens in the data.\n",
      "There are 44 unique tokens in the data.\n",
      "There are 508 characters in the data.\n",
      "The lexical diversity is 0.404 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     body         24\n",
      "1    heart         17\n",
      "2       im          4\n",
      "3     eyes          3\n",
      "4  feeling          3\n",
      "There are 82 tokens in the data.\n",
      "There are 55 unique tokens in the data.\n",
      "There are 429 characters in the data.\n",
      "The lexical diversity is 0.671 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  hunger         10\n",
      "1    born          6\n",
      "2   never          5\n",
      "3   youre          4\n",
      "4    dies          4\n",
      "There are 153 tokens in the data.\n",
      "There are 53 unique tokens in the data.\n",
      "There are 771 characters in the data.\n",
      "The lexical diversity is 0.346 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  borrowed          9\n",
      "1      time          9\n",
      "2    living          8\n",
      "3      love          8\n",
      "4   another          8\n",
      "There are 183 tokens in the data.\n",
      "There are 106 unique tokens in the data.\n",
      "There are 871 characters in the data.\n",
      "The lexical diversity is 0.579 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0     go          6\n",
      "1   boys          5\n",
      "2   cant          5\n",
      "3  shine          4\n",
      "4  shoes          4\n",
      "There are 78 tokens in the data.\n",
      "There are 52 unique tokens in the data.\n",
      "There are 395 characters in the data.\n",
      "The lexical diversity is 0.667 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   love          5\n",
      "1  morei          4\n",
      "2   stop          4\n",
      "3   know          3\n",
      "4   cant          3\n",
      "There are 102 tokens in the data.\n",
      "There are 31 unique tokens in the data.\n",
      "There are 411 characters in the data.\n",
      "The lexical diversity is 0.304 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  gotta         13\n",
      "1     go         10\n",
      "2     im          9\n",
      "3    way          6\n",
      "4    end          5\n",
      "There are 114 tokens in the data.\n",
      "There are 62 unique tokens in the data.\n",
      "There are 492 characters in the data.\n",
      "The lexical diversity is 0.544 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0    ya          8\n",
      "1  cant          6\n",
      "2  fool          5\n",
      "3  cher          5\n",
      "4  greg          4\n",
      "There are 44 tokens in the data.\n",
      "There are 28 unique tokens in the data.\n",
      "There are 192 characters in the data.\n",
      "The lexical diversity is 0.636 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       ill          4\n",
      "1      sing          3\n",
      "2  carnival          3\n",
      "3      time          3\n",
      "4       sun          2\n",
      "There are 144 tokens in the data.\n",
      "There are 73 unique tokens in the data.\n",
      "There are 747 characters in the data.\n",
      "The lexical diversity is 0.507 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       man         14\n",
      "1  carousel         13\n",
      "2    around         10\n",
      "3      know          5\n",
      "4     going          5\n",
      "There are 77 tokens in the data.\n",
      "There are 53 unique tokens in the data.\n",
      "There are 343 characters in the data.\n",
      "The lexical diversity is 0.688 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0     ah          4\n",
      "1    may          4\n",
      "2   well          4\n",
      "3    try          4\n",
      "4  catch          4\n",
      "There are 120 tokens in the data.\n",
      "There are 45 unique tokens in the data.\n",
      "There are 568 characters in the data.\n",
      "The lexical diversity is 0.375 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   good         14\n",
      "1  times          8\n",
      "2   long          7\n",
      "3   life          6\n",
      "4  gonna          4\n",
      "There are 97 tokens in the data.\n",
      "There are 66 unique tokens in the data.\n",
      "There are 444 characters in the data.\n",
      "The lexical diversity is 0.680 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   sun          3\n",
      "1   one          3\n",
      "2  make          3\n",
      "3  hate          3\n",
      "4  love          3\n",
      "There are 135 tokens in the data.\n",
      "There are 67 unique tokens in the data.\n",
      "There are 678 characters in the data.\n",
      "The lexical diversity is 0.496 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  chiquitita         12\n",
      "1        sing          7\n",
      "2        like          6\n",
      "3         new          5\n",
      "4        song          5\n",
      "There are 180 tokens in the data.\n",
      "There are 83 unique tokens in the data.\n",
      "There are 913 characters in the data.\n",
      "The lexical diversity is 0.461 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  chiquitita         13\n",
      "1          tu          9\n",
      "2         que          8\n",
      "3      quiero          7\n",
      "4   compartir          6\n",
      "There are 71 tokens in the data.\n",
      "There are 48 unique tokens in the data.\n",
      "There are 299 characters in the data.\n",
      "The lexical diversity is 0.676 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  love          7\n",
      "1  time          6\n",
      "2   one          4\n",
      "3  wish          3\n",
      "4  know          2\n",
      "There are 20 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 136 characters in the data.\n",
      "The lexical diversity is 0.400 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0       nguqo          4\n",
      "1  ngqothwane          4\n",
      "2      igqira          2\n",
      "3    lendlela          2\n",
      "4  sebeqabele          2\n",
      "There are 76 tokens in the data.\n",
      "There are 47 unique tokens in the data.\n",
      "There are 334 characters in the data.\n",
      "The lexical diversity is 0.618 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    ill          8\n",
      "1   stay          6\n",
      "2  youll          5\n",
      "3   true          4\n",
      "4   come          4\n",
      "There are 122 tokens in the data.\n",
      "There are 87 unique tokens in the data.\n",
      "There are 567 characters in the data.\n",
      "The lexical diversity is 0.713 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   come          6\n",
      "1   dont          4\n",
      "2    let          4\n",
      "3     im          3\n",
      "4  youre          3\n",
      "There are 142 tokens in the data.\n",
      "There are 62 unique tokens in the data.\n",
      "There are 709 characters in the data.\n",
      "The lexical diversity is 0.437 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0   couldve         18\n",
      "1      baby         13\n",
      "2       see          7\n",
      "3       say          5\n",
      "4  standing          5\n",
      "There are 90 tokens in the data.\n",
      "There are 35 unique tokens in the data.\n",
      "There are 419 characters in the data.\n",
      "The lexical diversity is 0.389 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    cry         10\n",
      "1   like          9\n",
      "2   baby          9\n",
      "3  think          5\n",
      "4   love          5\n",
      "There are 60 tokens in the data.\n",
      "There are 23 unique tokens in the data.\n",
      "There are 265 characters in the data.\n",
      "The lexical diversity is 0.383 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    cry         10\n",
      "1  sleep          7\n",
      "2    hes          5\n",
      "3   gone          5\n",
      "4  still          4\n",
      "There are 114 tokens in the data.\n",
      "There are 56 unique tokens in the data.\n",
      "There are 577 characters in the data.\n",
      "The lexical diversity is 0.491 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  dancing          9\n",
      "1    queen          9\n",
      "2    dance          5\n",
      "3  digging          5\n",
      "4    youre          4\n",
      "There are 90 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 483 characters in the data.\n",
      "The lexical diversity is 0.633 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0      would          6\n",
      "1  dangerous          5\n",
      "2      times          5\n",
      "3       keep          4\n",
      "4       know          3\n",
      "There are 78 tokens in the data.\n",
      "There are 52 unique tokens in the data.\n",
      "There are 350 characters in the data.\n",
      "The lexical diversity is 0.667 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   come          4\n",
      "1     oh          3\n",
      "2  danny          3\n",
      "3    boy          3\n",
      "4   find          3\n",
      "There are 129 tokens in the data.\n",
      "There are 101 unique tokens in the data.\n",
      "There are 647 characters in the data.\n",
      "The lexical diversity is 0.783 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    said          4\n",
      "1   black          3\n",
      "2  chorus          3\n",
      "3    dark          3\n",
      "4    lady          3\n",
      "There are 92 tokens in the data.\n",
      "There are 46 unique tokens in the data.\n",
      "There are 452 characters in the data.\n",
      "The lexical diversity is 0.500 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    could         12\n",
      "1    start          6\n",
      "2  singing          6\n",
      "3   finish          4\n",
      "4      ole          4\n",
      "There are 193 tokens in the data.\n",
      "There are 116 unique tokens in the data.\n",
      "There are 952 characters in the data.\n",
      "The lexical diversity is 0.601 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0     youre         14\n",
      "1      read          7\n",
      "2      lips          7\n",
      "3  disaster          6\n",
      "4      babe          5\n",
      "There are 103 tokens in the data.\n",
      "There are 87 unique tokens in the data.\n",
      "There are 516 characters in the data.\n",
      "The lexical diversity is 0.845 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   dixie          3\n",
      "1   wanna          3\n",
      "2    like          3\n",
      "3    land          2\n",
      "4  cotton          2\n",
      "There are 143 tokens in the data.\n",
      "There are 83 unique tokens in the data.\n",
      "There are 735 characters in the data.\n",
      "The lexical diversity is 0.580 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    dixie          6\n",
      "1    small          6\n",
      "2     girl          5\n",
      "3      day          4\n",
      "4  waiting          3\n",
      "There are 202 tokens in the data.\n",
      "There are 71 unique tokens in the data.\n",
      "There are 1000 characters in the data.\n",
      "The lexical diversity is 0.351 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0   anybody         15\n",
      "1    really         15\n",
      "2      know         15\n",
      "3      love         12\n",
      "4  somebody         12\n",
      "There are 72 tokens in the data.\n",
      "There are 28 unique tokens in the data.\n",
      "There are 386 characters in the data.\n",
      "The lexical diversity is 0.389 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     ever         15\n",
      "1    cross          7\n",
      "2     mind          7\n",
      "3  darling          7\n",
      "4     time          4\n",
      "There are 98 tokens in the data.\n",
      "There are 81 unique tokens in the data.\n",
      "There are 541 characters in the data.\n",
      "The lexical diversity is 0.827 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    dont          3\n",
      "1    come          3\n",
      "2  around          3\n",
      "3      im          3\n",
      "4    used          3\n",
      "There are 121 tokens in the data.\n",
      "There are 61 unique tokens in the data.\n",
      "There are 545 characters in the data.\n",
      "The lexical diversity is 0.504 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   love         16\n",
      "1   dont         14\n",
      "2   hide         10\n",
      "3   care          4\n",
      "4  heart          3\n",
      "There are 131 tokens in the data.\n",
      "There are 81 unique tokens in the data.\n",
      "There are 608 characters in the data.\n",
      "The lexical diversity is 0.618 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   aint          7\n",
      "1   dont          7\n",
      "2    use          6\n",
      "3   baby          4\n",
      "4  think          4\n",
      "There are 117 tokens in the data.\n",
      "There are 56 unique tokens in the data.\n",
      "There are 521 characters in the data.\n",
      "The lexical diversity is 0.479 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    time          5\n",
      "1     say          4\n",
      "2    dont          4\n",
      "3  spring          4\n",
      "4     sun          4\n",
      "There are 97 tokens in the data.\n",
      "There are 41 unique tokens in the data.\n",
      "There are 465 characters in the data.\n",
      "The lexical diversity is 0.423 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  right         10\n",
      "1   alls         10\n",
      "2    man          7\n",
      "3   want          5\n",
      "4    day          5\n",
      "There are 151 tokens in the data.\n",
      "There are 61 unique tokens in the data.\n",
      "There are 682 characters in the data.\n",
      "The lexical diversity is 0.404 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  love         15\n",
      "1  song          9\n",
      "2   ill          7\n",
      "3  dove          6\n",
      "4  come          5\n",
      "There are 111 tokens in the data.\n",
      "There are 67 unique tokens in the data.\n",
      "There are 483 characters in the data.\n",
      "The lexical diversity is 0.604 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    see          6\n",
      "1   love          4\n",
      "2  sweet          4\n",
      "3  never          4\n",
      "4   come          4\n",
      "There are 78 tokens in the data.\n",
      "There are 45 unique tokens in the data.\n",
      "There are 389 characters in the data.\n",
      "The lexical diversity is 0.577 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      rock          6\n",
      "1      know          4\n",
      "2      wont          4\n",
      "3      push          3\n",
      "4  mountain          3\n",
      "There are 120 tokens in the data.\n",
      "There are 73 unique tokens in the data.\n",
      "There are 579 characters in the data.\n",
      "The lexical diversity is 0.608 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  believe         15\n",
      "1    magic          9\n",
      "2     like          6\n",
      "3    music          5\n",
      "4       go          4\n",
      "There are 89 tokens in the data.\n",
      "There are 40 unique tokens in the data.\n",
      "There are 374 characters in the data.\n",
      "The lexical diversity is 0.449 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    hes          7\n",
      "1  dream          7\n",
      "2     oh          7\n",
      "3   love          4\n",
      "4   feel          4\n",
      "There are 111 tokens in the data.\n",
      "There are 52 unique tokens in the data.\n",
      "There are 510 characters in the data.\n",
      "The lexical diversity is 0.468 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0       im          7\n",
      "1  dressed          7\n",
      "2     kill          7\n",
      "3     know          6\n",
      "4      one          4\n",
      "There are 101 tokens in the data.\n",
      "There are 49 unique tokens in the data.\n",
      "There are 527 characters in the data.\n",
      "The lexical diversity is 0.485 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0     mornin          7\n",
      "1       love          7\n",
      "2      early          5\n",
      "3  strangers          5\n",
      "4    without          4\n",
      "There are 55 tokens in the data.\n",
      "There are 23 unique tokens in the data.\n",
      "There are 293 characters in the data.\n",
      "The lexical diversity is 0.418 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    easy         13\n",
      "1  people          7\n",
      "2   proud          4\n",
      "3    cold          3\n",
      "4     say          3\n",
      "There are 104 tokens in the data.\n",
      "There are 73 unique tokens in the data.\n",
      "There are 589 characters in the data.\n",
      "The lexical diversity is 0.702 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0      might          5\n",
      "1  something          5\n",
      "2     dreams          3\n",
      "3  brightest          3\n",
      "4       wind          2\n",
      "There are 141 tokens in the data.\n",
      "There are 50 unique tokens in the data.\n",
      "There are 690 characters in the data.\n",
      "The lexical diversity is 0.355 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       fire         15\n",
      "1  emotional         12\n",
      "2       cant          8\n",
      "3        see          6\n",
      "4       baby          6\n",
      "There are 130 tokens in the data.\n",
      "There are 61 unique tokens in the data.\n",
      "There are 626 characters in the data.\n",
      "The lexical diversity is 0.469 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  company          9\n",
      "1    youre          7\n",
      "2     fast          6\n",
      "3     life          4\n",
      "4     lord          4\n",
      "There are 165 tokens in the data.\n",
      "There are 64 unique tokens in the data.\n",
      "There are 848 characters in the data.\n",
      "The lexical diversity is 0.388 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      love         16\n",
      "1  favorite         12\n",
      "2     scars         12\n",
      "3     loves          6\n",
      "4        oh          6\n",
      "There are 142 tokens in the data.\n",
      "There are 82 unique tokens in the data.\n",
      "There are 806 characters in the data.\n",
      "The lexical diversity is 0.577 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  fernando         17\n",
      "1     night          5\n",
      "2     would          5\n",
      "3    friend          5\n",
      "4     could          4\n",
      "There are 142 tokens in the data.\n",
      "There are 83 unique tokens in the data.\n",
      "There are 805 characters in the data.\n",
      "The lexical diversity is 0.585 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  fernando         16\n",
      "1     night          5\n",
      "2     would          5\n",
      "3    friend          5\n",
      "4     could          4\n",
      "There are 155 tokens in the data.\n",
      "There are 82 unique tokens in the data.\n",
      "There are 697 characters in the data.\n",
      "The lexical diversity is 0.529 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      ive         12\n",
      "1     seen         12\n",
      "2  thought          7\n",
      "3      see          5\n",
      "4     time          5\n",
      "There are 145 tokens in the data.\n",
      "There are 79 unique tokens in the data.\n",
      "There are 714 characters in the data.\n",
      "The lexical diversity is 0.545 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0     still          8\n",
      "1      love          7\n",
      "2  remember          6\n",
      "3      know          6\n",
      "4     fires          6\n",
      "There are 111 tokens in the data.\n",
      "There are 82 unique tokens in the data.\n",
      "There are 500 characters in the data.\n",
      "The lexical diversity is 0.739 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0    im          8\n",
      "1   fit          6\n",
      "2   fly          6\n",
      "3  cant          3\n",
      "4   see          3\n",
      "There are 123 tokens in the data.\n",
      "There are 73 unique tokens in the data.\n",
      "There are 667 characters in the data.\n",
      "The lexical diversity is 0.593 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0   travis         12\n",
      "1     wish          3\n",
      "2    could          3\n",
      "3   helped          3\n",
      "4  somehow          3\n",
      "There are 117 tokens in the data.\n",
      "There are 63 unique tokens in the data.\n",
      "There are 633 characters in the data.\n",
      "The lexical diversity is 0.538 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0      whats         14\n",
      "1       stop          7\n",
      "2      sound          7\n",
      "3  everybody          7\n",
      "4       look          7\n",
      "There are 77 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 399 characters in the data.\n",
      "The lexical diversity is 0.740 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  whatever          4\n",
      "1     youre          3\n",
      "2      love          3\n",
      "3        im          2\n",
      "4      take          2\n",
      "There are 115 tokens in the data.\n",
      "There are 60 unique tokens in the data.\n",
      "There are 586 characters in the data.\n",
      "The lexical diversity is 0.522 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0        oh         10\n",
      "1      back          7\n",
      "2  cadillac          7\n",
      "3      boys          6\n",
      "4      take          6\n",
      "There are 151 tokens in the data.\n",
      "There are 59 unique tokens in the data.\n",
      "There are 767 characters in the data.\n",
      "The lexical diversity is 0.391 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0     gimme         30\n",
      "1       man         10\n",
      "2  midnight         10\n",
      "3    theres          4\n",
      "4       one          4\n",
      "There are 117 tokens in the data.\n",
      "There are 34 unique tokens in the data.\n",
      "There are 488 characters in the data.\n",
      "The lexical diversity is 0.291 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   wait         12\n",
      "1  wanna         11\n",
      "2   girl         10\n",
      "3   dont         10\n",
      "4   come         10\n",
      "There are 131 tokens in the data.\n",
      "There are 41 unique tokens in the data.\n",
      "There are 546 characters in the data.\n",
      "The lexical diversity is 0.313 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   away         28\n",
      "1  wanna         12\n",
      "2   take          8\n",
      "3    get          6\n",
      "4    old          6\n",
      "There are 107 tokens in the data.\n",
      "There are 71 unique tokens in the data.\n",
      "There are 488 characters in the data.\n",
      "The lexical diversity is 0.664 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     love          7\n",
      "1     give          4\n",
      "2  fightin          4\n",
      "3   chance          4\n",
      "4     away          3\n",
      "There are 91 tokens in the data.\n",
      "There are 35 unique tokens in the data.\n",
      "There are 371 characters in the data.\n",
      "The lexical diversity is 0.385 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0      go         17\n",
      "1  darlin          7\n",
      "2    dont          6\n",
      "3     see          5\n",
      "4    want          5\n",
      "There are 107 tokens in the data.\n",
      "There are 74 unique tokens in the data.\n",
      "There are 555 characters in the data.\n",
      "The lexical diversity is 0.692 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  chorus          4\n",
      "1   money          3\n",
      "2   theyd          3\n",
      "3    born          2\n",
      "4   wagon          2\n",
      "There are 71 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 382 characters in the data.\n",
      "The lexical diversity is 0.803 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0        man          3\n",
      "1     chorus          3\n",
      "2  halfbreed          3\n",
      "3   cherokee          2\n",
      "4    ashamed          2\n",
      "There are 88 tokens in the data.\n",
      "There are 47 unique tokens in the data.\n",
      "There are 434 characters in the data.\n",
      "The lexical diversity is 0.534 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0        joe          9\n",
      "1  president          5\n",
      "2      seems          4\n",
      "3       like          3\n",
      "4  happiness          3\n",
      "There are 122 tokens in the data.\n",
      "There are 65 unique tokens in the data.\n",
      "There are 557 characters in the data.\n",
      "The lexical diversity is 0.533 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   ever          6\n",
      "1    day          6\n",
      "2  happy          6\n",
      "3    met          5\n",
      "4  never          5\n",
      "There are 133 tokens in the data.\n",
      "There are 72 unique tokens in the data.\n",
      "There are 653 characters in the data.\n",
      "The lexical diversity is 0.541 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     hard          6\n",
      "1   enough          6\n",
      "2  getting          6\n",
      "3     dont          5\n",
      "4    could          5\n",
      "There are 66 tokens in the data.\n",
      "There are 42 unique tokens in the data.\n",
      "There are 319 characters in the data.\n",
      "The lexical diversity is 0.636 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      hes          6\n",
      "1  brother          6\n",
      "2     aint          4\n",
      "3    heavy          4\n",
      "4     long          3\n",
      "There are 151 tokens in the data.\n",
      "There are 93 unique tokens in the data.\n",
      "There are 764 characters in the data.\n",
      "The lexical diversity is 0.616 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0      heart         13\n",
      "1      stone          9\n",
      "2       wish          7\n",
      "3       dont          5\n",
      "4  sometimes          5\n",
      "There are 107 tokens in the data.\n",
      "There are 71 unique tokens in the data.\n",
      "There are 516 characters in the data.\n",
      "The lexical diversity is 0.664 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   hell          8\n",
      "1  never          8\n",
      "2   know          8\n",
      "3   mine          3\n",
      "4    son          3\n",
      "There are 229 tokens in the data.\n",
      "There are 51 unique tokens in the data.\n",
      "There are 1016 characters in the data.\n",
      "The lexical diversity is 0.223 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0        see         16\n",
      "1  something         16\n",
      "2         go         12\n",
      "3       lets         12\n",
      "4       roll         12\n",
      "There are 94 tokens in the data.\n",
      "There are 78 unique tokens in the data.\n",
      "There are 474 characters in the data.\n",
      "The lexical diversity is 0.830 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       like          3\n",
      "1        ill          3\n",
      "2  beautiful          3\n",
      "3       made          2\n",
      "4      music          2\n",
      "There are 105 tokens in the data.\n",
      "There are 32 unique tokens in the data.\n",
      "There are 427 characters in the data.\n",
      "The lexical diversity is 0.305 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  shot         11\n",
      "1   hey          9\n",
      "2   joe          8\n",
      "3  goin          6\n",
      "4  said          6\n",
      "There are 148 tokens in the data.\n",
      "There are 68 unique tokens in the data.\n",
      "There are 697 characters in the data.\n",
      "The lexical diversity is 0.459 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  holdin         25\n",
      "1      im         15\n",
      "2    love         15\n",
      "3    time         11\n",
      "4    mind          6\n",
      "There are 174 tokens in the data.\n",
      "There are 66 unique tokens in the data.\n",
      "There are 826 characters in the data.\n",
      "The lexical diversity is 0.379 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   holy         15\n",
      "1  smoke         15\n",
      "2    say         11\n",
      "3    get          6\n",
      "4  solve          6\n",
      "There are 104 tokens in the data.\n",
      "There are 64 unique tokens in the data.\n",
      "There are 603 characters in the data.\n",
      "The lexical diversity is 0.615 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      home          9\n",
      "1  homeward          6\n",
      "2     bound          6\n",
      "3  silently          4\n",
      "4      evry          3\n",
      "There are 72 tokens in the data.\n",
      "There are 47 unique tokens in the data.\n",
      "There are 321 characters in the data.\n",
      "The lexical diversity is 0.653 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  house          6\n",
      "1    one          5\n",
      "2   room          4\n",
      "3  chair          3\n",
      "4  still          3\n",
      "There are 85 tokens in the data.\n",
      "There are 44 unique tokens in the data.\n",
      "There are 406 characters in the data.\n",
      "The lexical diversity is 0.518 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    mend          6\n",
      "1  broken          6\n",
      "2   heart          4\n",
      "3    stop          4\n",
      "4     man          3\n",
      "There are 86 tokens in the data.\n",
      "There are 40 unique tokens in the data.\n",
      "There are 386 characters in the data.\n",
      "The lexical diversity is 0.465 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    long          8\n",
      "1   going          8\n",
      "2      oh          7\n",
      "3  heaven          4\n",
      "4   could          3\n",
      "There are 128 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 636 characters in the data.\n",
      "The lexical diversity is 0.445 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  believe         33\n",
      "1      man         21\n",
      "2      got          8\n",
      "3    every          3\n",
      "4     baby          3\n",
      "There are 149 tokens in the data.\n",
      "There are 60 unique tokens in the data.\n",
      "There are 767 characters in the data.\n",
      "The lexical diversity is 0.403 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  dream         14\n",
      "1  youre         10\n",
      "2   dont          8\n",
      "3  sleep          8\n",
      "4  never          6\n",
      "There are 83 tokens in the data.\n",
      "There are 46 unique tokens in the data.\n",
      "There are 398 characters in the data.\n",
      "The lexical diversity is 0.554 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0     youve          9\n",
      "1     loved          8\n",
      "2  somebody          7\n",
      "3       ive          5\n",
      "4      like          4\n",
      "There are 88 tokens in the data.\n",
      "There are 45 unique tokens in the data.\n",
      "There are 413 characters in the data.\n",
      "The lexical diversity is 0.511 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       feel          5\n",
      "1  something          4\n",
      "2       used          4\n",
      "3       fall          4\n",
      "4        air          3\n",
      "There are 200 tokens in the data.\n",
      "There are 64 unique tokens in the data.\n",
      "There are 871 characters in the data.\n",
      "The lexical diversity is 0.320 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  could         18\n",
      "1   back         14\n",
      "2   turn         10\n",
      "3   time         10\n",
      "4     id          7\n",
      "There are 74 tokens in the data.\n",
      "There are 48 unique tokens in the data.\n",
      "There are 355 characters in the data.\n",
      "The lexical diversity is 0.649 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    know          6\n",
      "1   would          4\n",
      "2    knew          4\n",
      "3    many          4\n",
      "4  cowboy          3\n",
      "There are 132 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 666 characters in the data.\n",
      "The lexical diversity is 0.432 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   away          9\n",
      "1   take          8\n",
      "2   baby          7\n",
      "3  since          7\n",
      "4  youve          7\n",
      "There are 145 tokens in the data.\n",
      "There are 47 unique tokens in the data.\n",
      "There are 685 characters in the data.\n",
      "The lexical diversity is 0.324 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   love         20\n",
      "1  gonna         18\n",
      "2  found         11\n",
      "3    hes          6\n",
      "4    new          5\n",
      "There are 55 tokens in the data.\n",
      "There are 28 unique tokens in the data.\n",
      "There are 225 characters in the data.\n",
      "The lexical diversity is 0.509 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   bad         12\n",
      "1   got          6\n",
      "2  aint          4\n",
      "3  good          3\n",
      "4   way          2\n",
      "There are 70 tokens in the data.\n",
      "There are 41 unique tokens in the data.\n",
      "There are 338 characters in the data.\n",
      "The lexical diversity is 0.586 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    sleep         12\n",
      "1       go          6\n",
      "2  imagine          6\n",
      "3    youre          6\n",
      "4     look          2\n",
      "There are 113 tokens in the data.\n",
      "There are 59 unique tokens in the data.\n",
      "There are 460 characters in the data.\n",
      "The lexical diversity is 0.522 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   got         24\n",
      "1  babe         14\n",
      "2  dont          4\n",
      "3   say          3\n",
      "4  wont          3\n",
      "There are 56 tokens in the data.\n",
      "There are 33 unique tokens in the data.\n",
      "There are 253 characters in the data.\n",
      "The lexical diversity is 0.589 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  alone          5\n",
      "1   hate          4\n",
      "2  sleep          4\n",
      "3   know          3\n",
      "4  could          2\n",
      "There are 130 tokens in the data.\n",
      "There are 61 unique tokens in the data.\n",
      "There are 667 characters in the data.\n",
      "The lexical diversity is 0.469 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   hope         14\n",
      "1   find          7\n",
      "2  youre          6\n",
      "3   know          5\n",
      "4     id          3\n",
      "There are 42 tokens in the data.\n",
      "There are 18 unique tokens in the data.\n",
      "There are 156 characters in the data.\n",
      "The lexical diversity is 0.429 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0    uh          6\n",
      "1  love          6\n",
      "2   yes          5\n",
      "3  dont          3\n",
      "4  know          3\n",
      "There are 106 tokens in the data.\n",
      "There are 59 unique tokens in the data.\n",
      "There are 522 characters in the data.\n",
      "The lexical diversity is 0.557 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    stop          6\n",
      "1     ill          5\n",
      "2   never          5\n",
      "3  loving          5\n",
      "4     one          4\n",
      "There are 108 tokens in the data.\n",
      "There are 66 unique tokens in the data.\n",
      "There are 497 characters in the data.\n",
      "The lexical diversity is 0.611 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    love         14\n",
      "1  making          6\n",
      "2      oh          6\n",
      "3     ooh          5\n",
      "4    dont          3\n",
      "There are 47 tokens in the data.\n",
      "There are 36 unique tokens in the data.\n",
      "There are 228 characters in the data.\n",
      "The lexical diversity is 0.766 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   love          4\n",
      "1    ive          2\n",
      "2  blind          2\n",
      "3   find          2\n",
      "4  cause          2\n",
      "There are 119 tokens in the data.\n",
      "There are 44 unique tokens in the data.\n",
      "There are 638 characters in the data.\n",
      "The lexical diversity is 0.370 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0        dont         12\n",
      "1          im          9\n",
      "2      middle          8\n",
      "3   something          8\n",
      "4  understand          8\n",
      "There are 70 tokens in the data.\n",
      "There are 62 unique tokens in the data.\n",
      "There are 378 characters in the data.\n",
      "The lexical diversity is 0.886 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0        dream          2\n",
      "1        fight          2\n",
      "2        right          2\n",
      "3        reach          2\n",
      "4  unreachable          2\n",
      "There are 76 tokens in the data.\n",
      "There are 56 unique tokens in the data.\n",
      "There are 392 characters in the data.\n",
      "The lexical diversity is 0.737 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      like          4\n",
      "1    chorus          3\n",
      "2      mama          2\n",
      "3      used          2\n",
      "4  weathers          2\n",
      "There are 145 tokens in the data.\n",
      "There are 62 unique tokens in the data.\n",
      "There are 711 characters in the data.\n",
      "The lexical diversity is 0.428 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   got          9\n",
      "1   way          8\n",
      "2    im          6\n",
      "3  take          6\n",
      "4  like          5\n",
      "There are 83 tokens in the data.\n",
      "There are 63 unique tokens in the data.\n",
      "There are 396 characters in the data.\n",
      "The lexical diversity is 0.759 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  danced          6\n",
      "1     saw          3\n",
      "2    kept          3\n",
      "3  walked          2\n",
      "4    song          2\n",
      "There are 67 tokens in the data.\n",
      "There are 41 unique tokens in the data.\n",
      "There are 342 characters in the data.\n",
      "The lexical diversity is 0.612 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   cant          4\n",
      "1    get          4\n",
      "2   need          3\n",
      "3    way          3\n",
      "4  think          2\n",
      "There are 69 tokens in the data.\n",
      "There are 60 unique tokens in the data.\n",
      "There are 320 characters in the data.\n",
      "The lexical diversity is 0.870 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   love          4\n",
      "1    way          2\n",
      "2   look          2\n",
      "3  heart          2\n",
      "4   kind          2\n",
      "There are 53 tokens in the data.\n",
      "There are 30 unique tokens in the data.\n",
      "There are 250 characters in the data.\n",
      "The lexical diversity is 0.566 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    eyes          6\n",
      "1  around          4\n",
      "2    many          4\n",
      "3    gets          3\n",
      "4   wanna          3\n",
      "There are 67 tokens in the data.\n",
      "There are 53 unique tokens in the data.\n",
      "There are 310 characters in the data.\n",
      "The lexical diversity is 0.791 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   away          5\n",
      "1   love          4\n",
      "2  threw          3\n",
      "3   dont          2\n",
      "4   cant          2\n",
      "There are 85 tokens in the data.\n",
      "There are 42 unique tokens in the data.\n",
      "There are 414 characters in the data.\n",
      "The lexical diversity is 0.494 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0   monday          7\n",
      "1    might          6\n",
      "2     well          6\n",
      "3     stay          6\n",
      "4  nothing          6\n",
      "There are 135 tokens in the data.\n",
      "There are 70 unique tokens in the data.\n",
      "There are 621 characters in the data.\n",
      "The lexical diversity is 0.519 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    love         11\n",
      "1   shame         11\n",
      "2   cryin         10\n",
      "3    dont         10\n",
      "4  happen          9\n",
      "There are 98 tokens in the data.\n",
      "There are 44 unique tokens in the data.\n",
      "There are 465 characters in the data.\n",
      "The lexical diversity is 0.449 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      man         10\n",
      "1      hes          7\n",
      "2     lost          7\n",
      "3  nothing          6\n",
      "4     made          5\n",
      "There are 55 tokens in the data.\n",
      "There are 31 unique tokens in the data.\n",
      "There are 271 characters in the data.\n",
      "The lexical diversity is 0.564 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  unusual          8\n",
      "1   anyone          6\n",
      "2     love          5\n",
      "3      see          3\n",
      "4     find          3\n",
      "There are 82 tokens in the data.\n",
      "There are 29 unique tokens in the data.\n",
      "There are 358 characters in the data.\n",
      "The lexical diversity is 0.354 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  late         10\n",
      "1  love         10\n",
      "2   say          4\n",
      "3   bad          4\n",
      "4  know          4\n",
      "There are 189 tokens in the data.\n",
      "There are 76 unique tokens in the data.\n",
      "There are 883 characters in the data.\n",
      "The lexical diversity is 0.402 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    time         31\n",
      "1  theres         17\n",
      "2    walk         13\n",
      "3   alone         13\n",
      "4   gotta         10\n",
      "There are 158 tokens in the data.\n",
      "There are 67 unique tokens in the data.\n",
      "There are 750 characters in the data.\n",
      "The lexical diversity is 0.424 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     come         24\n",
      "1     till         16\n",
      "2     burn         16\n",
      "3     walk         11\n",
      "4  guilded          9\n",
      "There are 101 tokens in the data.\n",
      "There are 73 unique tokens in the data.\n",
      "There are 497 characters in the data.\n",
      "The lexical diversity is 0.723 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   want         16\n",
      "1    bad          4\n",
      "2  honey          4\n",
      "3   wait          3\n",
      "4  wasnt          2\n",
      "There are 65 tokens in the data.\n",
      "There are 36 unique tokens in the data.\n",
      "There are 309 characters in the data.\n",
      "The lexical diversity is 0.554 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  walked          6\n",
      "1   wasnt          5\n",
      "2   ready          5\n",
      "3    last          3\n",
      "4   night          3\n",
      "There are 53 tokens in the data.\n",
      "There are 24 unique tokens in the data.\n",
      "There are 276 characters in the data.\n",
      "The lexical diversity is 0.453 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      wait          7\n",
      "1      till          5\n",
      "2   forever          4\n",
      "3     takes          3\n",
      "4  thousand          3\n",
      "There are 94 tokens in the data.\n",
      "There are 58 unique tokens in the data.\n",
      "There are 430 characters in the data.\n",
      "The lexical diversity is 0.617 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      way          7\n",
      "1  treated          7\n",
      "2  wouldnt          6\n",
      "3      dog          6\n",
      "4    treat          5\n",
      "There are 120 tokens in the data.\n",
      "There are 68 unique tokens in the data.\n",
      "There are 543 characters in the data.\n",
      "The lexical diversity is 0.567 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    sonny          6\n",
      "1      boy          6\n",
      "2       im          6\n",
      "3  forsake          4\n",
      "4   coming          4\n",
      "There are 123 tokens in the data.\n",
      "There are 65 unique tokens in the data.\n",
      "There are 581 characters in the data.\n",
      "The lexical diversity is 0.528 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  julie         30\n",
      "1  lying          6\n",
      "2     im          5\n",
      "3  youre          5\n",
      "4   well          4\n",
      "There are 91 tokens in the data.\n",
      "There are 45 unique tokens in the data.\n",
      "There are 448 characters in the data.\n",
      "The lexical diversity is 0.495 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  enough         15\n",
      "1    keep          8\n",
      "2  hangin          7\n",
      "3      ah          5\n",
      "4   honey          4\n",
      "There are 207 tokens in the data.\n",
      "There are 105 unique tokens in the data.\n",
      "There are 951 characters in the data.\n",
      "The lexical diversity is 0.507 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    gonna         10\n",
      "1    youre          9\n",
      "2     come          9\n",
      "3     baby          9\n",
      "4  tonight          8\n",
      "There are 101 tokens in the data.\n",
      "There are 64 unique tokens in the data.\n",
      "There are 465 characters in the data.\n",
      "The lexical diversity is 0.634 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      one          7\n",
      "1     time          7\n",
      "2      ive          5\n",
      "3      got          4\n",
      "4  believe          4\n",
      "There are 75 tokens in the data.\n",
      "There are 41 unique tokens in the data.\n",
      "There are 382 characters in the data.\n",
      "The lexical diversity is 0.547 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    youre          5\n",
      "1      ive          5\n",
      "2  looking          5\n",
      "3    never          3\n",
      "4     time          3\n",
      "There are 164 tokens in the data.\n",
      "There are 79 unique tokens in the data.\n",
      "There are 803 characters in the data.\n",
      "The lexical diversity is 0.482 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      kiss         34\n",
      "1     makin          4\n",
      "2      miss          4\n",
      "3    hiding          4\n",
      "4  daylight          4\n",
      "There are 125 tokens in the data.\n",
      "There are 43 unique tokens in the data.\n",
      "There are 643 characters in the data.\n",
      "The lexical diversity is 0.344 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    knock         27\n",
      "1   better         12\n",
      "2     wood         10\n",
      "3     love          7\n",
      "4  thunder          6\n",
      "There are 142 tokens in the data.\n",
      "There are 65 unique tokens in the data.\n",
      "There are 685 characters in the data.\n",
      "The lexical diversity is 0.458 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     get         16\n",
      "1      im         12\n",
      "2  coming          6\n",
      "3   tired          5\n",
      "4    home          4\n",
      "There are 118 tokens in the data.\n",
      "There are 51 unique tokens in the data.\n",
      "There are 498 characters in the data.\n",
      "The lexical diversity is 0.432 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     lay         18\n",
      "1    stay         12\n",
      "2    baby         10\n",
      "3  across          6\n",
      "4     big          6\n",
      "There are 63 tokens in the data.\n",
      "There are 21 unique tokens in the data.\n",
      "There are 249 characters in the data.\n",
      "The lexical diversity is 0.333 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    let         15\n",
      "1  youre          6\n",
      "2   easy          6\n",
      "3     ah          6\n",
      "4  gonna          5\n",
      "There are 142 tokens in the data.\n",
      "There are 83 unique tokens in the data.\n",
      "There are 690 characters in the data.\n",
      "The lexical diversity is 0.585 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    shes         13\n",
      "1  pretty          9\n",
      "2    tied          9\n",
      "3  hangin          6\n",
      "4  upside          6\n",
      "There are 113 tokens in the data.\n",
      "There are 68 unique tokens in the data.\n",
      "There are 523 characters in the data.\n",
      "The lexical diversity is 0.602 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   dont          5\n",
      "1  youre          4\n",
      "2     us          4\n",
      "3    one          4\n",
      "4     oh          3\n",
      "There are 146 tokens in the data.\n",
      "There are 106 unique tokens in the data.\n",
      "There are 778 characters in the data.\n",
      "The lexical diversity is 0.726 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     like          7\n",
      "1     feel          6\n",
      "2     used          4\n",
      "3  without          4\n",
      "4     home          4\n",
      "There are 49 tokens in the data.\n",
      "There are 38 unique tokens in the data.\n",
      "There are 257 characters in the data.\n",
      "The lexical diversity is 0.776 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   sad          3\n",
      "1  look          2\n",
      "2    us          2\n",
      "3  long          2\n",
      "4   ago          2\n",
      "There are 126 tokens in the data.\n",
      "There are 55 unique tokens in the data.\n",
      "There are 617 characters in the data.\n",
      "The lexical diversity is 0.437 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     love         13\n",
      "1     long          8\n",
      "2  distant          8\n",
      "3   affair          8\n",
      "4  station          8\n",
      "There are 45 tokens in the data.\n",
      "There are 21 unique tokens in the data.\n",
      "There are 196 characters in the data.\n",
      "The lexical diversity is 0.467 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    look          6\n",
      "1    tell          6\n",
      "2     see          6\n",
      "3  nights          2\n",
      "4    long          2\n",
      "There are 152 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 809 characters in the data.\n",
      "The lexical diversity is 0.375 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0         enough         21\n",
      "1           love         17\n",
      "2  understanding          8\n",
      "3         theres          6\n",
      "4            got          5\n",
      "There are 103 tokens in the data.\n",
      "There are 58 unique tokens in the data.\n",
      "There are 524 characters in the data.\n",
      "The lexical diversity is 0.563 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    love          9\n",
      "1  enough          9\n",
      "2   gotta          6\n",
      "3   comes          4\n",
      "4    goes          3\n",
      "There are 108 tokens in the data.\n",
      "There are 51 unique tokens in the data.\n",
      "There are 491 characters in the data.\n",
      "The lexical diversity is 0.472 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   love         18\n",
      "1  hurts         12\n",
      "2   know          6\n",
      "3    lot          5\n",
      "4   isnt          4\n",
      "There are 87 tokens in the data.\n",
      "There are 50 unique tokens in the data.\n",
      "There are 399 characters in the data.\n",
      "The lexical diversity is 0.575 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   love         12\n",
      "1  hurts          7\n",
      "2    lot          5\n",
      "3   know          4\n",
      "4    ive          3\n",
      "There are 109 tokens in the data.\n",
      "There are 45 unique tokens in the data.\n",
      "There are 518 characters in the data.\n",
      "The lexical diversity is 0.413 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    love         14\n",
      "1   place          9\n",
      "2      im          7\n",
      "3  lonely          6\n",
      "4  almost          6\n",
      "There are 144 tokens in the data.\n",
      "There are 84 unique tokens in the data.\n",
      "There are 705 characters in the data.\n",
      "The lexical diversity is 0.583 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    love         15\n",
      "1  groove         15\n",
      "2    move         14\n",
      "3     get          4\n",
      "4    like          3\n",
      "There are 67 tokens in the data.\n",
      "There are 34 unique tokens in the data.\n",
      "There are 292 characters in the data.\n",
      "The lexical diversity is 0.507 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    ill          5\n",
      "1     oh          4\n",
      "2   love          3\n",
      "3  heart          3\n",
      "4   ever          3\n",
      "There are 117 tokens in the data.\n",
      "There are 52 unique tokens in the data.\n",
      "There are 627 characters in the data.\n",
      "The lexical diversity is 0.444 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      love         11\n",
      "1  remember          7\n",
      "2   rooftop          7\n",
      "3       got          6\n",
      "4     night          5\n",
      "There are 117 tokens in the data.\n",
      "There are 63 unique tokens in the data.\n",
      "There are 636 characters in the data.\n",
      "The lexical diversity is 0.538 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       love         12\n",
      "1        one         12\n",
      "2    another         12\n",
      "3  everybody          9\n",
      "4      needs          3\n",
      "There are 103 tokens in the data.\n",
      "There are 44 unique tokens in the data.\n",
      "There are 511 characters in the data.\n",
      "The lexical diversity is 0.427 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    pain          6\n",
      "1  theres          6\n",
      "2    well          6\n",
      "3   guess          6\n",
      "4    help          6\n",
      "There are 73 tokens in the data.\n",
      "There are 61 unique tokens in the data.\n",
      "There are 387 characters in the data.\n",
      "The lexical diversity is 0.836 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    well          4\n",
      "1   offer          2\n",
      "2    show          2\n",
      "3  worlds          2\n",
      "4  chorus          2\n",
      "There are 87 tokens in the data.\n",
      "There are 59 unique tokens in the data.\n",
      "There are 400 characters in the data.\n",
      "The lexical diversity is 0.678 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   love          6\n",
      "1     mi          4\n",
      "2  amore          4\n",
      "3  could          4\n",
      "4   eyes          3\n",
      "There are 83 tokens in the data.\n",
      "There are 55 unique tokens in the data.\n",
      "There are 343 characters in the data.\n",
      "The lexical diversity is 0.663 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    put         11\n",
      "1    lid         11\n",
      "2  right          3\n",
      "3   time          3\n",
      "4  whats          2\n",
      "There are 103 tokens in the data.\n",
      "There are 36 unique tokens in the data.\n",
      "There are 444 characters in the data.\n",
      "The lexical diversity is 0.350 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  youre         14\n",
      "1   main         14\n",
      "2    man         14\n",
      "3     oh          8\n",
      "4  woman          6\n",
      "There are 108 tokens in the data.\n",
      "There are 55 unique tokens in the data.\n",
      "There are 454 characters in the data.\n",
      "The lexical diversity is 0.509 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  make         10\n",
      "1   man          8\n",
      "2  lord          5\n",
      "3  love          5\n",
      "4    ah          5\n",
      "There are 105 tokens in the data.\n",
      "There are 63 unique tokens in the data.\n",
      "There are 495 characters in the data.\n",
      "The lexical diversity is 0.600 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     mama          9\n",
      "1     away          5\n",
      "2  dollies          4\n",
      "3   babies          4\n",
      "4      big          4\n",
      "There are 157 tokens in the data.\n",
      "There are 68 unique tokens in the data.\n",
      "There are 684 characters in the data.\n",
      "The lexical diversity is 0.433 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  mamma         10\n",
      "1    mia         10\n",
      "2    ive          9\n",
      "3     go          9\n",
      "4   know          6\n",
      "There are 168 tokens in the data.\n",
      "There are 124 unique tokens in the data.\n",
      "There are 793 characters in the data.\n",
      "The lexical diversity is 0.738 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  build          4\n",
      "1  death          4\n",
      "2   hide          4\n",
      "3    see          4\n",
      "4     im          4\n",
      "There are 77 tokens in the data.\n",
      "There are 68 unique tokens in the data.\n",
      "There are 391 characters in the data.\n",
      "The lexical diversity is 0.883 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  melody          5\n",
      "1    days          3\n",
      "2    home          2\n",
      "3   sleep          2\n",
      "4    wont          2\n",
      "There are 120 tokens in the data.\n",
      "There are 90 unique tokens in the data.\n",
      "There are 559 characters in the data.\n",
      "The lexical diversity is 0.750 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  milord         11\n",
      "1    come          3\n",
      "2    lips          3\n",
      "3    love          3\n",
      "4  hearts          3\n",
      "There are 171 tokens in the data.\n",
      "There are 65 unique tokens in the data.\n",
      "There are 882 characters in the data.\n",
      "The lexical diversity is 0.380 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  mirror         21\n",
      "1   image         20\n",
      "2     see         10\n",
      "3    life          6\n",
      "4   think          5\n",
      "There are 115 tokens in the data.\n",
      "There are 73 unique tokens in the data.\n",
      "There are 603 characters in the data.\n",
      "The lexical diversity is 0.635 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  little         10\n",
      "1     may          5\n",
      "2    look          3\n",
      "3    seen          3\n",
      "4      21          3\n",
      "There are 77 tokens in the data.\n",
      "There are 42 unique tokens in the data.\n",
      "There are 321 characters in the data.\n",
      "The lexical diversity is 0.545 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    hey         10\n",
      "1  momma          9\n",
      "2   look          5\n",
      "3  sharp          5\n",
      "4    ill          3\n",
      "There are 85 tokens in the data.\n",
      "There are 37 unique tokens in the data.\n",
      "There are 380 characters in the data.\n",
      "The lexical diversity is 0.435 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    know          6\n",
      "1  loving          6\n",
      "2      oh          4\n",
      "3   youre          3\n",
      "4   youll          3\n",
      "There are 76 tokens in the data.\n",
      "There are 18 unique tokens in the data.\n",
      "There are 319 characters in the data.\n",
      "The lexical diversity is 0.237 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    love         23\n",
      "1     way          9\n",
      "2    move          7\n",
      "3    keep          6\n",
      "4  groove          4\n",
      "There are 85 tokens in the data.\n",
      "There are 58 unique tokens in the data.\n",
      "There are 428 characters in the data.\n",
      "The lexical diversity is 0.682 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     dont          8\n",
      "1   change          6\n",
      "2  strange          5\n",
      "3      ask          4\n",
      "4     face          3\n",
      "There are 82 tokens in the data.\n",
      "There are 28 unique tokens in the data.\n",
      "There are 369 characters in the data.\n",
      "The lexical diversity is 0.341 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0        love         31\n",
      "1        good         10\n",
      "2          oh          8\n",
      "3  everywhere          4\n",
      "4  understood          3\n",
      "There are 113 tokens in the data.\n",
      "There are 48 unique tokens in the data.\n",
      "There are 520 characters in the data.\n",
      "The lexical diversity is 0.425 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       love         28\n",
      "1       know          6\n",
      "2      youre          5\n",
      "3       feel          5\n",
      "4  somewhere          4\n",
      "There are 113 tokens in the data.\n",
      "There are 66 unique tokens in the data.\n",
      "There are 497 characters in the data.\n",
      "The lexical diversity is 0.584 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    know          9\n",
      "1     far          8\n",
      "2    gone          8\n",
      "3  doesnt          5\n",
      "4     son          4\n",
      "There are 111 tokens in the data.\n",
      "There are 64 unique tokens in the data.\n",
      "There are 503 characters in the data.\n",
      "The lexical diversity is 0.577 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  needles          6\n",
      "1     pins          6\n",
      "2     stop          5\n",
      "3      saw          4\n",
      "4     face          4\n",
      "There are 86 tokens in the data.\n",
      "There are 39 unique tokens in the data.\n",
      "There are 427 characters in the data.\n",
      "The lexical diversity is 0.453 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      well          8\n",
      "1     never          7\n",
      "2      dont          4\n",
      "3  oklahoma          4\n",
      "4    matter          4\n",
      "There are 99 tokens in the data.\n",
      "There are 66 unique tokens in the data.\n",
      "There are 443 characters in the data.\n",
      "The lexical diversity is 0.667 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    cant          7\n",
      "1    wait          6\n",
      "2    holy          5\n",
      "3  mother          5\n",
      "4     ive          3\n",
      "There are 127 tokens in the data.\n",
      "There are 79 unique tokens in the data.\n",
      "There are 651 characters in the data.\n",
      "The lexical diversity is 0.622 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      know          5\n",
      "1    either          4\n",
      "2     world          4\n",
      "3      dont          3\n",
      "4  standing          3\n",
      "There are 123 tokens in the data.\n",
      "There are 83 unique tokens in the data.\n",
      "There are 554 characters in the data.\n",
      "The lexical diversity is 0.675 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   dont          6\n",
      "1    man          5\n",
      "2  river          5\n",
      "3     ol          4\n",
      "4  keeps          4\n",
      "There are 108 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 495 characters in the data.\n",
      "The lexical diversity is 0.528 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   one         17\n",
      "1  love          6\n",
      "2   end          4\n",
      "3  much          4\n",
      "4  take          4\n",
      "There are 113 tokens in the data.\n",
      "There are 45 unique tokens in the data.\n",
      "There are 539 characters in the data.\n",
      "The lexical diversity is 0.398 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    cant         13\n",
      "1    find         12\n",
      "2     man         12\n",
      "3     one         10\n",
      "4  honest         10\n",
      "There are 115 tokens in the data.\n",
      "There are 54 unique tokens in the data.\n",
      "There are 590 characters in the data.\n",
      "The lexical diversity is 0.470 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      one         12\n",
      "1       us         12\n",
      "2   lonely          5\n",
      "3  wishing          5\n",
      "4  feeling          4\n",
      "There are 128 tokens in the data.\n",
      "There are 54 unique tokens in the data.\n",
      "There are 584 characters in the data.\n",
      "The lexical diversity is 0.422 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    one         17\n",
      "1   step         16\n",
      "2  small         15\n",
      "3   time          7\n",
      "4   weve          5\n",
      "There are 106 tokens in the data.\n",
      "There are 40 unique tokens in the data.\n",
      "There are 460 characters in the data.\n",
      "The lexical diversity is 0.377 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   ooga         37\n",
      "1    boo         12\n",
      "2   find          7\n",
      "3     go          5\n",
      "4  heres          4\n",
      "There are 47 tokens in the data.\n",
      "There are 25 unique tokens in the data.\n",
      "There are 201 characters in the data.\n",
      "The lexical diversity is 0.532 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  come          6\n",
      "1  love          6\n",
      "2   day          4\n",
      "3  well          4\n",
      "4  wait          2\n",
      "There are 91 tokens in the data.\n",
      "There are 71 unique tokens in the data.\n",
      "There are 451 characters in the data.\n",
      "The lexical diversity is 0.780 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0        day          4\n",
      "1        met          3\n",
      "2      woman          3\n",
      "3        san          3\n",
      "4  francisco          3\n",
      "There are 127 tokens in the data.\n",
      "There are 70 unique tokens in the data.\n",
      "There are 652 characters in the data.\n",
      "The lexical diversity is 0.551 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  outrageous         15\n",
      "1          im         12\n",
      "2        rage          7\n",
      "3       gonna          6\n",
      "4         say          3\n",
      "There are 114 tokens in the data.\n",
      "There are 65 unique tokens in the data.\n",
      "There are 590 characters in the data.\n",
      "The lexical diversity is 0.570 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      need          8\n",
      "1     right          8\n",
      "2    loving          5\n",
      "3  paradise          4\n",
      "4      dont          4\n",
      "There are 240 tokens in the data.\n",
      "There are 87 unique tokens in the data.\n",
      "There are 1234 characters in the data.\n",
      "The lexical diversity is 0.362 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0  perfection         17\n",
      "1        love         15\n",
      "2         ive         13\n",
      "3      driven         11\n",
      "4        know          9\n",
      "There are 82 tokens in the data.\n",
      "There are 33 unique tokens in the data.\n",
      "There are 367 characters in the data.\n",
      "The lexical diversity is 0.402 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0      im          8\n",
      "1    pied          8\n",
      "2   piper          8\n",
      "3  follow          6\n",
      "4    babe          5\n",
      "There are 143 tokens in the data.\n",
      "There are 78 unique tokens in the data.\n",
      "There are 631 characters in the data.\n",
      "The lexical diversity is 0.545 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    love         11\n",
      "1    know          8\n",
      "2  pirate          7\n",
      "3    much          5\n",
      "4     sea          5\n",
      "There are 43 tokens in the data.\n",
      "There are 33 unique tokens in the data.\n",
      "There are 186 characters in the data.\n",
      "The lexical diversity is 0.767 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      far          2\n",
      "1     time          2\n",
      "2      see          2\n",
      "3    drift          2\n",
      "4  machine          2\n",
      "There are 149 tokens in the data.\n",
      "There are 50 unique tokens in the data.\n",
      "There are 657 characters in the data.\n",
      "The lexical diversity is 0.336 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0     oh         22\n",
      "1  pride         11\n",
      "2  night          8\n",
      "3   wont          8\n",
      "4   stop          8\n",
      "There are 153 tokens in the data.\n",
      "There are 52 unique tokens in the data.\n",
      "There are 753 characters in the data.\n",
      "The lexical diversity is 0.340 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0        im         18\n",
      "1  prisoner         11\n",
      "2      love         10\n",
      "3       hey          8\n",
      "4       got          4\n",
      "There are 99 tokens in the data.\n",
      "There are 73 unique tokens in the data.\n",
      "There are 476 characters in the data.\n",
      "The lexical diversity is 0.737 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    rain         13\n",
      "1     ooh          3\n",
      "2     see          3\n",
      "3   youre          3\n",
      "4  chorus          3\n",
      "There are 119 tokens in the data.\n",
      "There are 34 unique tokens in the data.\n",
      "There are 539 characters in the data.\n",
      "The lexical diversity is 0.286 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     time         20\n",
      "1     love         17\n",
      "2     real          8\n",
      "3    still          8\n",
      "4  believe          8\n",
      "There are 71 tokens in the data.\n",
      "There are 35 unique tokens in the data.\n",
      "There are 365 characters in the data.\n",
      "The lexical diversity is 0.493 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     find          6\n",
      "1  believe          5\n",
      "2       id          3\n",
      "3      way          3\n",
      "4  knowing          3\n",
      "There are 143 tokens in the data.\n",
      "There are 61 unique tokens in the data.\n",
      "There are 623 characters in the data.\n",
      "The lexical diversity is 0.427 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     red         26\n",
      "1     see          9\n",
      "2    like          5\n",
      "3  around          4\n",
      "4   heart          4\n",
      "There are 101 tokens in the data.\n",
      "There are 25 unique tokens in the data.\n",
      "There are 429 characters in the data.\n",
      "The lexical diversity is 0.248 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    baby         13\n",
      "1    come         10\n",
      "2  rescue          9\n",
      "3      im          9\n",
      "4  lonely          7\n",
      "There are 151 tokens in the data.\n",
      "There are 80 unique tokens in the data.\n",
      "There are 679 characters in the data.\n",
      "The lexical diversity is 0.530 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  feel          8\n",
      "1  fine          8\n",
      "2   hes          7\n",
      "3  beat          6\n",
      "4  like          5\n",
      "There are 130 tokens in the data.\n",
      "There are 46 unique tokens in the data.\n",
      "There are 655 characters in the data.\n",
      "The lexical diversity is 0.354 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    rudy         18\n",
      "1   youre         16\n",
      "2   still          8\n",
      "3  always          8\n",
      "4    mind          8\n",
      "There are 134 tokens in the data.\n",
      "There are 42 unique tokens in the data.\n",
      "There are 706 characters in the data.\n",
      "The lexical diversity is 0.313 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  runaway         22\n",
      "1     cant         12\n",
      "2     love         11\n",
      "3     find         10\n",
      "4    gotta          9\n",
      "There are 152 tokens in the data.\n",
      "There are 82 unique tokens in the data.\n",
      "There are 693 characters in the data.\n",
      "The lexical diversity is 0.539 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  runnin         24\n",
      "1    sail          9\n",
      "2    keep          5\n",
      "3    like          5\n",
      "4    cant          4\n",
      "There are 74 tokens in the data.\n",
      "There are 62 unique tokens in the data.\n",
      "There are 412 characters in the data.\n",
      "The lexical diversity is 0.838 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   eyes          4\n",
      "1   dont          3\n",
      "2  youre          3\n",
      "3     im          2\n",
      "4   long          2\n",
      "There are 195 tokens in the data.\n",
      "There are 63 unique tokens in the data.\n",
      "There are 868 characters in the data.\n",
      "The lexical diversity is 0.323 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   dont         17\n",
      "1   know         13\n",
      "2  cryin         13\n",
      "3  tears         12\n",
      "4  youll         12\n",
      "There are 99 tokens in the data.\n",
      "There are 59 unique tokens in the data.\n",
      "There are 490 characters in the data.\n",
      "The lexical diversity is 0.596 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  blade          7\n",
      "1  think          6\n",
      "2    see          6\n",
      "3    big          6\n",
      "4     im          4\n",
      "There are 114 tokens in the data.\n",
      "There are 50 unique tokens in the data.\n",
      "There are 527 characters in the data.\n",
      "The lexical diversity is 0.439 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  baby         11\n",
      "1  want          8\n",
      "2  dont          7\n",
      "3  tell          5\n",
      "4   say          5\n",
      "There are 136 tokens in the data.\n",
      "There are 99 unique tokens in the data.\n",
      "There are 733 characters in the data.\n",
      "The lexical diversity is 0.728 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  today          4\n",
      "1   send          4\n",
      "2   call          3\n",
      "3    say          3\n",
      "4    way          3\n",
      "There are 64 tokens in the data.\n",
      "There are 45 unique tokens in the data.\n",
      "There are 335 characters in the data.\n",
      "The lexical diversity is 0.703 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      yeah          5\n",
      "1  princess          4\n",
      "2    prince          4\n",
      "3     meant          3\n",
      "4     could          3\n",
      "There are 139 tokens in the data.\n",
      "There are 55 unique tokens in the data.\n",
      "There are 640 characters in the data.\n",
      "The lexical diversity is 0.396 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  things         18\n",
      "1    come         16\n",
      "2   shape         15\n",
      "3     two          8\n",
      "4     one          5\n",
      "There are 151 tokens in the data.\n",
      "There are 91 unique tokens in the data.\n",
      "There are 731 characters in the data.\n",
      "The lexical diversity is 0.603 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   shes          8\n",
      "1  loves          7\n",
      "2   hear          6\n",
      "3    got          6\n",
      "4  music          5\n",
      "There are 225 tokens in the data.\n",
      "There are 128 unique tokens in the data.\n",
      "There are 1110 characters in the data.\n",
      "The lexical diversity is 0.569 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  shoppin         25\n",
      "1       im         14\n",
      "2    gonna          8\n",
      "3      buy          4\n",
      "4    blues          4\n",
      "There are 105 tokens in the data.\n",
      "There are 84 unique tokens in the data.\n",
      "There are 503 characters in the data.\n",
      "The lexical diversity is 0.800 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    love          4\n",
      "1    back          4\n",
      "2    time          3\n",
      "3   night          2\n",
      "4  lonely          2\n",
      "There are 86 tokens in the data.\n",
      "There are 39 unique tokens in the data.\n",
      "There are 429 characters in the data.\n",
      "The lexical diversity is 0.453 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     sing          8\n",
      "1    youll          8\n",
      "2      get          4\n",
      "3  swallow          4\n",
      "4     said          4\n",
      "There are 100 tokens in the data.\n",
      "There are 45 unique tokens in the data.\n",
      "There are 475 characters in the data.\n",
      "The lexical diversity is 0.450 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   sound          6\n",
      "1  sirens          6\n",
      "2     sky          3\n",
      "3   leave          3\n",
      "4  behind          3\n",
      "There are 182 tokens in the data.\n",
      "There are 116 unique tokens in the data.\n",
      "There are 959 characters in the data.\n",
      "The lexical diversity is 0.637 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    mercy         16\n",
      "1  sisters          8\n",
      "2    grace          6\n",
      "3    place          6\n",
      "4    shows          5\n",
      "There are 94 tokens in the data.\n",
      "There are 48 unique tokens in the data.\n",
      "There are 443 characters in the data.\n",
      "The lexical diversity is 0.511 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0   sittin          8\n",
      "1     dock          7\n",
      "2      bay          7\n",
      "3     roll          5\n",
      "4  watchin          4\n",
      "There are 147 tokens in the data.\n",
      "There are 53 unique tokens in the data.\n",
      "There are 647 characters in the data.\n",
      "The lexical diversity is 0.361 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  skin         15\n",
      "1  deep         15\n",
      "2  bone         13\n",
      "3    im          9\n",
      "4    go          8\n",
      "There are 71 tokens in the data.\n",
      "There are 41 unique tokens in the data.\n",
      "There are 360 characters in the data.\n",
      "The lexical diversity is 0.577 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  children          5\n",
      "1     still          5\n",
      "2      time          4\n",
      "3     close          3\n",
      "4    theres          3\n",
      "There are 83 tokens in the data.\n",
      "There are 55 unique tokens in the data.\n",
      "There are 421 characters in the data.\n",
      "The lexical diversity is 0.663 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  singing          6\n",
      "1     song          6\n",
      "2     life          4\n",
      "3      ive          3\n",
      "4    alone          3\n",
      "There are 109 tokens in the data.\n",
      "There are 43 unique tokens in the data.\n",
      "There are 504 characters in the data.\n",
      "The lexical diversity is 0.394 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   youre         11\n",
      "1    gone          8\n",
      "2     try          8\n",
      "3    love          5\n",
      "4  though          5\n",
      "There are 146 tokens in the data.\n",
      "There are 91 unique tokens in the data.\n",
      "There are 731 characters in the data.\n",
      "The lexical diversity is 0.623 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    long         10\n",
      "1  spring          9\n",
      "2    time          7\n",
      "3    said          4\n",
      "4      ah          4\n",
      "There are 159 tokens in the data.\n",
      "There are 106 unique tokens in the data.\n",
      "There are 769 characters in the data.\n",
      "The lexical diversity is 0.667 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     come          9\n",
      "1    never          6\n",
      "2  singing          5\n",
      "3       go          4\n",
      "4     make          4\n",
      "There are 128 tokens in the data.\n",
      "There are 75 unique tokens in the data.\n",
      "There are 640 characters in the data.\n",
      "The lexical diversity is 0.586 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  startin         12\n",
      "1     back          6\n",
      "2      let          4\n",
      "3     time          4\n",
      "4   around          4\n",
      "There are 121 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 606 characters in the data.\n",
      "The lexical diversity is 0.471 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   love         15\n",
      "1  still         11\n",
      "2   baby          6\n",
      "3     oh          5\n",
      "4  cried          4\n",
      "There are 97 tokens in the data.\n",
      "There are 46 unique tokens in the data.\n",
      "There are 438 characters in the data.\n",
      "The lexical diversity is 0.474 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   know         11\n",
      "1  still          8\n",
      "2   love          8\n",
      "3  lying          6\n",
      "4    ive          5\n",
      "There are 173 tokens in the data.\n",
      "There are 62 unique tokens in the data.\n",
      "There are 810 characters in the data.\n",
      "The lexical diversity is 0.358 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  enough         19\n",
      "1  strong         15\n",
      "2      im         13\n",
      "3     say          7\n",
      "4    know          7\n",
      "There are 79 tokens in the data.\n",
      "There are 40 unique tokens in the data.\n",
      "There are 359 characters in the data.\n",
      "The lexical diversity is 0.506 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  sunny         13\n",
      "1   love          8\n",
      "2  thank          6\n",
      "3   true          4\n",
      "4   gave          4\n",
      "There are 92 tokens in the data.\n",
      "There are 45 unique tokens in the data.\n",
      "There are 379 characters in the data.\n",
      "The lexical diversity is 0.489 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  baby         11\n",
      "1    ah          7\n",
      "2    oh          6\n",
      "3  love          5\n",
      "4   ooh          5\n",
      "There are 209 tokens in the data.\n",
      "There are 71 unique tokens in the data.\n",
      "There are 990 characters in the data.\n",
      "The lexical diversity is 0.340 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   take         30\n",
      "1   boys         30\n",
      "2   well         10\n",
      "3  might          9\n",
      "4   wise          9\n",
      "There are 193 tokens in the data.\n",
      "There are 77 unique tokens in the data.\n",
      "There are 872 characters in the data.\n",
      "The lexical diversity is 0.399 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   gotta         11\n",
      "1   heart         10\n",
      "2  better          9\n",
      "3    take          8\n",
      "4    like          8\n",
      "There are 53 tokens in the data.\n",
      "There are 40 unique tokens in the data.\n",
      "There are 246 characters in the data.\n",
      "The lexical diversity is 0.755 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     love          5\n",
      "1     dont          3\n",
      "2  forever          3\n",
      "3      ive          2\n",
      "4     make          2\n",
      "There are 220 tokens in the data.\n",
      "There are 55 unique tokens in the data.\n",
      "There are 979 characters in the data.\n",
      "The lexical diversity is 0.250 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    home         38\n",
      "1    take         36\n",
      "2  heaven         11\n",
      "3   wanna          7\n",
      "4    baby          7\n",
      "There are 234 tokens in the data.\n",
      "There are 69 unique tokens in the data.\n",
      "There are 1108 characters in the data.\n",
      "The lexical diversity is 0.295 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   back         26\n",
      "1   baby         25\n",
      "2  heart         22\n",
      "3  takin         16\n",
      "4     im         15\n",
      "There are 177 tokens in the data.\n",
      "There are 58 unique tokens in the data.\n",
      "There are 794 characters in the data.\n",
      "The lexical diversity is 0.328 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   taxi         30\n",
      "1   ride         18\n",
      "2     im         17\n",
      "3  gonna         17\n",
      "4  night         13\n",
      "There are 74 tokens in the data.\n",
      "There are 28 unique tokens in the data.\n",
      "There are 351 characters in the data.\n",
      "The lexical diversity is 0.378 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      say         12\n",
      "1    bells         12\n",
      "2     give          4\n",
      "3      sad          4\n",
      "4  rhymney          4\n",
      "There are 101 tokens in the data.\n",
      "There are 60 unique tokens in the data.\n",
      "There are 463 characters in the data.\n",
      "The lexical diversity is 0.594 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0      oh          4\n",
      "1     get          4\n",
      "2  bigger          4\n",
      "3    come          4\n",
      "4  harder          4\n",
      "There are 130 tokens in the data.\n",
      "There are 76 unique tokens in the data.\n",
      "There are 643 characters in the data.\n",
      "The lexical diversity is 0.585 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     love         14\n",
      "1     book         12\n",
      "2  writing          6\n",
      "3    heyho          3\n",
      "4   broken          3\n",
      "There are 76 tokens in the data.\n",
      "There are 45 unique tokens in the data.\n",
      "There are 331 characters in the data.\n",
      "The lexical diversity is 0.592 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  love          6\n",
      "1  wont          4\n",
      "2   let          4\n",
      "3    go          4\n",
      "4   ill          4\n",
      "There are 105 tokens in the data.\n",
      "There are 82 unique tokens in the data.\n",
      "There are 530 characters in the data.\n",
      "The lexical diversity is 0.781 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  heard          3\n",
      "1   news          3\n",
      "2   knew          2\n",
      "3   well          2\n",
      "4    one          2\n",
      "There are 71 tokens in the data.\n",
      "There are 47 unique tokens in the data.\n",
      "There are 302 characters in the data.\n",
      "The lexical diversity is 0.662 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     dont          4\n",
      "1    think          3\n",
      "2      see          3\n",
      "3  anymore          3\n",
      "4     know          3\n",
      "There are 83 tokens in the data.\n",
      "There are 40 unique tokens in the data.\n",
      "There are 390 characters in the data.\n",
      "The lexical diversity is 0.482 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  passes          6\n",
      "1    goes          5\n",
      "2    tall          3\n",
      "3     tan          3\n",
      "4   young          3\n",
      "There are 95 tokens in the data.\n",
      "There are 72 unique tokens in the data.\n",
      "There are 453 characters in the data.\n",
      "The lexical diversity is 0.758 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      ever          7\n",
      "1       ive          3\n",
      "2  greatest          3\n",
      "3      song          3\n",
      "4     heard          3\n",
      "There are 201 tokens in the data.\n",
      "There are 58 unique tokens in the data.\n",
      "There are 1018 characters in the data.\n",
      "The lexical diversity is 0.289 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  greatest         19\n",
      "1     thing         13\n",
      "2     youre         12\n",
      "3       see          8\n",
      "4       ill          8\n",
      "There are 89 tokens in the data.\n",
      "There are 48 unique tokens in the data.\n",
      "There are 422 characters in the data.\n",
      "The lexical diversity is 0.539 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    love         11\n",
      "1  gunman          9\n",
      "2   mercy          5\n",
      "3    time          3\n",
      "4  sights          3\n",
      "There are 84 tokens in the data.\n",
      "There are 40 unique tokens in the data.\n",
      "There are 383 characters in the data.\n",
      "The lexical diversity is 0.476 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      long          8\n",
      "1      road          5\n",
      "2   winding          4\n",
      "3      door          4\n",
      "4  standing          4\n",
      "There are 103 tokens in the data.\n",
      "There are 51 unique tokens in the data.\n",
      "There are 451 characters in the data.\n",
      "The lexical diversity is 0.495 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  gonna          7\n",
      "1    hes          6\n",
      "2    man          4\n",
      "3    ill          4\n",
      "4   grab          4\n",
      "There are 85 tokens in the data.\n",
      "There are 74 unique tokens in the data.\n",
      "There are 419 characters in the data.\n",
      "The lexical diversity is 0.871 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     man          4\n",
      "1   night          2\n",
      "2     got          2\n",
      "3    away          2\n",
      "4  dreams          2\n",
      "There are 84 tokens in the data.\n",
      "There are 66 unique tokens in the data.\n",
      "There are 433 characters in the data.\n",
      "The lexical diversity is 0.786 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  chorus          4\n",
      "1    good          4\n",
      "2    come          4\n",
      "3    back          4\n",
      "4  musics          3\n",
      "There are 224 tokens in the data.\n",
      "There are 72 unique tokens in the data.\n",
      "There are 1068 characters in the data.\n",
      "The lexical diversity is 0.321 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  doodoo         28\n",
      "1    name         10\n",
      "2    game         10\n",
      "3   whats          9\n",
      "4    know          7\n",
      "There are 145 tokens in the data.\n",
      "There are 72 unique tokens in the data.\n",
      "There are 701 characters in the data.\n",
      "The lexical diversity is 0.497 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    power         13\n",
      "1    every          6\n",
      "2  believe          5\n",
      "3    holds          4\n",
      "4     hand          4\n",
      "There are 66 tokens in the data.\n",
      "There are 33 unique tokens in the data.\n",
      "There are 313 characters in the data.\n",
      "The lexical diversity is 0.500 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     show         14\n",
      "1      ill          4\n",
      "2    young          4\n",
      "3     many          4\n",
      "4  reasons          4\n",
      "There are 122 tokens in the data.\n",
      "There are 63 unique tokens in the data.\n",
      "There are 570 characters in the data.\n",
      "The lexical diversity is 0.516 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       know          7\n",
      "1       love          6\n",
      "2  sometimes          6\n",
      "3       give          5\n",
      "4       take          5\n",
      "There are 81 tokens in the data.\n",
      "There are 54 unique tokens in the data.\n",
      "There are 382 characters in the data.\n",
      "The lexical diversity is 0.667 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0       days          8\n",
      "1        ive          5\n",
      "2         oh          5\n",
      "3       well          4\n",
      "4  forgotten          3\n",
      "There are 89 tokens in the data.\n",
      "There are 31 unique tokens in the data.\n",
      "There are 391 characters in the data.\n",
      "The lexical diversity is 0.348 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  thats         14\n",
      "1   kiss         12\n",
      "2   know          7\n",
      "3     oh          6\n",
      "4   want          5\n",
      "There are 113 tokens in the data.\n",
      "There are 34 unique tokens in the data.\n",
      "There are 557 characters in the data.\n",
      "The lexical diversity is 0.301 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     aint         10\n",
      "1    gonna         10\n",
      "2   always          7\n",
      "3    youre          6\n",
      "4  without          6\n",
      "There are 69 tokens in the data.\n",
      "There are 55 unique tokens in the data.\n",
      "There are 395 characters in the data.\n",
      "The lexical diversity is 0.797 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    right          4\n",
      "1  thought          3\n",
      "2   loving          3\n",
      "3      way          2\n",
      "4       im          2\n",
      "There are 125 tokens in the data.\n",
      "There are 98 unique tokens in the data.\n",
      "There are 662 characters in the data.\n",
      "The lexical diversity is 0.784 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      come          5\n",
      "1     times          5\n",
      "2  achangin          5\n",
      "3      dont          4\n",
      "4     later          4\n",
      "There are 70 tokens in the data.\n",
      "There are 41 unique tokens in the data.\n",
      "There are 318 characters in the data.\n",
      "The lexical diversity is 0.586 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     long          7\n",
      "1    never          5\n",
      "2     love          4\n",
      "3  twelfth          4\n",
      "4      ill          3\n",
      "There are 69 tokens in the data.\n",
      "There are 30 unique tokens in the data.\n",
      "There are 273 characters in the data.\n",
      "The lexical diversity is 0.435 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   way          8\n",
      "1  love          6\n",
      "2  meet          2\n",
      "3   boy          2\n",
      "4  like          2\n",
      "There are 141 tokens in the data.\n",
      "There are 98 unique tokens in the data.\n",
      "There are 740 characters in the data.\n",
      "The lexical diversity is 0.695 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0     takes          8\n",
      "1    winner          6\n",
      "2     small          5\n",
      "3  standing          3\n",
      "4      feel          3\n",
      "There are 110 tokens in the data.\n",
      "There are 80 unique tokens in the data.\n",
      "There are 548 characters in the data.\n",
      "The lexical diversity is 0.727 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0        youve          5\n",
      "1         gone          5\n",
      "2         away          5\n",
      "3          day          4\n",
      "4  godforsaken          4\n",
      "There are 114 tokens in the data.\n",
      "There are 69 unique tokens in the data.\n",
      "There are 571 characters in the data.\n",
      "The lexical diversity is 0.605 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    song          6\n",
      "1  lonely          5\n",
      "2    hear          4\n",
      "3    dont          4\n",
      "4   gonna          4\n",
      "There are 111 tokens in the data.\n",
      "There are 64 unique tokens in the data.\n",
      "There are 569 characters in the data.\n",
      "The lexical diversity is 0.577 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "          token  frequency\n",
      "0          good          4\n",
      "1          feel          4\n",
      "2          knew          3\n",
      "3        coming          3\n",
      "4  thunderstorm          3\n",
      "There are 77 tokens in the data.\n",
      "There are 43 unique tokens in the data.\n",
      "There are 353 characters in the data.\n",
      "The lexical diversity is 0.558 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    time          8\n",
      "1  people          5\n",
      "2      go          5\n",
      "3      oh          4\n",
      "4    good          4\n",
      "There are 70 tokens in the data.\n",
      "There are 46 unique tokens in the data.\n",
      "There are 372 characters in the data.\n",
      "The lexical diversity is 0.657 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    throw          6\n",
      "1  tonight          4\n",
      "2      ill          4\n",
      "3  staying          4\n",
      "4    cause          3\n",
      "There are 83 tokens in the data.\n",
      "There are 36 unique tokens in the data.\n",
      "There are 360 characters in the data.\n",
      "The lexical diversity is 0.434 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    know          6\n",
      "1   touch          4\n",
      "2      go          4\n",
      "3    weak          4\n",
      "4  strong          4\n",
      "There are 128 tokens in the data.\n",
      "There are 81 unique tokens in the data.\n",
      "There are 604 characters in the data.\n",
      "The lexical diversity is 0.633 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    gotta         10\n",
      "1      get         10\n",
      "2    train          6\n",
      "3     time          4\n",
      "4  thought          4\n",
      "There are 59 tokens in the data.\n",
      "There are 54 unique tokens in the data.\n",
      "There are 293 characters in the data.\n",
      "The lexical diversity is 0.915 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     wake          2\n",
      "1    night          2\n",
      "2  pretend          2\n",
      "3       go          2\n",
      "4      day          2\n",
      "There are 67 tokens in the data.\n",
      "There are 49 unique tokens in the data.\n",
      "There are 293 characters in the data.\n",
      "The lexical diversity is 0.731 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  youre          3\n",
      "1     im          3\n",
      "2   stay          3\n",
      "3   time          3\n",
      "4     go          3\n",
      "There are 173 tokens in the data.\n",
      "There are 85 unique tokens in the data.\n",
      "There are 903 characters in the data.\n",
      "The lexical diversity is 0.491 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  walking         21\n",
      "1  memphis         17\n",
      "2     feet         10\n",
      "3     feel         10\n",
      "4      got          5\n",
      "There are 92 tokens in the data.\n",
      "There are 40 unique tokens in the data.\n",
      "There are 432 characters in the data.\n",
      "The lexical diversity is 0.435 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      walk         16\n",
      "1      take          4\n",
      "2      hand          4\n",
      "3     count          4\n",
      "4  troubles          4\n",
      "There are 140 tokens in the data.\n",
      "There are 60 unique tokens in the data.\n",
      "There are 707 characters in the data.\n",
      "The lexical diversity is 0.429 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       see          9\n",
      "1     wanna          8\n",
      "2     walls          7\n",
      "3  crashing          7\n",
      "4     cause          6\n",
      "There are 172 tokens in the data.\n",
      "There are 76 unique tokens in the data.\n",
      "There are 921 characters in the data.\n",
      "The lexical diversity is 0.442 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       war          8\n",
      "1     paint          8\n",
      "2      soft          8\n",
      "3  feathers          8\n",
      "4      love          6\n",
      "There are 142 tokens in the data.\n",
      "There are 66 unique tokens in the data.\n",
      "There are 674 characters in the data.\n",
      "The lexical diversity is 0.465 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   good         15\n",
      "1  wasnt          8\n",
      "2   know          6\n",
      "3  party          5\n",
      "4   baby          5\n",
      "There are 106 tokens in the data.\n",
      "There are 43 unique tokens in the data.\n",
      "There are 611 characters in the data.\n",
      "The lexical diversity is 0.406 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  waterloo         20\n",
      "1      woah          8\n",
      "2      ever          4\n",
      "3   knowing          4\n",
      "4      fate          4\n",
      "There are 77 tokens in the data.\n",
      "There are 42 unique tokens in the data.\n",
      "There are 363 characters in the data.\n",
      "The lexical diversity is 0.545 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     fly          6\n",
      "1    home          6\n",
      "2    well          4\n",
      "3  sooner          4\n",
      "4   later          4\n",
      "There are 91 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 461 characters in the data.\n",
      "The lexical diversity is 0.626 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   alone          8\n",
      "1   sleep          7\n",
      "2   youre          3\n",
      "3  sooner          3\n",
      "4   later          3\n",
      "There are 89 tokens in the data.\n",
      "There are 58 unique tokens in the data.\n",
      "There are 479 characters in the data.\n",
      "The lexical diversity is 0.652 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0     little          8\n",
      "1    welcome          5\n",
      "2  burlesque          5\n",
      "3       show          4\n",
      "4       less          2\n",
      "There are 115 tokens in the data.\n",
      "There are 54 unique tokens in the data.\n",
      "There are 488 characters in the data.\n",
      "The lexical diversity is 0.470 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  gonna         15\n",
      "1   make         15\n",
      "2   know          9\n",
      "3    got          6\n",
      "4    may          5\n",
      "There are 105 tokens in the data.\n",
      "There are 62 unique tokens in the data.\n",
      "There are 568 characters in the data.\n",
      "The lexical diversity is 0.590 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       way          6\n",
      "1     loves          5\n",
      "2    dreams          5\n",
      "3    change          5\n",
      "4  tomorrow          5\n",
      "There are 24 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 127 characters in the data.\n",
      "The lexical diversity is 0.625 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  whatll          7\n",
      "1     far          2\n",
      "2    away          2\n",
      "3    blue          2\n",
      "4      im          1\n",
      "There are 136 tokens in the data.\n",
      "There are 68 unique tokens in the data.\n",
      "There are 661 characters in the data.\n",
      "The lexical diversity is 0.500 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    love         10\n",
      "1   calls         10\n",
      "2    name         10\n",
      "3  theres          4\n",
      "4     way          4\n",
      "There are 157 tokens in the data.\n",
      "There are 75 unique tokens in the data.\n",
      "There are 826 characters in the data.\n",
      "The lexical diversity is 0.478 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0      shame         13\n",
      "1     lovers          8\n",
      "2     become          8\n",
      "3  strangers          8\n",
      "4       dont          7\n",
      "There are 117 tokens in the data.\n",
      "There are 61 unique tokens in the data.\n",
      "There are 549 characters in the data.\n",
      "The lexical diversity is 0.521 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    love         11\n",
      "1    gone          9\n",
      "2   gotta          7\n",
      "3  theres          5\n",
      "4  strong          4\n",
      "There are 107 tokens in the data.\n",
      "There are 76 unique tokens in the data.\n",
      "There are 483 characters in the data.\n",
      "The lexical diversity is 0.710 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  moneys          8\n",
      "1    gone          8\n",
      "2   still          4\n",
      "3      oh          4\n",
      "4    want          3\n",
      "There are 77 tokens in the data.\n",
      "There are 47 unique tokens in the data.\n",
      "There are 338 characters in the data.\n",
      "The lexical diversity is 0.610 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   know          7\n",
      "1   find          6\n",
      "2  youre          6\n",
      "3   goin          5\n",
      "4    let          5\n",
      "There are 171 tokens in the data.\n",
      "There are 56 unique tokens in the data.\n",
      "There are 777 characters in the data.\n",
      "The lexical diversity is 0.327 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    walk         24\n",
      "1    away         18\n",
      "2    wont         11\n",
      "3  crying          6\n",
      "4   cause          5\n",
      "There are 57 tokens in the data.\n",
      "There are 42 unique tokens in the data.\n",
      "There are 235 characters in the data.\n",
      "The lexical diversity is 0.737 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0     go          4\n",
      "1   dont          4\n",
      "2   know          4\n",
      "3  right          3\n",
      "4  youre          2\n",
      "There are 144 tokens in the data.\n",
      "There are 81 unique tokens in the data.\n",
      "There are 710 characters in the data.\n",
      "The lexical diversity is 0.562 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    gonna         12\n",
      "1  believe          9\n",
      "2       oh          5\n",
      "3     love          4\n",
      "4     hope          4\n",
      "There are 44 tokens in the data.\n",
      "There are 24 unique tokens in the data.\n",
      "There are 184 characters in the data.\n",
      "The lexical diversity is 0.545 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  born          3\n",
      "1  tell          3\n",
      "2    im          3\n",
      "3  hope          2\n",
      "4   try          2\n",
      "There are 58 tokens in the data.\n",
      "There are 35 unique tokens in the data.\n",
      "There are 306 characters in the data.\n",
      "The lexical diversity is 0.603 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      love         10\n",
      "1  tomorrow          6\n",
      "2     still          5\n",
      "3   tonight          3\n",
      "4      tell          2\n",
      "There are 129 tokens in the data.\n",
      "There are 66 unique tokens in the data.\n",
      "There are 614 characters in the data.\n",
      "The lexical diversity is 0.512 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   wait         11\n",
      "1   know          6\n",
      "2  feels          6\n",
      "3   love          5\n",
      "4  youre          4\n",
      "There are 98 tokens in the data.\n",
      "There are 68 unique tokens in the data.\n",
      "There are 492 characters in the data.\n",
      "The lexical diversity is 0.694 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    heart          5\n",
      "1    cause          4\n",
      "2  without          3\n",
      "3    alone          3\n",
      "4   broken          3\n",
      "There are 216 tokens in the data.\n",
      "There are 59 unique tokens in the data.\n",
      "There are 1097 characters in the data.\n",
      "The lexical diversity is 0.273 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   world         34\n",
      "1  womans         32\n",
      "2    tell         12\n",
      "3   truth         12\n",
      "4      im         10\n",
      "There are 108 tokens in the data.\n",
      "There are 68 unique tokens in the data.\n",
      "There are 547 characters in the data.\n",
      "The lexical diversity is 0.630 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     girl         10\n",
      "1  working          8\n",
      "2     shes          6\n",
      "3    livin          3\n",
      "4     mans          3\n",
      "There are 135 tokens in the data.\n",
      "There are 79 unique tokens in the data.\n",
      "There are 592 characters in the data.\n",
      "The lexical diversity is 0.585 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    kids         12\n",
      "1     say          5\n",
      "2  mother          5\n",
      "3     ill          4\n",
      "4      im          4\n",
      "There are 84 tokens in the data.\n",
      "There are 39 unique tokens in the data.\n",
      "There are 414 characters in the data.\n",
      "The lexical diversity is 0.464 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  believe          9\n",
      "1     dont          7\n",
      "2     love          6\n",
      "3     stay          4\n",
      "4     left          4\n",
      "There are 131 tokens in the data.\n",
      "There are 60 unique tokens in the data.\n",
      "There are 582 characters in the data.\n",
      "The lexical diversity is 0.458 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  havent          7\n",
      "1    seen          7\n",
      "2    last          7\n",
      "3      im          7\n",
      "4     ive          6\n",
      "There are 58 tokens in the data.\n",
      "There are 38 unique tokens in the data.\n",
      "There are 268 characters in the data.\n",
      "The lexical diversity is 0.655 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   know          8\n",
      "1   baby          4\n",
      "2   well          3\n",
      "3   love          3\n",
      "4  youre          2\n",
      "There are 94 tokens in the data.\n",
      "There are 54 unique tokens in the data.\n",
      "There are 468 characters in the data.\n",
      "The lexical diversity is 0.574 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     say          5\n",
      "1    make          5\n",
      "2   youre          5\n",
      "3   young          5\n",
      "4  pretty          5\n",
      "There are 86 tokens in the data.\n",
      "There are 58 unique tokens in the data.\n",
      "There are 437 characters in the data.\n",
      "The lexical diversity is 0.674 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  tomorrow          9\n",
      "1       let          8\n",
      "2   tonight          3\n",
      "3       one          3\n",
      "4     night          3\n",
      "There are 73 tokens in the data.\n",
      "There are 35 unique tokens in the data.\n",
      "There are 328 characters in the data.\n",
      "The lexical diversity is 0.479 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0      every          6\n",
      "1       ever          6\n",
      "2  sometimes          4\n",
      "3        ooh          4\n",
      "4       take          4\n",
      "There are 80 tokens in the data.\n",
      "There are 45 unique tokens in the data.\n",
      "There are 344 characters in the data.\n",
      "The lexical diversity is 0.562 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0     im          6\n",
      "1   came          6\n",
      "2   made          5\n",
      "3  happy          5\n",
      "4   life          5\n",
      "There are 128 tokens in the data.\n",
      "There are 35 unique tokens in the data.\n",
      "There are 551 characters in the data.\n",
      "The lexical diversity is 0.273 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    hold         24\n",
      "1  really         16\n",
      "2     got         16\n",
      "3   youve          8\n",
      "4    want          8\n",
      "There are 169 tokens in the data.\n",
      "There are 52 unique tokens in the data.\n",
      "There are 843 characters in the data.\n",
      "The lexical diversity is 0.308 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     know         28\n",
      "1  wouldnt         26\n",
      "2     love         22\n",
      "3  knocked          5\n",
      "4     door          5\n",
      "There are 203 tokens in the data.\n",
      "There are 80 unique tokens in the data.\n",
      "There are 881 characters in the data.\n",
      "The lexical diversity is 0.394 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   got         24\n",
      "1  work         16\n",
      "2    88         13\n",
      "3  days         13\n",
      "4   ive         11\n",
      "There are 64 tokens in the data.\n",
      "There are 40 unique tokens in the data.\n",
      "There are 296 characters in the data.\n",
      "The lexical diversity is 0.625 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  really          6\n",
      "1    want          4\n",
      "2    baby          4\n",
      "3   thing          4\n",
      "4     boy          3\n",
      "There are 117 tokens in the data.\n",
      "There are 56 unique tokens in the data.\n",
      "There are 567 characters in the data.\n",
      "The lexical diversity is 0.479 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  pressure         13\n",
      "1      tell         10\n",
      "2      like          8\n",
      "3       boy          8\n",
      "4      love          5\n",
      "There are 75 tokens in the data.\n",
      "There are 34 unique tokens in the data.\n",
      "There are 332 characters in the data.\n",
      "The lexical diversity is 0.453 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     baby         18\n",
      "1  forgive         12\n",
      "2     wont          3\n",
      "3     give          3\n",
      "4   chance          3\n",
      "There are 172 tokens in the data.\n",
      "There are 32 unique tokens in the data.\n",
      "There are 704 characters in the data.\n",
      "The lexical diversity is 0.186 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  party         19\n",
      "1     go         15\n",
      "2     ok         14\n",
      "3  beach         13\n",
      "4   lets         10\n",
      "There are 128 tokens in the data.\n",
      "There are 59 unique tokens in the data.\n",
      "There are 634 characters in the data.\n",
      "The lexical diversity is 0.461 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  anyway          7\n",
      "1   music          6\n",
      "2    yeah          6\n",
      "3      im          6\n",
      "4   right          6\n",
      "There are 174 tokens in the data.\n",
      "There are 83 unique tokens in the data.\n",
      "There are 853 characters in the data.\n",
      "The lexical diversity is 0.477 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   never         29\n",
      "1    mine         13\n",
      "2  theres          5\n",
      "3   every          5\n",
      "4   cause          5\n",
      "There are 165 tokens in the data.\n",
      "There are 51 unique tokens in the data.\n",
      "There are 780 characters in the data.\n",
      "The lexical diversity is 0.309 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  reading         19\n",
      "1    lines         19\n",
      "2     like         15\n",
      "3     baby         13\n",
      "4       im          8\n",
      "There are 179 tokens in the data.\n",
      "There are 75 unique tokens in the data.\n",
      "There are 849 characters in the data.\n",
      "The lexical diversity is 0.419 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    wont          8\n",
      "1    dont          7\n",
      "2  really          7\n",
      "3    make          6\n",
      "4    city          5\n",
      "There are 8 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 56 characters in the data.\n",
      "The lexical diversity is 1.000 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      good          1\n",
      "1   evening          1\n",
      "2    ladies          1\n",
      "3   captain          1\n",
      "4  speaking          1\n",
      "There are 89 tokens in the data.\n",
      "There are 53 unique tokens in the data.\n",
      "There are 428 characters in the data.\n",
      "The lexical diversity is 0.596 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  youre         10\n",
      "1   blow          6\n",
      "2   mind          6\n",
      "3   baby          4\n",
      "4    ill          4\n",
      "There are 62 tokens in the data.\n",
      "There are 43 unique tokens in the data.\n",
      "There are 321 characters in the data.\n",
      "The lexical diversity is 0.694 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    break          3\n",
      "1  suckers          3\n",
      "2   nobody          2\n",
      "3    knows          2\n",
      "4    whats          2\n",
      "There are 275 tokens in the data.\n",
      "There are 132 unique tokens in the data.\n",
      "There are 1371 characters in the data.\n",
      "The lexical diversity is 0.480 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      love         13\n",
      "1   looking         11\n",
      "2  moneyman         11\n",
      "3       win         10\n",
      "4      good          8\n",
      "There are 167 tokens in the data.\n",
      "There are 63 unique tokens in the data.\n",
      "There are 816 characters in the data.\n",
      "The lexical diversity is 0.377 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0    wasting         12\n",
      "1       time         12\n",
      "2        new         11\n",
      "3       like         11\n",
      "4  favourite         10\n",
      "There are 136 tokens in the data.\n",
      "There are 97 unique tokens in the data.\n",
      "There are 681 characters in the data.\n",
      "The lexical diversity is 0.713 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   whos          4\n",
      "1   hold          4\n",
      "2  youve          4\n",
      "3  gotta          4\n",
      "4  never          4\n",
      "There are 128 tokens in the data.\n",
      "There are 47 unique tokens in the data.\n",
      "There are 626 characters in the data.\n",
      "The lexical diversity is 0.367 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "        token  frequency\n",
      "0        call          6\n",
      "1  girlfriend          6\n",
      "2        give          6\n",
      "3        tell          6\n",
      "4        time          5\n",
      "There are 350 tokens in the data.\n",
      "There are 61 unique tokens in the data.\n",
      "There are 1532 characters in the data.\n",
      "The lexical diversity is 0.174 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  deng         32\n",
      "1  digi         32\n",
      "2  bomb         16\n",
      "3    di         16\n",
      "4    gi         16\n",
      "There are 149 tokens in the data.\n",
      "There are 84 unique tokens in the data.\n",
      "There are 716 characters in the data.\n",
      "The lexical diversity is 0.564 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   girl          9\n",
      "1   dont          6\n",
      "2  crash          6\n",
      "3   burn          6\n",
      "4   mind          5\n",
      "There are 281 tokens in the data.\n",
      "There are 89 unique tokens in the data.\n",
      "There are 1624 characters in the data.\n",
      "The lexical diversity is 0.317 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "         token  frequency\n",
      "0     somebody         16\n",
      "1        alert         16\n",
      "2  authorities         16\n",
      "3          got         16\n",
      "4     criminal         16\n",
      "There are 155 tokens in the data.\n",
      "There are 74 unique tokens in the data.\n",
      "There are 724 characters in the data.\n",
      "The lexical diversity is 0.477 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    get         10\n",
      "1  never          7\n",
      "2   told          7\n",
      "3  light          6\n",
      "4   love          6\n",
      "There are 168 tokens in the data.\n",
      "There are 160 unique tokens in the data.\n",
      "There are 1027 characters in the data.\n",
      "The lexical diversity is 0.952 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0  konichiwa          2\n",
      "1    records          2\n",
      "2        get          2\n",
      "3     listen          2\n",
      "4       turn          2\n",
      "There are 185 tokens in the data.\n",
      "There are 62 unique tokens in the data.\n",
      "There are 865 characters in the data.\n",
      "The lexical diversity is 0.335 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    wow         16\n",
      "1   like         12\n",
      "2  thing          8\n",
      "3  queen          8\n",
      "4    jaw          7\n",
      "There are 185 tokens in the data.\n",
      "There are 62 unique tokens in the data.\n",
      "There are 865 characters in the data.\n",
      "The lexical diversity is 0.335 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    wow         16\n",
      "1   like         12\n",
      "2  thing          8\n",
      "3  queen          8\n",
      "4    jaw          7\n",
      "There are 136 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 596 characters in the data.\n",
      "The lexical diversity is 0.419 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0       im         19\n",
      "1       oh         11\n",
      "2     keep          9\n",
      "3  dancing          9\n",
      "4    youre          5\n",
      "There are 136 tokens in the data.\n",
      "There are 58 unique tokens in the data.\n",
      "There are 607 characters in the data.\n",
      "The lexical diversity is 0.426 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0       im         19\n",
      "1     keep          9\n",
      "2  dancing          9\n",
      "3      ohh          7\n",
      "4    youre          5\n",
      "There are 81 tokens in the data.\n",
      "There are 29 unique tokens in the data.\n",
      "There are 375 characters in the data.\n",
      "The lexical diversity is 0.358 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   lets          9\n",
      "1   dont          6\n",
      "2   know          5\n",
      "3   wait          4\n",
      "4  hurts          4\n",
      "There are 174 tokens in the data.\n",
      "There are 40 unique tokens in the data.\n",
      "There are 1063 characters in the data.\n",
      "The lexical diversity is 0.230 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0   killing         68\n",
      "1  drinking         16\n",
      "2      dont          9\n",
      "3   fucking          9\n",
      "4      tell          9\n",
      "There are 174 tokens in the data.\n",
      "There are 40 unique tokens in the data.\n",
      "There are 1063 characters in the data.\n",
      "The lexical diversity is 0.230 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0   killing         68\n",
      "1  drinking         16\n",
      "2      dont          9\n",
      "3   fucking          9\n",
      "4      tell          9\n",
      "There are 148 tokens in the data.\n",
      "There are 87 unique tokens in the data.\n",
      "There are 718 characters in the data.\n",
      "The lexical diversity is 0.588 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   come         10\n",
      "1   stop          8\n",
      "2   baby          8\n",
      "3  music          7\n",
      "4   keep          6\n",
      "There are 64 tokens in the data.\n",
      "There are 35 unique tokens in the data.\n",
      "There are 285 characters in the data.\n",
      "The lexical diversity is 0.547 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  wanna          7\n",
      "1   dont          6\n",
      "2   back          4\n",
      "3   even          3\n",
      "4   know          2\n",
      "There are 93 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 438 characters in the data.\n",
      "The lexical diversity is 0.613 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    know          8\n",
      "1   takes          5\n",
      "2  around          4\n",
      "3     got          4\n",
      "4    love          4\n",
      "There are 107 tokens in the data.\n",
      "There are 70 unique tokens in the data.\n",
      "There are 499 characters in the data.\n",
      "The lexical diversity is 0.654 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  right          4\n",
      "1   know          3\n",
      "2  treat          3\n",
      "3   love          3\n",
      "4   want          3\n",
      "There are 102 tokens in the data.\n",
      "There are 64 unique tokens in the data.\n",
      "There are 543 characters in the data.\n",
      "The lexical diversity is 0.627 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0     right          9\n",
      "1     youre          4\n",
      "2     words          4\n",
      "3  unspoken          4\n",
      "4     falls          4\n",
      "There are 152 tokens in the data.\n",
      "There are 63 unique tokens in the data.\n",
      "There are 810 characters in the data.\n",
      "The lexical diversity is 0.414 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0  electric         27\n",
      "1      cant          6\n",
      "2      deny          6\n",
      "3   natural          5\n",
      "4      high          5\n",
      "There are 198 tokens in the data.\n",
      "There are 60 unique tokens in the data.\n",
      "There are 989 characters in the data.\n",
      "The lexical diversity is 0.303 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "           token  frequency\n",
      "0          gonna         23\n",
      "1          never         21\n",
      "2           ever         17\n",
      "3  brokenhearted         11\n",
      "4           come          8\n",
      "There are 93 tokens in the data.\n",
      "There are 29 unique tokens in the data.\n",
      "There are 443 characters in the data.\n",
      "The lexical diversity is 0.312 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     know         12\n",
      "1  waiting         10\n",
      "2    every          8\n",
      "3   little          8\n",
      "4    thing          8\n",
      "There are 245 tokens in the data.\n",
      "There are 112 unique tokens in the data.\n",
      "There are 1346 characters in the data.\n",
      "The lexical diversity is 0.457 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   back          8\n",
      "1    got          7\n",
      "2   gone          7\n",
      "3   tech          7\n",
      "4  never          7\n",
      "There are 245 tokens in the data.\n",
      "There are 112 unique tokens in the data.\n",
      "There are 1346 characters in the data.\n",
      "The lexical diversity is 0.457 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   back          8\n",
      "1    got          7\n",
      "2   gone          7\n",
      "3   tech          7\n",
      "4  never          7\n",
      "There are 222 tokens in the data.\n",
      "There are 80 unique tokens in the data.\n",
      "There are 988 characters in the data.\n",
      "The lexical diversity is 0.360 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0       got         35\n",
      "1       get         20\n",
      "2      gone         10\n",
      "3  together         10\n",
      "4      cant          8\n",
      "There are 110 tokens in the data.\n",
      "There are 69 unique tokens in the data.\n",
      "There are 520 characters in the data.\n",
      "The lexical diversity is 0.627 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    right          7\n",
      "1       im          6\n",
      "2     back          5\n",
      "3  nothing          4\n",
      "4  another          3\n",
      "There are 214 tokens in the data.\n",
      "There are 51 unique tokens in the data.\n",
      "There are 890 characters in the data.\n",
      "The lexical diversity is 0.238 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   work         42\n",
      "1    got         20\n",
      "2  shake         16\n",
      "3   make         12\n",
      "4    fit         12\n",
      "There are 253 tokens in the data.\n",
      "There are 96 unique tokens in the data.\n",
      "There are 1234 characters in the data.\n",
      "The lexical diversity is 0.379 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    cant         17\n",
      "1  handle         17\n",
      "2   youre         11\n",
      "3    sure         10\n",
      "4    yeah          8\n",
      "There are 133 tokens in the data.\n",
      "There are 42 unique tokens in the data.\n",
      "There are 709 characters in the data.\n",
      "The lexical diversity is 0.316 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  gonna         12\n",
      "1   hang         11\n",
      "2     im          7\n",
      "3  right          6\n",
      "4  guess          4\n",
      "There are 133 tokens in the data.\n",
      "There are 42 unique tokens in the data.\n",
      "There are 709 characters in the data.\n",
      "The lexical diversity is 0.316 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  gonna         12\n",
      "1   hang         11\n",
      "2     im          7\n",
      "3  right          6\n",
      "4  guess          4\n",
      "There are 99 tokens in the data.\n",
      "There are 42 unique tokens in the data.\n",
      "There are 528 characters in the data.\n",
      "The lexical diversity is 0.424 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   hang         10\n",
      "1  gonna          8\n",
      "2     im          5\n",
      "3  right          4\n",
      "4  guess          3\n",
      "There are 148 tokens in the data.\n",
      "There are 74 unique tokens in the data.\n",
      "There are 737 characters in the data.\n",
      "The lexical diversity is 0.500 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     love         11\n",
      "1  healthy         10\n",
      "2     ever          9\n",
      "3  strange          7\n",
      "4  feeling          7\n",
      "There are 47 tokens in the data.\n",
      "There are 32 unique tokens in the data.\n",
      "There are 244 characters in the data.\n",
      "The lexical diversity is 0.681 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0       go          4\n",
      "1  thought          3\n",
      "2    makin          3\n",
      "3  another          2\n",
      "4     baby          2\n",
      "There are 220 tokens in the data.\n",
      "There are 60 unique tokens in the data.\n",
      "There are 1023 characters in the data.\n",
      "The lexical diversity is 0.273 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0    get         19\n",
      "1   baby         18\n",
      "2   want         13\n",
      "3  honey         13\n",
      "4   need         10\n",
      "There are 92 tokens in the data.\n",
      "There are 64 unique tokens in the data.\n",
      "There are 455 characters in the data.\n",
      "The lexical diversity is 0.696 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   know          5\n",
      "1   time          5\n",
      "2  right          4\n",
      "3  cause          3\n",
      "4   long          3\n",
      "There are 112 tokens in the data.\n",
      "There are 44 unique tokens in the data.\n",
      "There are 474 characters in the data.\n",
      "The lexical diversity is 0.393 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   move         15\n",
      "1     im         12\n",
      "2  human         12\n",
      "3   dont          8\n",
      "4   body          8\n",
      "There are 233 tokens in the data.\n",
      "There are 89 unique tokens in the data.\n",
      "There are 1179 characters in the data.\n",
      "The lexical diversity is 0.382 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0  include         18\n",
      "1     dont         16\n",
      "2    world          8\n",
      "3     fall          8\n",
      "4    apart          8\n",
      "There are 181 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 880 characters in the data.\n",
      "The lexical diversity is 0.315 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0     im         22\n",
      "1   love         20\n",
      "2   like         13\n",
      "3  gonna         12\n",
      "4  never          8\n",
      "There are 179 tokens in the data.\n",
      "There are 58 unique tokens in the data.\n",
      "There are 858 characters in the data.\n",
      "The lexical diversity is 0.324 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0     im         20\n",
      "1   love         18\n",
      "2   like         13\n",
      "3  gonna         10\n",
      "4  never          8\n",
      "There are 145 tokens in the data.\n",
      "There are 66 unique tokens in the data.\n",
      "There are 666 characters in the data.\n",
      "The lexical diversity is 0.455 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   look          8\n",
      "1   eyes          8\n",
      "2     ok          7\n",
      "3   like          6\n",
      "4  think          6\n",
      "There are 145 tokens in the data.\n",
      "There are 66 unique tokens in the data.\n",
      "There are 666 characters in the data.\n",
      "The lexical diversity is 0.455 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   look          8\n",
      "1   eyes          8\n",
      "2     ok          7\n",
      "3   like          6\n",
      "4  think          6\n",
      "There are 58 tokens in the data.\n",
      "There are 32 unique tokens in the data.\n",
      "There are 274 characters in the data.\n",
      "The lexical diversity is 0.552 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  gonna          7\n",
      "1  heart          5\n",
      "2     im          4\n",
      "3  never          4\n",
      "4  think          3\n",
      "There are 108 tokens in the data.\n",
      "There are 54 unique tokens in the data.\n",
      "There are 474 characters in the data.\n",
      "The lexical diversity is 0.500 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   wish         10\n",
      "1   baby          6\n",
      "2   know          4\n",
      "3  could          4\n",
      "4    day          4\n",
      "There are 144 tokens in the data.\n",
      "There are 61 unique tokens in the data.\n",
      "There are 549 characters in the data.\n",
      "The lexical diversity is 0.424 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0      u         30\n",
      "1   jack         24\n",
      "2    ill         16\n",
      "3  youre          5\n",
      "4     go          3\n",
      "There are 96 tokens in the data.\n",
      "There are 76 unique tokens in the data.\n",
      "There are 414 characters in the data.\n",
      "The lexical diversity is 0.792 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   och          4\n",
      "1   som          4\n",
      "2   jag          3\n",
      "3   nãr          3\n",
      "4    sã          3\n",
      "There are 81 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 383 characters in the data.\n",
      "The lexical diversity is 0.704 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     baby          5\n",
      "1      say          4\n",
      "2  another          4\n",
      "3     stay          4\n",
      "4    cause          3\n",
      "There are 177 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 797 characters in the data.\n",
      "The lexical diversity is 0.322 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0      ill         17\n",
      "1     keep         15\n",
      "2     even          8\n",
      "3     fire          7\n",
      "4  burning          7\n",
      "There are 193 tokens in the data.\n",
      "There are 151 unique tokens in the data.\n",
      "There are 978 characters in the data.\n",
      "The lexical diversity is 0.782 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   like         10\n",
      "1     im          7\n",
      "2  wanna          4\n",
      "3    ill          3\n",
      "4    put          3\n",
      "There are 162 tokens in the data.\n",
      "There are 80 unique tokens in the data.\n",
      "There are 723 characters in the data.\n",
      "The lexical diversity is 0.494 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    gone         25\n",
      "1    long         22\n",
      "2      im         18\n",
      "3   today          5\n",
      "4  coming          4\n",
      "There are 119 tokens in the data.\n",
      "There are 35 unique tokens in the data.\n",
      "There are 519 characters in the data.\n",
      "The lexical diversity is 0.294 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     cant         23\n",
      "1     dont         16\n",
      "2     like         15\n",
      "3  control         15\n",
      "4      hey          4\n",
      "There are 313 tokens in the data.\n",
      "There are 48 unique tokens in the data.\n",
      "There are 1418 characters in the data.\n",
      "The lexical diversity is 0.153 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  boom         46\n",
      "1  free         31\n",
      "2  baby         24\n",
      "3  love         18\n",
      "4  give         18\n",
      "There are 244 tokens in the data.\n",
      "There are 42 unique tokens in the data.\n",
      "There are 1153 characters in the data.\n",
      "The lexical diversity is 0.172 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   love         24\n",
      "1   know         16\n",
      "2    cus         14\n",
      "3  kills         14\n",
      "4   dont         11\n",
      "There are 245 tokens in the data.\n",
      "There are 42 unique tokens in the data.\n",
      "There are 1158 characters in the data.\n",
      "The lexical diversity is 0.171 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   love         24\n",
      "1   know         16\n",
      "2    cus         14\n",
      "3  kills         14\n",
      "4   dont         11\n",
      "There are 145 tokens in the data.\n",
      "There are 68 unique tokens in the data.\n",
      "There are 650 characters in the data.\n",
      "The lexical diversity is 0.469 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   work         13\n",
      "1   lets         12\n",
      "2  thing          7\n",
      "3   weve          6\n",
      "4    got          6\n",
      "There are 186 tokens in the data.\n",
      "There are 75 unique tokens in the data.\n",
      "There are 995 characters in the data.\n",
      "The lexical diversity is 0.403 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   space         10\n",
      "1    left         10\n",
      "2  theres         10\n",
      "3   empty          9\n",
      "4  behind          9\n",
      "There are 96 tokens in the data.\n",
      "There are 63 unique tokens in the data.\n",
      "There are 518 characters in the data.\n",
      "The lexical diversity is 0.656 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      love          7\n",
      "1    friday          4\n",
      "2  saturday          4\n",
      "3    sunday          4\n",
      "4     could          4\n",
      "There are 81 tokens in the data.\n",
      "There are 30 unique tokens in the data.\n",
      "There are 342 characters in the data.\n",
      "The lexical diversity is 0.370 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  life         14\n",
      "1  gone          7\n",
      "2  love          7\n",
      "3   let          6\n",
      "4    im          4\n",
      "There are 73 tokens in the data.\n",
      "There are 41 unique tokens in the data.\n",
      "There are 310 characters in the data.\n",
      "The lexical diversity is 0.562 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  baby          8\n",
      "1  dont          8\n",
      "2    go          5\n",
      "3    oh          3\n",
      "4   let          2\n",
      "There are 99 tokens in the data.\n",
      "There are 68 unique tokens in the data.\n",
      "There are 504 characters in the data.\n",
      "The lexical diversity is 0.687 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    love          5\n",
      "1  chorus          4\n",
      "2   thats          4\n",
      "3      im          4\n",
      "4    dont          4\n",
      "There are 101 tokens in the data.\n",
      "There are 68 unique tokens in the data.\n",
      "There are 519 characters in the data.\n",
      "The lexical diversity is 0.673 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0   truth         11\n",
      "1    dont          6\n",
      "2   cause          4\n",
      "3  chorus          4\n",
      "4    cant          3\n",
      "There are 124 tokens in the data.\n",
      "There are 58 unique tokens in the data.\n",
      "There are 548 characters in the data.\n",
      "The lexical diversity is 0.468 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  none         13\n",
      "1  take          6\n",
      "2  away          6\n",
      "3   ive          4\n",
      "4    im          4\n",
      "There are 124 tokens in the data.\n",
      "There are 58 unique tokens in the data.\n",
      "There are 548 characters in the data.\n",
      "The lexical diversity is 0.468 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0  none         13\n",
      "1  take          6\n",
      "2  away          6\n",
      "3   ive          4\n",
      "4    im          4\n",
      "There are 93 tokens in the data.\n",
      "There are 70 unique tokens in the data.\n",
      "There are 480 characters in the data.\n",
      "The lexical diversity is 0.753 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0     never          4\n",
      "1       hes          4\n",
      "2      isnt          3\n",
      "3    chorus          3\n",
      "4  loveless          3\n",
      "There are 89 tokens in the data.\n",
      "There are 62 unique tokens in the data.\n",
      "There are 413 characters in the data.\n",
      "The lexical diversity is 0.697 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    dont          6\n",
      "1    baby          5\n",
      "2  chorus          4\n",
      "3   youre          4\n",
      "4     say          4\n",
      "There are 95 tokens in the data.\n",
      "There are 55 unique tokens in the data.\n",
      "There are 444 characters in the data.\n",
      "The lexical diversity is 0.579 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    yeah         14\n",
      "1     say          7\n",
      "2  people          6\n",
      "3   never          5\n",
      "4     yet          3\n",
      "There are 193 tokens in the data.\n",
      "There are 73 unique tokens in the data.\n",
      "There are 948 characters in the data.\n",
      "The lexical diversity is 0.378 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    baby         20\n",
      "1  psycho         18\n",
      "2   youre         15\n",
      "3    know         11\n",
      "4    dont          9\n",
      "There are 51 tokens in the data.\n",
      "There are 38 unique tokens in the data.\n",
      "There are 249 characters in the data.\n",
      "The lexical diversity is 0.745 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     boy          5\n",
      "1     hey          4\n",
      "2  little          3\n",
      "3    lost          2\n",
      "4   robot          2\n",
      "There are 136 tokens in the data.\n",
      "There are 94 unique tokens in the data.\n",
      "There are 696 characters in the data.\n",
      "The lexical diversity is 0.691 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0     im          8\n",
      "1  robyn          6\n",
      "2   make          4\n",
      "3    let          3\n",
      "4   know          3\n",
      "There are 51 tokens in the data.\n",
      "There are 13 unique tokens in the data.\n",
      "There are 217 characters in the data.\n",
      "The lexical diversity is 0.255 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     want         28\n",
      "1      say          9\n",
      "2    woman          3\n",
      "3    ready          2\n",
      "4  bitches          1\n",
      "There are 101 tokens in the data.\n",
      "There are 36 unique tokens in the data.\n",
      "There are 463 characters in the data.\n",
      "The lexical diversity is 0.356 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0        got         11\n",
      "1        say         11\n",
      "2       baby          9\n",
      "3  something          6\n",
      "4    tonight          5\n",
      "There are 136 tokens in the data.\n",
      "There are 25 unique tokens in the data.\n",
      "There are 503 characters in the data.\n",
      "The lexical diversity is 0.184 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "  token  frequency\n",
      "0   set         43\n",
      "1   got         34\n",
      "2  free         17\n",
      "3  know         15\n",
      "4  body          4\n",
      "There are 96 tokens in the data.\n",
      "There are 66 unique tokens in the data.\n",
      "There are 475 characters in the data.\n",
      "The lexical diversity is 0.688 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    known          8\n",
      "1      let          3\n",
      "2     even          3\n",
      "3     know          3\n",
      "4  believe          3\n",
      "There are 96 tokens in the data.\n",
      "There are 66 unique tokens in the data.\n",
      "There are 475 characters in the data.\n",
      "The lexical diversity is 0.688 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0    known          8\n",
      "1      let          3\n",
      "2     even          3\n",
      "3     know          3\n",
      "4  believe          3\n",
      "There are 179 tokens in the data.\n",
      "There are 57 unique tokens in the data.\n",
      "There are 807 characters in the data.\n",
      "The lexical diversity is 0.318 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "     token  frequency\n",
      "0     show         33\n",
      "1     love         23\n",
      "2  alright         10\n",
      "3     baby          8\n",
      "4      one          7\n",
      "There are 83 tokens in the data.\n",
      "There are 31 unique tokens in the data.\n",
      "There are 440 characters in the data.\n",
      "The lexical diversity is 0.373 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0     stars         11\n",
      "1   forever         10\n",
      "2  together          6\n",
      "3        4x          6\n",
      "4     right          6\n",
      "There are 178 tokens in the data.\n",
      "There are 89 unique tokens in the data.\n",
      "There are 820 characters in the data.\n",
      "The lexical diversity is 0.500 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  still         11\n",
      "1     im          9\n",
      "2   know          7\n",
      "3    let          6\n",
      "4   make          5\n",
      "There are 72 tokens in the data.\n",
      "There are 28 unique tokens in the data.\n",
      "There are 329 characters in the data.\n",
      "The lexical diversity is 0.389 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    tell         20\n",
      "1   today         10\n",
      "2    want          8\n",
      "3  chance          2\n",
      "4   dance          2\n",
      "There are 169 tokens in the data.\n",
      "There are 96 unique tokens in the data.\n",
      "There are 771 characters in the data.\n",
      "The lexical diversity is 0.568 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    back          8\n",
      "1  always          7\n",
      "2    love          6\n",
      "3   cause          6\n",
      "4    want          6\n",
      "There are 127 tokens in the data.\n",
      "There are 53 unique tokens in the data.\n",
      "There are 613 characters in the data.\n",
      "The lexical diversity is 0.417 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    back         14\n",
      "1  taking         12\n",
      "2   could          5\n",
      "3    time          5\n",
      "4     one          5\n",
      "There are 28 tokens in the data.\n",
      "There are 15 unique tokens in the data.\n",
      "There are 114 characters in the data.\n",
      "The lexical diversity is 0.536 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0        uh          6\n",
      "1      like          4\n",
      "2      dont          2\n",
      "3     gimme          2\n",
      "4  somethin          2\n",
      "There are 106 tokens in the data.\n",
      "There are 83 unique tokens in the data.\n",
      "There are 506 characters in the data.\n",
      "The lexical diversity is 0.783 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0     see          4\n",
      "1    love          4\n",
      "2  chorus          3\n",
      "3   place          2\n",
      "4    like          2\n",
      "There are 81 tokens in the data.\n",
      "There are 59 unique tokens in the data.\n",
      "There are 421 characters in the data.\n",
      "The lexical diversity is 0.728 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0    love          6\n",
      "1    shes          5\n",
      "2  chorus          3\n",
      "3      x1          3\n",
      "4   verse          2\n",
      "There are 320 tokens in the data.\n",
      "There are 182 unique tokens in the data.\n",
      "There are 1589 characters in the data.\n",
      "The lexical diversity is 0.569 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  better         41\n",
      "1    know         32\n",
      "2    fuck         15\n",
      "3      im          8\n",
      "4    yyou          7\n",
      "There are 320 tokens in the data.\n",
      "There are 182 unique tokens in the data.\n",
      "There are 1589 characters in the data.\n",
      "The lexical diversity is 0.569 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "    token  frequency\n",
      "0  better         41\n",
      "1    know         32\n",
      "2    fuck         15\n",
      "3      im          8\n",
      "4    yyou          7\n",
      "There are 220 tokens in the data.\n",
      "There are 72 unique tokens in the data.\n",
      "There are 1120 characters in the data.\n",
      "The lexical diversity is 0.327 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  dance         67\n",
      "1   beat         66\n",
      "2   dont          7\n",
      "3   stop          7\n",
      "4   loud          3\n",
      "There are 220 tokens in the data.\n",
      "There are 72 unique tokens in the data.\n",
      "There are 1120 characters in the data.\n",
      "The lexical diversity is 0.327 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0  dance         67\n",
      "1   beat         66\n",
      "2   dont          7\n",
      "3   stop          7\n",
      "4   loud          3\n",
      "There are 53 tokens in the data.\n",
      "There are 42 unique tokens in the data.\n",
      "There are 260 characters in the data.\n",
      "The lexical diversity is 0.792 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      love          4\n",
      "1   thinkin          3\n",
      "2  thoughts          2\n",
      "3      used          2\n",
      "4       one          2\n",
      "There are 168 tokens in the data.\n",
      "There are 54 unique tokens in the data.\n",
      "There are 754 characters in the data.\n",
      "The lexical diversity is 0.321 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "      token  frequency\n",
      "0      girl         23\n",
      "1      whos         18\n",
      "2      cant         10\n",
      "3      take          7\n",
      "4  pressure          7\n",
      "There are 104 tokens in the data.\n",
      "There are 33 unique tokens in the data.\n",
      "There are 523 characters in the data.\n",
      "The lexical diversity is 0.317 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "       token  frequency\n",
      "0      every         11\n",
      "1      hurts          8\n",
      "2  heartbeat          8\n",
      "3      could          6\n",
      "4       dont          6\n",
      "There are 78 tokens in the data.\n",
      "There are 56 unique tokens in the data.\n",
      "There are 349 characters in the data.\n",
      "The lexical diversity is 0.718 in the data.\n",
      "===========================\n",
      "The 5 most common tokens: \n",
      "   token  frequency\n",
      "0   know          5\n",
      "1    ive          3\n",
      "2    got          3\n",
      "3  youve          3\n",
      "4   look          2\n"
     ]
    }
   ],
   "source": [
    "# Perform descriptive stats on Lyrics data\n",
    "lyrics_stats = [descriptive_stats(i) for i in lyrics_df['lyrics_tokenized']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46294409",
   "metadata": {},
   "source": [
    "Q: How do you think the \"top 5 words\" would be different if we left stopwords in the data? \n",
    "\n",
    "A: All the top words would be stopwords.\n",
    "\n",
    "---\n",
    "\n",
    "Q: What were your prior beliefs about the lexical diversity between the artists? Does the difference (or lack thereof) in lexical diversity between the artists conform to your prior beliefs? \n",
    "\n",
    "A: Many songs have repetitve lyrics. Especially, for songs with short chorus' and sang multiple times throughout the song. I expected each artist to each have moderate lexical diversity, which was shown in the results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e1ac1",
   "metadata": {},
   "source": [
    "\n",
    "## Specialty Statistics\n",
    "\n",
    "The descriptive statistics we have calculated are quite generic. You will now calculate a handful of statistics tailored to these data.\n",
    "\n",
    "1. Ten most common emojis by artist in the twitter descriptions.\n",
    "1. Ten most common hashtags by artist in the twitter descriptions.\n",
    "1. Five most common words in song titles by artist. \n",
    "1. For each artist, a histogram of song lengths (in terms of number of tokens) \n",
    "\n",
    "We can use the `emoji` library to help us identify emojis and you have been given a function to help you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "753a5a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(emoji.is_emoji(\"❤️\"))\n",
    "assert(not emoji.is_emoji(\":-)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986fc4c0",
   "metadata": {},
   "source": [
    "### Emojis 😁\n",
    "\n",
    "What are the ten most common emojis by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e805372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_emojis(text):\n",
    "    \"\"\"\n",
    "    Count the top 10 emojis used by an artist's follower.\n",
    "    :param text: list, contains only string elements\n",
    "    :return df: dataframe, ranked emojis w/ their usage counts\n",
    "    \"\"\"\n",
    "    # Get emojis from follower descriptions\n",
    "    emoji_list = [emoji.emoji_list(item) for item in text if len(emoji.emoji_list(item)) > 0]\n",
    "\n",
    "    # Count each emoji and obtain the top 10 used\n",
    "    top_10_emojis = Counter([emoji['emoji'] for sublist in emoji_list for emoji in sublist]).most_common(10)\n",
    "\n",
    "    # Convert to dataframe \n",
    "    df = pd.DataFrame(top_10_emojis, columns=['emoji', 'count'])\n",
    "    df = df.sort_values(by='count', ascending=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "269cd433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emoji</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>❤️</td>\n",
       "      <td>40445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>❤</td>\n",
       "      <td>27998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>🏳️‍🌈</td>\n",
       "      <td>27032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>♥</td>\n",
       "      <td>24687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>✨</td>\n",
       "      <td>24515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>💙</td>\n",
       "      <td>19221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>🌊</td>\n",
       "      <td>18395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>🌈</td>\n",
       "      <td>14799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>💜</td>\n",
       "      <td>14493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>🇺🇸</td>\n",
       "      <td>13409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  emoji  count\n",
       "0    ❤️  40445\n",
       "1     ❤  27998\n",
       "2  🏳️‍🌈  27032\n",
       "3     ♥  24687\n",
       "4     ✨  24515\n",
       "5     💙  19221\n",
       "6     🌊  18395\n",
       "7     🌈  14799\n",
       "8     💜  14493\n",
       "9    🇺🇸  13409"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rank emojis for each artist\n",
    "rank_emojis(twitter_dict['cher'])  # Cher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "996427ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emoji</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>🏳️‍🌈</td>\n",
       "      <td>2956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>♥</td>\n",
       "      <td>2527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>❤️</td>\n",
       "      <td>2425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>✨</td>\n",
       "      <td>1939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>❤</td>\n",
       "      <td>1503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>🌈</td>\n",
       "      <td>1266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>💙</td>\n",
       "      <td>722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>💜</td>\n",
       "      <td>661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>🎶</td>\n",
       "      <td>634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>🖤</td>\n",
       "      <td>542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  emoji  count\n",
       "0  🏳️‍🌈   2956\n",
       "1     ♥   2527\n",
       "2    ❤️   2425\n",
       "3     ✨   1939\n",
       "4     ❤   1503\n",
       "5     🌈   1266\n",
       "6     💙    722\n",
       "7     💜    661\n",
       "8     🎶    634\n",
       "9     🖤    542"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_emojis(twitter_dict['robyn'])  # Robyn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab9b770",
   "metadata": {},
   "source": [
    "### Hashtags\n",
    "\n",
    "What are the ten most common hashtags by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "07c396f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_hashtags(text):\n",
    "\t\"\"\"\n",
    "\tCount the top 10 most commonly used hashtags.\n",
    "    :param text: list, contains only string elements\n",
    "    :return df: dataframe, ranked emojis w/ their usage counts\n",
    "\t\"\"\"\n",
    "\t# Store the hashtags in a list\n",
    "\thashtag_list = [word[1:] for desc in text for word in desc.split() if word[0] == '#']\n",
    "\n",
    "\t# Count the most common hashtags\n",
    "\thashtag_top10 = Counter(hashtag_list).most_common(10)\n",
    "\n",
    "\t# Convert to dataframe \n",
    "\tdf = pd.DataFrame(hashtag_top10, columns=['hashtag', 'count'])\n",
    "\tdf = df.sort_values(by='count', ascending=False)\n",
    "    \n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0a681cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtag</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BLM</td>\n",
       "      <td>7607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Resist</td>\n",
       "      <td>4737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BlackLivesMatter</td>\n",
       "      <td>3908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>resist</td>\n",
       "      <td>2949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FBR</td>\n",
       "      <td>2691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TheResistance</td>\n",
       "      <td>2280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>blacklivesmatter</td>\n",
       "      <td>2163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>1747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Resistance</td>\n",
       "      <td>1431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            hashtag  count\n",
       "0               BLM   7607\n",
       "1            Resist   4737\n",
       "2  BlackLivesMatter   3908\n",
       "3            resist   2949\n",
       "4               FBR   2691\n",
       "5     TheResistance   2280\n",
       "6  blacklivesmatter   2163\n",
       "7                 1   1949\n",
       "8                     1747\n",
       "9        Resistance   1431"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rank Cher's followers top 10 hashtags\n",
    "rank_hashtags(twitter_dict['cher'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b3191bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtag</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BlackLivesMatter</td>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BLM</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blacklivesmatter</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>music</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Music</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EDM</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>blm</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LGBTQ</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            hashtag  count\n",
       "0  BlackLivesMatter    281\n",
       "1               BLM    264\n",
       "2  blacklivesmatter    174\n",
       "3                 1    156\n",
       "4             music    145\n",
       "5                      130\n",
       "6             Music     84\n",
       "7               EDM     74\n",
       "8               blm     47\n",
       "9             LGBTQ     45"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rank Robyn's followers top 10 hashtags\n",
    "rank_hashtags(twitter_dict['robyn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10f21d5",
   "metadata": {},
   "source": [
    "### Song Titles\n",
    "\n",
    "What are the five most common words in song titles by artist? The song titles should be on the first line of the lyrics pages, so if you have kept the raw file contents around, you will not need to re-read the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "bb69b36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>filename</th>\n",
       "      <th>title</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>lyrics_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cher</td>\n",
       "      <td>cher_88degrees.txt</td>\n",
       "      <td>\"88 Degrees\"</td>\n",
       "      <td>Stuck in L.A., ain't got no friends And so Hol...</td>\n",
       "      <td>[stuck, la, aint, got, friends, hollywood, nut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cher</td>\n",
       "      <td>cher_adifferentkindoflovesong.txt</td>\n",
       "      <td>\"A Different Kind Of Love Song\"</td>\n",
       "      <td>What if the world was crazy and I was sane Wou...</td>\n",
       "      <td>[world, crazy, sane, would, strange, cant, bel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cher</td>\n",
       "      <td>cher_afterall.txt</td>\n",
       "      <td>\"After All\"</td>\n",
       "      <td>Well, here we are again I guess it must be fat...</td>\n",
       "      <td>[well, guess, must, fate, weve, tried, deep, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cher</td>\n",
       "      <td>cher_again.txt</td>\n",
       "      <td>\"Again\"</td>\n",
       "      <td>Again evening finds me at your door Here to as...</td>\n",
       "      <td>[evening, finds, door, ask, could, try, dont, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cher</td>\n",
       "      <td>cher_alfie.txt</td>\n",
       "      <td>\"Alfie\"</td>\n",
       "      <td>What's it all about, Alfie? Is it just for the...</td>\n",
       "      <td>[whats, alfie, moment, live, whats, sort, alfi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                           filename                            title  \\\n",
       "0   cher                 cher_88degrees.txt                     \"88 Degrees\"   \n",
       "1   cher  cher_adifferentkindoflovesong.txt  \"A Different Kind Of Love Song\"   \n",
       "2   cher                  cher_afterall.txt                      \"After All\"   \n",
       "3   cher                     cher_again.txt                          \"Again\"   \n",
       "4   cher                     cher_alfie.txt                          \"Alfie\"   \n",
       "\n",
       "                                              lyrics  \\\n",
       "0  Stuck in L.A., ain't got no friends And so Hol...   \n",
       "1  What if the world was crazy and I was sane Wou...   \n",
       "2  Well, here we are again I guess it must be fat...   \n",
       "3  Again evening finds me at your door Here to as...   \n",
       "4  What's it all about, Alfie? Is it just for the...   \n",
       "\n",
       "                                    lyrics_tokenized  \n",
       "0  [stuck, la, aint, got, friends, hollywood, nut...  \n",
       "1  [world, crazy, sane, would, strange, cant, bel...  \n",
       "2  [well, guess, must, fate, weve, tried, deep, i...  \n",
       "3  [evening, finds, door, ask, could, try, dont, ...  \n",
       "4  [whats, alfie, moment, live, whats, sort, alfi...  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-check the lyrics dataframe\n",
    "lyrics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "496249bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_title_words(df, artist):\n",
    "\t\"\"\"\n",
    "\tCount the top 5 most commonly used words in song titles.\n",
    "    :param df: dataframe, artist and lyrics data\n",
    "\t:param artist: str, artist name\n",
    "    :return df: dataframe, ranked emojis w/ their usage counts\n",
    "\t\"\"\"\n",
    "\t# Create subset of lyrics based on artist\n",
    "\ttitles = df.loc[df['artist'] == artist, 'title']\n",
    "\n",
    "\t# Store the title words in a list\n",
    "\ttitle_list = [word.replace('\"', '') for title in titles for word in title.split()]\n",
    "\n",
    "\t# Count the most common title words\n",
    "\twords_top10 = Counter(title_list).most_common(5)\n",
    "\n",
    "\t# Convert to dataframe \n",
    "\tdf = pd.DataFrame(words_top10, columns=['title_word', 'count'])\n",
    "\tdf['artist'] = artist\n",
    "\tdf = df.sort_values(by='count', ascending=False)\n",
    "    \n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "fbdfa9f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_word</th>\n",
       "      <th>count</th>\n",
       "      <th>artist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The</td>\n",
       "      <td>53</td>\n",
       "      <td>cher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You</td>\n",
       "      <td>40</td>\n",
       "      <td>cher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Love</td>\n",
       "      <td>38</td>\n",
       "      <td>cher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I</td>\n",
       "      <td>32</td>\n",
       "      <td>cher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>To</td>\n",
       "      <td>28</td>\n",
       "      <td>cher</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  title_word  count artist\n",
       "0        The     53   cher\n",
       "1        You     40   cher\n",
       "2       Love     38   cher\n",
       "3          I     32   cher\n",
       "4         To     28   cher"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rank Cher's top 5 most used title words\n",
    "rank_title_words(lyrics_df, 'cher')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "32fe2ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_word</th>\n",
       "      <th>count</th>\n",
       "      <th>artist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Me</td>\n",
       "      <td>11</td>\n",
       "      <td>robyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You</td>\n",
       "      <td>8</td>\n",
       "      <td>robyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The</td>\n",
       "      <td>8</td>\n",
       "      <td>robyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My</td>\n",
       "      <td>8</td>\n",
       "      <td>robyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>To</td>\n",
       "      <td>6</td>\n",
       "      <td>robyn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  title_word  count artist\n",
       "0         Me     11  robyn\n",
       "1        You      8  robyn\n",
       "2        The      8  robyn\n",
       "3         My      8  robyn\n",
       "4         To      6  robyn"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rank Robyn's top 5 most used title words\n",
    "rank_title_words(lyrics_df, 'robyn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4fd71",
   "metadata": {},
   "source": [
    "### Song Lengths\n",
    "\n",
    "For each artist, a histogram of song lengths (in terms of number of tokens). If you put the song lengths in a data frame with an artist column, matplotlib will make the plotting quite easy. An example is given to help you out. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "805a1e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist\n",
       "Artist 1    AxesSubplot(0.125,0.11;0.775x0.77)\n",
       "Artist 2    AxesSubplot(0.125,0.11;0.775x0.77)\n",
       "Name: length, dtype: object"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_replicates = 1000\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"artist\" : ['Artist 1'] * num_replicates + ['Artist 2']*num_replicates,\n",
    "    \"length\" : np.concatenate((np.random.poisson(125,num_replicates),np.random.poisson(150,num_replicates)))\n",
    "})\n",
    "\n",
    "df.groupby('artist')['length'].plot(kind=\"hist\",density=True,alpha=0.5,legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde9ebb",
   "metadata": {},
   "source": [
    "Since the lyrics may be stored with carriage returns or tabs, it may be useful to have a function that can collapse whitespace, using regular expressions, and be used for splitting. \n",
    "\n",
    "Q: What does the regular expression `'\\s+'` match on? \n",
    "\n",
    "A: The first part `\\s` matches any whitespace character and the `+` matches the previous token as many times as possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f0e34516",
   "metadata": {},
   "outputs": [],
   "source": [
    "collapse_whitespace = re.compile(r'\\s+')\n",
    "\n",
    "def tokenize_lyrics(lyric) : \n",
    "    \"\"\"strip and split on whitespace\"\"\"\n",
    "    return([item.lower() for item in collapse_whitespace.split(lyric)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2294c440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>filename</th>\n",
       "      <th>title</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>lyrics_tokenized</th>\n",
       "      <th>lyric_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cher</td>\n",
       "      <td>cher_88degrees.txt</td>\n",
       "      <td>\"88 Degrees\"</td>\n",
       "      <td>Stuck in L.A., ain't got no friends And so Hol...</td>\n",
       "      <td>[stuck, la, aint, got, friends, hollywood, nut...</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cher</td>\n",
       "      <td>cher_adifferentkindoflovesong.txt</td>\n",
       "      <td>\"A Different Kind Of Love Song\"</td>\n",
       "      <td>What if the world was crazy and I was sane Wou...</td>\n",
       "      <td>[world, crazy, sane, would, strange, cant, bel...</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cher</td>\n",
       "      <td>cher_afterall.txt</td>\n",
       "      <td>\"After All\"</td>\n",
       "      <td>Well, here we are again I guess it must be fat...</td>\n",
       "      <td>[well, guess, must, fate, weve, tried, deep, i...</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cher</td>\n",
       "      <td>cher_again.txt</td>\n",
       "      <td>\"Again\"</td>\n",
       "      <td>Again evening finds me at your door Here to as...</td>\n",
       "      <td>[evening, finds, door, ask, could, try, dont, ...</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cher</td>\n",
       "      <td>cher_alfie.txt</td>\n",
       "      <td>\"Alfie\"</td>\n",
       "      <td>What's it all about, Alfie? Is it just for the...</td>\n",
       "      <td>[whats, alfie, moment, live, whats, sort, alfi...</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                           filename                            title  \\\n",
       "0   cher                 cher_88degrees.txt                     \"88 Degrees\"   \n",
       "1   cher  cher_adifferentkindoflovesong.txt  \"A Different Kind Of Love Song\"   \n",
       "2   cher                  cher_afterall.txt                      \"After All\"   \n",
       "3   cher                     cher_again.txt                          \"Again\"   \n",
       "4   cher                     cher_alfie.txt                          \"Alfie\"   \n",
       "\n",
       "                                              lyrics  \\\n",
       "0  Stuck in L.A., ain't got no friends And so Hol...   \n",
       "1  What if the world was crazy and I was sane Wou...   \n",
       "2  Well, here we are again I guess it must be fat...   \n",
       "3  Again evening finds me at your door Here to as...   \n",
       "4  What's it all about, Alfie? Is it just for the...   \n",
       "\n",
       "                                    lyrics_tokenized  lyric_length  \n",
       "0  [stuck, la, aint, got, friends, hollywood, nut...           180  \n",
       "1  [world, crazy, sane, would, strange, cant, bel...           133  \n",
       "2  [well, guess, must, fate, weve, tried, deep, i...           120  \n",
       "3  [evening, finds, door, ask, could, try, dont, ...            34  \n",
       "4  [whats, alfie, moment, live, whats, sort, alfi...            66  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain length of artist's lyrics and store as new column\n",
    "lyrics_df['lyric_length'] = [len(item) for item in lyrics_df['lyrics_tokenized']]\n",
    "lyrics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f6178330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist\n",
       "cher     AxesSubplot(0.125,0.11;0.775x0.77)\n",
       "robyn    AxesSubplot(0.125,0.11;0.775x0.77)\n",
       "Name: lyric_length, dtype: object"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your lyric length comparison chart here.\n",
    "lyrics_df.groupby('artist')['lyric_length'].plot(kind=\"hist\",density=True,alpha=0.5,legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6226dc16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
